- en: '[**33        Machine-Learning Algorithms**](toc.xhtml#chap-33)'
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning may be viewed as a subfield of artificial intelligence. Broadly
    speaking, artificial intelligence aims to enable computers to carry out complex
    perception and information-processing tasks with human-like performance. The field
    of AI is vast and uses many different algorithmic methods.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning is rich and fascinating, with strong ties to statistics and
    optimization. Technology today produces enormous amounts of data, providing myriad
    opportunities for machine-learning algorithms to formulate and test hypotheses
    about patterns within the data. These hypotheses can then be used to make predictions
    about the characteristics or classifications in new data. Because machine learning
    is particularly good with challenging tasks involving uncertainty, where observed
    data follows unknown rules, it has markedly transformed fields such as medicine,
    advertising, and speech recognition.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter presents three important machine-learning algorithms: *k*-means
    clustering, multiplicative weights, and gradient descent. You can view each of
    these tasks as a learning problem, whereby an algorithm uses the data collected
    so far to produce a hypothesis that describes the regularities learned and/or
    makes predictions about new data. The boundaries of machine learning are imprecise
    and evolving—some might say that the *k*-means clustering algorithm should be
    called “data science” and not “machine learning,” and gradient descent, though
    an immensely important algorithm for machine learning, also has a multitude of
    applications outside of machine learning (most notably for optimization problems).'
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning typically starts with a ***training phase*** followed by a
    ***prediction phase*** in which predictions are made about new data. For ***online
    learning***, the training and prediction phases are intermingled. The training
    phase takes as input ***training data***, where each input data point has an associated
    output or ***label***; the label might be a category name or some real-valued
    attribute. It then produces as an output one or more ***hypotheses*** about how
    the labels depend on the attributes of the input data points. Hypotheses can take
    many forms, typically some type of formula or algorithm. The learning algorithm
    used is often a form of gradient descent. The prediction phase then uses the hypothesis
    on new data in order to make ***predictions*** regarding the labels of new data
    points.
  prefs: []
  type: TYPE_NORMAL
- en: The type of learning just described is known as ***supervised learning***, since
    it starts with a set of inputs that are each labeled. As an example, consider
    a machine-learning algorithm to recognize spam emails. The training data comprises
    a collection of emails, each of which is labeled either “spam” or “not spam.”
    The machine-learning algorithm frames a hypothesis, possibly a rule of the form
    “if an email has one of a set of words, then it is likely to be spam.” Or it might
    learn rules that assign a spam score to each word and then evaluates a document
    by the sum of the spam scores of its constituent words, so that a document with
    a total score above a certain threshold value is classified as spam. The machine-learning
    algorithm can then predict whether a new email is spam or not.
  prefs: []
  type: TYPE_NORMAL
- en: A second form of machine learning is ***unsupervised learning***, where the
    training data is unlabeled, as in the clustering problem of [Section 33.1](chapter033.xhtml#Sec_33.1).
    Here the machine-learning algorithm produces hypotheses regarding the centers
    of groups of input data points.
  prefs: []
  type: TYPE_NORMAL
- en: A third form of machine learning (not covered further here) is ***reinforcement
    learning***, where the machine-learning algorithm takes actions in an environment,
    receives feedback for those actions from the environment, and then updates its
    model of the environment based on the feedback. The learner is in an environment
    that has some state, and the actions of the learner have an effect on that state.
    Reinforcement learning is a natural choice for situations such as game playing
    or operating a self-driving car.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes the goal in a supervised machine-learning application is not making
    accurate predictions of labels for new examples, but rather performing causal
    ***inference***: finding an explanatory model that describes how the various features
    of an input data point affect its associated label. Finding a model that fits
    a given set of training data well can be tricky. It may involve sophisticated
    optimization methods that need to balance between producing a hypothesis that
    fits the data well and producing a hypothesis that is simple.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter focuses on three problem domains: finding hypotheses that group
    the input data points well (using a clustering algorithm), learning which predictors
    (experts) to rely upon for making predictions in an online learning problem (using
    the multiplicative-weights algorithm), and fitting a model to data (using gradient
    descent).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Section 33.1](chapter033.xhtml#Sec_33.1) considers the clustering problem:
    how to divide a given set of *n* training data points into a given number *k*
    of groups, or “clusters,” based on a measure of how similar (or more accurately,
    how dissimilar) points are to each other. The approach is iterative, beginning
    with an arbitrary initial clustering and incorporating successive improvements
    until no further improvements occur. Clustering is often used as an initial step
    when working on a machine-learning problem to discover what structure exists in
    the data.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Section 33.2](chapter033.xhtml#Sec_33.2) shows how to make online predictions
    quite accurately when you have a set of predictors, often called “experts,” to
    rely on, many of which might be poor predictors, but some of which are good predictors.
    At first, you do not know which predictors are poor and which are good. The goal
    is to make predictions on new examples that are nearly as good as the predictions
    made by the best predictor. We study an effective multiplicative-weights prediction
    method that associates a positive real weight with each predictor and multiplicatively
    decreases the weights associated with predictors when they make poor predictions.
    The model in this section is online (see [Chapter 27](chapter027.xhtml)): at each
    step, we do not know anything about the future examples. In addition, we are able
    to make predictions even in the presence of adversarial experts, who are collaborating
    against us, a situation that actually happens in game-playing settings.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, [Section 33.3](chapter033.xhtml#Sec_33.3) introduces gradient descent,
    a powerful optimization technique used to find parameter settings in machine-learning
    models. Gradient descent also has many applications outside of machine learning.
    Intuitively, gradient descent finds the value that produces a local minimum for
    a function by “walking downhill.” In a learning application, a “downhill step”
    is a step that adjusts hypothesis parameters so that the hypothesis does better
    on the given set of labeled examples.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter makes extensive use of vectors. In contrast to the rest of the
    book, vector names in this chapter appear in boldface, such as **x**, to more
    clearly delineate which quantities are vectors. Components of vectors do not appear
    in boldface, so if vector **x** has *d* dimensions, we might write **x** = (*x*[1],
    *x*[2], …, *x*[*d*]).
  prefs: []
  type: TYPE_NORMAL
- en: '[**33.1    Clustering**](toc.xhtml#Rh1-194)'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that you have a large number of data points (examples), and you wish
    to group them into classes based on how similar they are to each other. For example,
    each data point might represent a celestial star, giving its temperature, size,
    and spectral characteristics. Or, each data point might represent a fragment of
    recorded speech. Grouping these speech fragments appropriately might reveal the
    set of accents of the fragments. Once a grouping of the training data points is
    found, new data can be placed into an appropriate group, facilitating star-type
    recognition or speech recognition.
  prefs: []
  type: TYPE_NORMAL
- en: These situations, along with many others, fall under the umbrella of clustering.
    The input to a ***clustering*** problem is a set of *n* examples (objects) and
    an integer *k*, with the goal of dividing the examples into at most *k* disjoint
    clusters such that the examples in each cluster are similar to each other. The
    clustering problem has several variations. For example, the integer *k* might
    not be given, but instead arises out of the clustering procedure. In this section
    we presume that *k* is given.
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature vectors and similarity**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s formally define the clustering problem. The input is a set of *n* ***examples***.
    Each example has a set of ***attributes*** in common with all other examples,
    though the attribute values may vary among examples. For example, the clustering
    problem shown in [Figure 33.1](chapter033.xhtml#Fig_33-1) clusters *n* = 49 examples—48
    state capitals plus the District of Columbia—into *k* = 4 clusters. Each example
    has two attributes: the latitude and longitude of the capital. In a given clustering
    problem, each example has *d* attributes, with an example **x** specified by a
    *d*-dimensional ***feature vector***'
  prefs: []
  type: TYPE_NORMAL
- en: '**x** = (*x*[1], *x*[2], …, *x*[*d*]).'
  prefs: []
  type: TYPE_NORMAL
- en: Here, *x*[*a*] for *a* = 1, 2, …, *d* is a real number giving the value of attribute
    *a* for example **x**. We call **x** the ***point*** in ℝ^(*d*) representing the
    example. For the example in [Figure 33.1](chapter033.xhtml#Fig_33-1), each capital
    **x** has its latitude in *x*[1] and its longitude in *x*[2].
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to cluster similar points together, we need to define similarity.
    Instead, let’s define the opposite: the ***dissimilarity*** Δ(**x**, **y**) of
    points **x** and **y** is the squared Euclidean distance between them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1325.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Of course, for Δ(**x**, **y**) to be well defined, all attribute values must
    be present. If any are missing, then you might just ignore that example, or you
    could fill in a missing attribute value with the median value for that attribute.
  prefs: []
  type: TYPE_NORMAL
- en: The attribute values are often “messy” in other ways, so that some “data cleaning”
    is necessary before the clustering algorithm is run. For example, the scale of
    attribute values can vary widely across attributes. In the example of [Figure
    33.1](chapter033.xhtml#Fig_33-1), the scales of the two attributes vary by a factor
    of 2, since latitude ranges from −90 to +90 degrees but longitude ranges from
    −180 to +180 degrees. You can imagine other scenarios where the differences in
    scales are even greater. If the examples contain information about students, one
    attribute might be grade-point average but another might be family income. Therefore,
    the attribute values are usually scaled or normalized, so that no single attribute
    can dominate the others when computing dissimilarities. One way to do so is by
    scaling attribute values with a linear transform so that the minimum value becomes
    0 and the maximum value becomes 1\. If the attribute values are binary values,
    then no scaling may be needed. Another option is scaling so that the values for
    each attribute have mean 0 and unit variance. Sometimes it makes sense to choose
    the same scaling rule for several related attributes (for example, if they are
    lengths measured to the same scale).
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1326.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 33.1** The iterations of Lloyd’s procedure when clustering the capitals
    of the lower 48 states and the District of Columbia into *k* = 4 clusters. Each
    capital has two attributes: latitude and longitude. Each iteration reduces the
    value *f*, measuring the sum of squares of distances of all capitals to their
    cluster centers, until the value of *f* does not change. **(a)** The initial four
    clusters, with the capitals of Arkansas, Kansas, Louisiana, and Tennessee chosen
    as centers. **(b)–(k)** Iterations of Lloyd’s procedure. **(l)** The 11th iteration
    results in the same value of *f* as the 10th iteration in part (k), and so the
    procedure terminates.'
  prefs: []
  type: TYPE_NORMAL
- en: Also, the choice of dissimilarity measure is somewhat arbitrary. The use of
    the sum of squared differences as in equation (33.1) is not required, but it is
    a conventional choice and mathematically convenient. For the example of [Figure
    33.1](chapter033.xhtml#Fig_33-1), you might use the actual distance between capitals
    rather than equation (33.1).
  prefs: []
  type: TYPE_NORMAL
- en: '**Clusterings**'
  prefs: []
  type: TYPE_NORMAL
- en: With the notion of similarity (actually, *dis*similarity) defined, let’s see
    how to define clusters of similar points. Let *S* denote the given set of *n*
    points in ℝ^(*d*). In some applications the points are not necessarily distinct,
    so that *S* is a multiset rather than a set.
  prefs: []
  type: TYPE_NORMAL
- en: Because the goal is to create *k* clusters, we define a ***k-clustering*** of
    *S* as a decomposition of *S* into a sequence 〈*S*^((1)), *S*^((2)), …, *S*^((*k*))〉
    of *k* disjoint subsets, or ***clusters***, so that
  prefs: []
  type: TYPE_NORMAL
- en: '*S* = *S*^((1)) ⋃ *S*^((2)) ⋃ ⋯ ⋃ *S*^((*k*)).'
  prefs: []
  type: TYPE_NORMAL
- en: A cluster may be empty, for example if *k* > 1 but all of the points in *S*
    have the same attribute values.
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways to define a *k*-clustering of *S* and many ways to evaluate
    the quality of a given *k*-clustering. We consider here only *k*-clusterings of
    *S* that are defined by a sequence *C* of *k* ***centers***
  prefs: []
  type: TYPE_NORMAL
- en: '*C* = 〈**c^((1))**, **c^((2))**, …, **c^((*k*))**〉,'
  prefs: []
  type: TYPE_NORMAL
- en: 'where each center is a point in ℝ^(*d*), and the ***nearest-center rule***
    says that a point **x** may belong to cluster *S*^((ℓ)) if the center of no other
    cluster is closer to **x** than the center **c^((ℓ))** of *S*^((ℓ)):'
  prefs: []
  type: TYPE_NORMAL
- en: '**x** ∈ *S*^((ℓ)) only if Δ(**x**, **c^((ℓ))**) = min {Δ(**x**, **c^((*j*))**):
    1 ≤ *j* ≤ *k*}.'
  prefs: []
  type: TYPE_NORMAL
- en: A center can be anywhere, and not necessarily a point in *S*.
  prefs: []
  type: TYPE_NORMAL
- en: Ties are possible and must be broken so that each point lies in exactly one
    cluster. In general, ties may be broken arbitrarily, although we’ll need the property
    that we never change which cluster a point **x** is assigned to unless the distance
    from **x** to its new cluster center is *strictly smaller* than the distance from
    **x** to its old cluster center. That is, if the current cluster has a center
    that is one of the closest cluster centers to **x**, then don’t change which cluster
    **x** is assigned to.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ***k-means problem*** is then the following: given a set *S* of *n* points
    and a positive integer *k*, find a sequence ***C*** = 〈**c^((1))**, **c^((2))**,
    …, **c^((*k*))**〉 of *k* center points minimizing the sum *f*(*S*, *C*) of the
    squared distance from each point to its nearest center, where'
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1327.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the second line, the *k*-clustering 〈*S*^((1)),*S*^((2)),…,*S*^((*k*))〉 is
    defined by the centers ***C*** and the nearest-center rule. See Exercise 33.1-1
    for an alternative formulation based on pairwise interpoint distances.
  prefs: []
  type: TYPE_NORMAL
- en: Is there a polynomial-time algorithm for the *k*-means problem? Probably not,
    because it is NP-hard [[310](bibliography001.xhtml#endnote_310)]. As we’ll see
    in [Chapter 34](chapter034.xhtml), NP-hard problems have no known polynomial-time
    algorithm, but nobody has ever proven that polynomial-time algorithms for NP-hard
    problems cannot exist. Although we know of no polynomial-time algorithm that finds
    the global minimum over all clusterings (according to equation (33.2)), we *can*
    find a local minimum.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lloyd [[304](bibliography001.xhtml#endnote_304)] proposed a simple procedure
    that finds a sequence ***C*** of *k* centers that yields a local minimum of *f*(***S***,
    ***C***). A local minimum in the *k*-means problem satisfies two simple properties:
    each cluster has an optimal center (defined below), and each point is assigned
    to the cluster (or one of the clusters) with the closest center. Lloyd’s procedure
    finds a good clustering—possibly optimal—that satisfies these two properties.
    These properties are necessary, but not sufficient, for optimality.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Optimal center for a given cluster**'
  prefs: []
  type: TYPE_NORMAL
- en: In an optimal solution to the *k*-means problem, each center point must be the
    ***centroid***, or ***mean***, of the points in its cluster. The centroid is a
    *d*-dimensional point, where the value in each dimension is the mean of the values
    of all the points in the cluster in that dimension (that is, the mean of the corresponding
    attribute values in the cluster). That is, if **c^((ℓ))** is the centroid for
    cluster *S*^((ℓ)), then for attributes *a* = 1, 2, …, *d*, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1328.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Over all attributes, we write
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1329.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '***Theorem 33.1***'
  prefs: []
  type: TYPE_NORMAL
- en: Given a nonempty cluster *S*^((ℓ)), its centroid (or mean) is the unique choice
    for the cluster center **c^((ℓ))** ∈ ℝ^(*d*) that minimizes
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1330.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '***Proof***   We wish to minimize, by choosing **c^((ℓ))** ∈ ℝ^(*d*), the sum'
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1331.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For each attribute *a*, the term summed is a convex quadratic function in ![art](images/Art_P1332.jpg).
    To minimize this function, take its derivative with respect to ![art](images/Art_P1332.jpg)
    and set it to 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: or, equivalently,
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1335.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Since the minimum is obtained uniquely when each coordinate of ![art](images/Art_P1332.jpg)
    is the average of the corresponding coordinate for **x** ∈ *S*^((ℓ)), the overall
    minimum is obtained when **c^((ℓ))** is the centroid of the points **x**, as in
    equation (33.3).
  prefs: []
  type: TYPE_NORMAL
- en: ▪
  prefs: []
  type: TYPE_NORMAL
- en: '**Optimal clusters for given centers**'
  prefs: []
  type: TYPE_NORMAL
- en: The following theorem shows that the nearest-center rule—assigning each point
    **x** to one of the clusters whose center is nearest to **x**—yields an optimal
    solution to the *k*-means problem.
  prefs: []
  type: TYPE_NORMAL
- en: '***Theorem 33.2***'
  prefs: []
  type: TYPE_NORMAL
- en: Given a set *S* of *n* points and a sequence 〈**c^((1))**, **c^((2))**, …, **c^((*k*))**〉
    of *k* centers, a clustering 〈*S*^((1)), *S*^((2)), …, *S*^((*k*))〉 minimizes
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1337.jpg)'
  prefs: []
  type: TYPE_IMG
- en: if and only if it assigns each point **x** ∈ *S* to a cluster *S*^((ℓ)) that
    minimizes Δ(**x**, **c^((ℓ))**).
  prefs: []
  type: TYPE_NORMAL
- en: '***Proof***   The proof is straightforward: each point **x** ∈ *S* contributes
    exactly once to the sum (33.4), and choosing to put **x** in a cluster whose center
    is nearest minimizes the contribution from **x**.'
  prefs: []
  type: TYPE_NORMAL
- en: ▪
  prefs: []
  type: TYPE_NORMAL
- en: '**Lloyd’s procedure**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Lloyd’s procedure just iterates two operations—assigning points to clusters
    based on the nearest-center rule, followed by recomputing the centers of clusters
    to be their centroids—until the results converge. Here is Lloyd’s procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input:** A set *S* of points in ℝ^(*d*), and a positive integer *k*.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Output:** A *k*-clustering 〈*S*^((1)), *S*^((2)), …, *S*^((*k*))〉 of *S*
    with a sequence of centers 〈**c^((1))**, **c^((2))**, …, **c^((*k*))**〉.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Initialize centers:** Generate an initial sequence 〈**c^((1))**, **c^((2))**,
    …, **c^((*k*))**〉 of *k* centers by picking *k* points independently from *S*
    at random. (If the points are not necessarily distinct, see Exercise 33.1-3.)
    Assign all points to cluster *S*^((1)) to begin.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Assign points to clusters:** Use the nearest-center rule to define the clustering
    〈*S*^((1)), *S*^((2)), …, *S*^((*k*))〉. That is, assign each point **x** ∈ *S*
    to a cluster *S*^((ℓ)) having a nearest center (breaking ties arbitrarily, but
    not changing the assignment for a point **x** unless the new cluster center is
    strictly closer to **x** than the old one).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Stop if no change:** If step 2 did not change the assignments of any points
    to clusters, then stop and return the clustering 〈*S*^((1)), *S*^((2)), …, *S*^((*k*))〉
    and the associated centers 〈**c^((1))**, **c^((2))**, …, **c^((*k*))**〉. Otherwise,
    go to step 4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Recompute centers as centroids:** For ℓ = 1, 2, …, *k*, compute the center
    **c^((ℓ))** of cluster *S*^((ℓ)) as the centroid of the points in *S*^((ℓ)). (If
    *S*^((ℓ)) is empty, let **c^((ℓ))** be the zero vector.) Then go to step 2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is possible for some of the clusters returned to be empty, particularly if
    many of the input points are identical.
  prefs: []
  type: TYPE_NORMAL
- en: Lloyd’s procedure always terminates. By Theorem 33.1, recomputing the centers
    of each cluster as the cluster centroid cannot increase *f*(*S*, *C*). Lloyd’s
    procedure ensures that a point is reassigned to a different cluster only when
    such an operation strictly decreases *f*(*S*, *C*). Thus each iteration of Lloyd’s
    procedure, except the last iteration, must strictly decrease *f*(*S*, *C*). Since
    there are only a finite number of possible *k*-clusterings of *S* (at most *k*^(*n*)),
    the procedure must terminate. Furthermore, once one iteration of Lloyd’s procedure
    yields no decrease in *f*, further iterations would not change anything, and the
    procedure can stop at this locally optimum assignment of points to clusters.
  prefs: []
  type: TYPE_NORMAL
- en: If Lloyd’s procedure really required *k*^(*n*) iterations, it would be impractical.
    In practice, it sometimes suffices to terminate the procedure when the percentage
    decrease in *f*(*S*, *C*) in the latest iteration falls below a predetermined
    threshold. Because Lloyd’s procedure is guaranteed to find only a locally optimal
    clustering, one approach to finding a good clustering is to run Lloyd’s procedure
    many times with different randomly chosen initial centers, taking the best result.
  prefs: []
  type: TYPE_NORMAL
- en: The running time of Lloyd’s procedure is proportional to the number *T* of iterations.
    In one iteration, assigning points to clusters based on the nearest-center rule
    requires *O*(*dkn*) time, and recomputing new centers for each cluster requires
    *O*(*dn*) time (because each point is in one cluster). The overall running time
    of the *k*-means procedure is thus *O*(*Tdkn*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Lloyd’s algorithm illustrates an approach common to many machine-learning algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: First, define a hypothesis space in terms an appropriate sequence *θ* of parameters,
    so that each *θ* is associated with a specific hypothesis *h*[*θ*]. (For the *k*-means
    problem, *θ* is a *dk*-dimensional vector, equivalent to *C*, containing the *d*-dimensional
    center of each of the *k* clusters, and *h*[*θ*] is the hypothesis that each data
    point **x** should be grouped with a cluster having a center closest to **x**.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second, define a measure *f*(*E*, *θ*) describing how poorly hypothesis *h*[*θ*]
    fits the given training data *E*. Smaller values of *f*(*E*, *θ*) are better,
    and a (locally) optimal solution (locally) minimizes *f*(*E*, *θ*). (For the *k*-means
    problem, *f*(*E*, *θ*) is just *f*(*S*, *C*).)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Third, given a set of training data *E*, use a suitable optimization procedure
    to find a value of *θ** that minimizes *f*(*E*, *θ**), at least locally. (For
    the *k*-means problem, this value of *θ** is the sequence *C* of *k* center points
    returned by Lloyd’s algorithm.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Return *θ** as the answer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this framework, we see that optimization becomes a powerful tool for machine
    learning. Using optimization in this way is flexible. For example, ***regularization***
    terms can be incorporated in the function to be minimized, in order to penalize
    hypotheses that are “too complicated” and that “overfit” the training data. (Regularization
    is a complex topic that isn’t pursued further here.)
  prefs: []
  type: TYPE_NORMAL
- en: '**Examples**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 33.1](chapter033.xhtml#Fig_33-1) demonstrates Lloyd’s procedure on
    a set of *n* = 49 cities: 48 U.S. state capitals and the District of Columbia.
    Each city has *d* = 2 dimensions: latitude and longitude. The initial clustering
    in part (a) of the figure has the initial cluster centers arbitrarily chosen as
    the capitals of Arkansas, Kansas, Louisiana, and Tennessee. As the procedure iterates,
    the value of the function *f* decreases, until the 11th iteration in part (l),
    where it remains the same as in the 10th iteration in part (k). Lloyd’s procedure
    then terminates with the clusters shown in part (l).'
  prefs: []
  type: TYPE_NORMAL
- en: As [Figure 33.2](chapter033.xhtml#Fig_33-2) shows, Lloyd’s procedure can also
    apply to “vector quantization.” Here, the goal is to reduce the number of distinct
    colors required to represent a photograph, thereby allowing the photograph to
    be greatly compressed (albeit in a lossy manner). In part (a) of the figure, an
    original photograph 700 pixels wide and 500 pixels high uses 24 bits (three bytes)
    per pixel to encode a triple of red, green, and blue (RGB) primary color intensities.
    Parts (b)–(e) of the figure show the results of using Lloyd’s procedure to compress
    the picture from a initial space of 2^(24) possible values per pixel to a space
    of only *k* = 4, *k* = 16, *k* = 64, or *k* = 256 possible values per pixel; these
    *k* values are the cluster centers. The photograph can then be represented with
    only 2, 4, 6, or 8 bits per pixel, respectively, instead of the 24-bits per pixel
    needed by the initial photograph. An auxiliary table, the “palette,” accompanies
    the compressed image; it holds the *k* 24-bit cluster centers and is used to map
    each pixel value to its 24-bit cluster center when the photo is decompressed.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercises**'
  prefs: []
  type: TYPE_NORMAL
- en: '***33.1-1***'
  prefs: []
  type: TYPE_NORMAL
- en: Show that the objective function *f*(*S*, *C*) of equation (33.2) may be alternatively
    written as
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1338.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '***33.1-2***'
  prefs: []
  type: TYPE_NORMAL
- en: Give an example in the plane with *n* = 4 points and *k* = 2 clusters where
    an iteration of Lloyd’s procedure does not improve *f*(*S*, *C*), yet the *k*-clustering
    is not optimal.
  prefs: []
  type: TYPE_NORMAL
- en: '***33.1-3***'
  prefs: []
  type: TYPE_NORMAL
- en: When the input to Lloyd’s procedure contains many repeated points, a different
    initialization procedure might be used. Describe a way to pick a number of centers
    at random that maximizes the number of distinct centers picked. (*Hint:* See Exercise
    5.3-5.)
  prefs: []
  type: TYPE_NORMAL
- en: '***33.1-4***'
  prefs: []
  type: TYPE_NORMAL
- en: Show how to find an optimal *k*-clustering in polynomial time when there is
    just one attribute (*d* = 1).
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1339.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 33.2** Using Lloyd’s procedure for vector quantization to compress
    a photo by using fewer colors. **(a)** The original photo has 350,000 pixels (700
    × 500), each a 24-bit RGB (red/blue/green) triple of 8-bit values; these pixels
    (colors) are the “points” to be clustered. Points repeat, so there are only 79,083
    distinct colors (less than 2^(24)). After compression, only *k* distinct colors
    are used, so each pixel is represented by only ⌈1g *k*⌉ bits instead of 24\. A
    “palette” maps these values back to 24-bit RGB values (the cluster centers). **(b)–(e)**
    The same photo with *k* = 4, 16, 64, and 256 colors. (Photo from standuppaddle,
    pixabay.com.)'
  prefs: []
  type: TYPE_NORMAL
- en: '[**33.2    Multiplicative-weights algorithms**](toc.xhtml#Rh1-195)'
  prefs: []
  type: TYPE_NORMAL
- en: 'This section considers problems that require you to make a series of decisions.
    After each decision you receive feedback as to whether your decision was correct.
    We will study a class of algorithms that are called ***multiplicative-weights
    algorithms***. This class of algorithms has a wide variety of applications, including
    game playing in economics, approximately solving linear-programming and multicommodity-flow
    problems, and various applications in online machine learning. We emphasize the
    online nature of the problem here: you have to make a sequence of decisions, but
    some of the information needed to make the *i*th decision appears only after you
    have already made the (*i* – 1)st decision. In this section, we look at one particular
    problem, known as “learning from experts,” and develop an example of a multiplicative-weights
    algorithm, called the weighted-majority algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose that a series of events will occur, and you want to make predictions
    about these events. For example, over a series of days, you want to predict whether
    it is going to rain. Or perhaps you want to predict whether the price of a stock
    will increase or decrease. One way to approach this problem is to assemble a group
    of “experts” and use their collective wisdom in order to make good predictions.
    Let’s denote the experts, *n* of them, by *E*[1], *E*[2], …, *E*[*n*], and let’s
    say that *T* events are going to take place. Each event has an outcome of either
    0 or 1, with *o*^((*t*)) denoting the outcome of the *t*th event. Before event
    *t*, each expert *E*^((*i*)) makes a prediction ![art](images/Art_P1340.jpg).
    You, as the “learner,” then take the set of *n* expert predictions for event *t*
    and produce a single prediction *p*^((*t*)) ∈ {0, 1} of your own. You base your
    prediction only on the predictions of the experts and anything you have learned
    about the experts from their previous predictions. You do not use any additional
    information about the event. Only after making your prediction do you ascertain
    the outcome *o*^((*t*)) of event *t*. If your prediction *p*^((*t*)) matches *o*^((*t*)),
    then you were correct; otherwise, you made a mistake. The goal is to minimize
    the total number *m* of mistakes, where ![art](images/Art_P1341.jpg). You can
    also keep track of the number of mistakes each expert makes: expert *E*[*i*] makes
    *m*[*i*] mistakes, where ![art](images/Art_P1342.jpg).'
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose that you are following the price of a stock, and each day
    you decide whether to invest in it for just that day by buying it at the beginning
    of the day and selling it at the end of the day. If, on some day, you buy the
    stock and it goes up, then you made the correct decision, but if the stock goes
    down, then you made a mistake. Similarly, if on some day, you do not buy the stock
    and it goes down, then you made the correct decision, but if the stock goes up,
    then you made a mistake. Since you would like to make as few mistakes as possible,
    you use the advice of the experts to make your decisions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll assume nothing about the movement of the stock. We’ll also assume nothing
    about the experts: the experts’ predictions could be correlated, they could be
    chosen to deceive you, or perhaps some are not really experts after all. What
    algorithm would you use?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before designing an algorithm for this problem, we need to consider what is
    a fair way to evaluate our algorithm. It is reasonable to expect that our algorithm
    performs better when the expert predictions are better, and that it performs worse
    when the expert predictions are worse. The goal of the algorithm is to limit the
    number of mistakes you make to be close to the number of mistakes that the best
    of the experts makes. At first, this goal might seem impossible, because you do
    not know until the end which expert is best. We’ll see, however, that by taking
    the advice provided by all the experts into account, you can achieve this goal.
    More formally, we use the notion of “regret,” which compares our algorithm to
    the performance of the best expert (in hindsight) over all. Letting *m** = min
    {*m*[*i*] : 1 ≤ *i* ≤ *n*} denote the number of mistakes made by the best expert,
    the ***regret*** is *m* – *m**. The goal is to design an algorithm with low regret.
    (Regret can be negative, although it typically isn’t, since it is rare that you
    do better than the best expert.)'
  prefs: []
  type: TYPE_NORMAL
- en: As a warm-up, let’s consider the case in which one of the experts makes a correct
    prediction each time. Even without knowing who that expert is, you can still achieve
    good results.
  prefs: []
  type: TYPE_NORMAL
- en: '***Lemma 33.3***'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that out of *n* experts, there is one who always makes the correct prediction
    for all *T* events. Then there is an algorithm that makes at most ⌈1g *n*⌉ mistakes.
  prefs: []
  type: TYPE_NORMAL
- en: '***Proof***   The algorithm maintains a set *S* consisting of experts who have
    not yet made a mistake. Initially, *S* contains all *n* experts. The algorithm’s
    prediction is always the majority vote of the predictions of the experts remaining
    in set *S*. In case of a tie, the algorithm makes any prediction. After each outcome
    is learned, set *S* is updated to remove all the experts who made an incorrect
    prediction about that outcome.'
  prefs: []
  type: TYPE_NORMAL
- en: We now analyze the algorithm. The expert who always makes the correct prediction
    will always be in set *S*. Every time the algorithm makes a mistake, at least
    half of the experts who were still in *S* also make a mistake, and these experts
    are removed from *S*. If *S*′ is the set of experts remaining after removing those
    who made a mistake, we have that |*S*′| ≤ |*S*|/2\. The size of *S* can be halved
    at most ⌈1g *n*⌉ times until |*S*| = 1\. From this point on, we know that the
    algorithm never makes a mistake, since the set *S* consists only of the one expert
    who never makes a mistake. Therefore, overall the algorithm makes at most ⌈1g
    *n*⌉ mistakes.
  prefs: []
  type: TYPE_NORMAL
- en: ▪
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 33.2-1 asks you to generalize this result to the case when there is
    no expert who makes perfect predictions and show that, for any set of experts,
    there is an algorithm that makes at most *m** ⌈1g *n*⌉ mistakes. The generalized
    algorithm begins in the same way. The set *S* might become empty at some point,
    however. If that ever happens, reset *S* to contain all the experts and continue
    the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: You can substantially improve your prediction ability by not just tracking which
    experts have not made any mistakes, or have not made any mistakes recently, to
    a more nuanced evaluation of the quality of each expert. The key idea is to use
    the feedback you receive to update your evaluation of how much trust to put in
    each expert. As the experts make predictions, you observe whether they were correct
    and decrease your confidence in the experts who make more mistakes. In this way,
    you can learn over time which experts are more reliable and which are less reliable,
    and weight their predictions accordlingly. The change in weights is accomplished
    via multiplication, hence the term “multiplicative weights.”
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm appears in the procedure WEIGHTED-MAJORITY on the following page,
    which takes a set *E* = {*E*[1], *E*[2], …, *E*[*n*]} of experts, a number *T*
    of events, the number *n* of experts, and a parameter 0 < *γ* ≤ 1/2 that controls
    how the weights change. The algorithm maintains weights ![art](images/Art_P1343.jpg)
    for *i* = 1, 2, …, *n* and *t* = 1, 2, …, *T*, where ![art](images/Art_P1344.jpg).
    The **for** loop of lines 1–2 sets the initial weights ![art](images/Art_P1345.jpg)
    to 1, capturing the idea that with no knowledge, you trust each expert equally.
    Each iteration of the main **for** loop of lines 3–18 does the following for an
    event *t* = 1, 2, …, *T*. Each expert *E*[*i*] makes a prediction for event *t*
    in line 4\. Lines 5–8 compute *upweight*^((*t*)), the sum of the weights of the
    experts who predict 1 for event *t*, and *downweight*^((*t*)), the sum of the
    weights of the experts who predict 0 for the event. Lines 9–11 decide the algorithm’s
    prediction *p*^((*t*)) for event *t* based on whichever weighted sum is larger
    (breaking ties in favor of deciding 1). The outcome of event *t* is revealed in
    line 12\. Finally, lines 14–17 decrease the weights of the experts who made an
    incorrect prediction for event *t* by multiplying their weights by 1 – *γ*, leaving
    alone the weights of the experts who correctly predicted the event’s outcome.
    Thus, the fewer mistakes each expert makes, the higher that expert’s weight.
  prefs: []
  type: TYPE_NORMAL
- en: The WEIGHTED-MAJORITY procedure doesn’t do much worse than any expert. In particular,
    it doesn’t do much worse than the best expert. To quantify this claim, let *m*^((*t*))
    be the number of mistakes made by the procedure through event *t*, and let ![art](images/Art_P1346.jpg)
    be the number of mistakes made by expert *E*[*i*] through event *t*. The following
    theorem is the key.
  prefs: []
  type: TYPE_NORMAL
- en: WEIGHTED-MAJORITY(*E*, *T*, *n*, *γ*)
  prefs: []
  type: TYPE_NORMAL
- en: '|   1 | **for** *i* = 1 **to** *n* |'
  prefs: []
  type: TYPE_TB
- en: '|   2 | ![art](images/Art_P1347.jpg) | **//** trust each expert equally |'
  prefs: []
  type: TYPE_TB
- en: '|   3 | **for** *t* = 1 **to** *T* |'
  prefs: []
  type: TYPE_TB
- en: '|   4 | each expert *E*[*i*] ∈ *E* makes a prediction ![art](images/Art_P1348.jpg)
    |'
  prefs: []
  type: TYPE_TB
- en: '|   5 | ![art](images/Art_P1349.jpg) | **//** experts who predicted 1 |'
  prefs: []
  type: TYPE_TB
- en: '|   6 | ![art](images/Art_P1350.jpg) | **//** sum of weights of who predicted
    1 |'
  prefs: []
  type: TYPE_TB
- en: '|   7 | ![art](images/Art_P1351.jpg) | **//** experts who predicted 0 |'
  prefs: []
  type: TYPE_TB
- en: '|   8 | ![art](images/Art_P1352.jpg) | **//** sum of weights of who predicted
    0 |'
  prefs: []
  type: TYPE_TB
- en: '|   9 | **if** *upweight*^((*t*)) ≥ *downweight*^((*t*)) |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | *p*^((*t*)) = 1 | **//** algorithm predicts 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | **else** *p*^((*t*)) = 0 | **//** algorithm predicts 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | outcome *o*^((*t*)) is revealed |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | **//** If *p*^((*t*)) ≠ *o*^((*t*)), the algorithm made a mistake. |'
  prefs: []
  type: TYPE_TB
- en: '| 14 | **for** *i* = 1 **to** *n* |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | **if** ![art](images/Art_P1353.jpg) | **//** if expert *E*^((*i*)) made
    a mistake … |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | ![art](images/Art_P1354.jpg) | **//** … then decrease that expert’s
    weight |'
  prefs: []
  type: TYPE_TB
- en: '| 17 | **else** ![art](images/Art_P1355.jpg) |'
  prefs: []
  type: TYPE_TB
- en: '| 18 | **return** *p*^((*t*)) |'
  prefs: []
  type: TYPE_TB
- en: '***Theorem 33.4***'
  prefs: []
  type: TYPE_NORMAL
- en: When running WEIGHTED-MAJORITY, we have, for every expert *E*[*i*] and every
    event *T*′ ≤ *T*,
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1356.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '***Proof***   Every time an expert *E*[*i*] makes a mistake, its weight, which
    is initially 1, is multiplied by 1 – *γ*, and so we have'
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1357.jpg)'
  prefs: []
  type: TYPE_IMG
- en: for *t* = 1, 2, …, *T*.
  prefs: []
  type: TYPE_NORMAL
- en: We use a potential function ![art](images/Art_P1358.jpg), summing the weights
    for all *n* experts after iteration *t* of the **for** loop of lines 3–18\. Initially,
    we have *W*(0) = *n* since all *n* weights start out with the value 1\. Because
    each expert belongs to either the set *U* or the set *D* (defined in lines 5 and
    7 of WEIGHTED-MAJORITY), we always have *W*(*t*) = *upweight*^((*t*)) + *downweight*^((*t*))
    after each execution of line 8.
  prefs: []
  type: TYPE_NORMAL
- en: Consider an iteration *t* in which the algorithm makes a mistake in its prediction,
    which means that either the algorithm predicts 1 and the outcome is 0 or the algorithm
    predicts 0 and the outcome is 1\. Without loss of generality, assume that the
    algorithm predicts 1 and the outcome is 0\. The algorithm predicted 1 because
    *upweight*^((*t*)) ≥ *downweight*^((*t*)) in line 9, which implies that
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1359.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Each expert in *U* then has its weight multiplied by 1 – *γ*, and each expert
    in *D* has its weight unchanged. Thus, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1360.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Therefore, for every iteration *t* in which the algorithm makes a mistake, we
    have
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1361.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In an iteration where the algorithm does not make a mistake, some of the weights
    decrease and some remain unchanged, so that we have
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1362.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Since there are *m*^((*T*′)) mistakes made through iteration *T*′, and *W*(1)
    = *n*, we can repeatedly apply inequality (33.8) to iterations where the algorithm
    makes a mistake and inequality (33.9) to iterations where the algorithm does not
    make a mistake, obtaining
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1363.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Because the function *W* is the sum of the weights and all weights are positive,
    its value exceeds any single weight. Therefore, using equation (33.6) we have,
    for any expert *E*[*i*] and for any iteration *T*′ ≤ *T*,
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1364.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Combining inequalities (33.10) and (33.11) gives
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1365.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Taking the natural logarithm of both sides yields
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1366.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We now use the Taylor series expansion to derive upper and lower bounds on the
    logarithmic factors in inequality (33.12). The Taylor series for ln(1+*x*) is
    given in equation (3.22) on page 67\. Substituting −*x* for *x*, we have that
    for 0 < *x* ≤ 1/2,
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1367.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Since each term on the right-hand side is negative, we can drop all terms except
    the first and obtain an upper bound of ln(1 – *x*) ≤ −*x*. Since 0 < *γ* ≤ 1/2,
    we have
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1368.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For the lower bound, Exercise 33.2-2 asks you to show that ln(1 – *x*) ≥ −*x*
    − *x*² when 0 < *x* ≤ 1/2, so that
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1369.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Thus, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1370.jpg)'
  prefs: []
  type: TYPE_IMG
- en: so that
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1371.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Subtracting ln *n* from both sides of inequality (33.16) and then multiplying
    both sides by −2/*γ* yields ![art](images/Art_P1372.jpg), thus proving the theorem.
  prefs: []
  type: TYPE_NORMAL
- en: ▪
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 33.4 applies to any expert and any event *T*′ ≤ *T*. In particular,
    we can compare against the best expert after all events have occurred, producing
    the following corollary.
  prefs: []
  type: TYPE_NORMAL
- en: '***Corollary 33.5***'
  prefs: []
  type: TYPE_NORMAL
- en: At the end of procedure WEIGHTED-MAJORITY, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1373.jpg)'
  prefs: []
  type: TYPE_IMG
- en: ▪
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explore this bound. Assuming that ![art](images/Art_P1374.jpg), we can
    choose ![art](images/Art_P1375.jpg) and plug into inequality (33.17) to obtain
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1376.jpg)'
  prefs: []
  type: TYPE_IMG
- en: and so the number of errors is at most twice the number of errors made by the
    best expert plus a term that is often slower growing than *m**. Exercise 33.2-4
    shows that you can decrease the bound on the number of errors by a factor of 2
    by using randomization, which leads to much stronger bounds. In particular, the
    upper bound on regret (*m* – *m**) is reduced from (1 + 2*γ*)*m** + (2 ln *n*)/*γ*
    to an expected value of *ϵm** + (ln *n*)/*ϵ*, where both *γ* and *ϵ* are at most
    1/2\. Numerically, we can see that if*γ* = 1/2, WEIGHTED-MAJORITY makes at most
    3 times the number of errors as the best expert, plus 4 ln *n* errors. As another
    example, suppose that *T* = 1000 predictions are being made by *n* = 20 experts,
    and the best expert is correct 95% of the time, making 50 errors. Then WEIGHTED-MAJORITY
    makes at most 100(1+*γ*)+2 ln 20/*γ* errors. By choosing *γ* = 1/4, WEIGHTED-MAJORITY
    makes at most 149 errors, or a success rate of at least 85%.
  prefs: []
  type: TYPE_NORMAL
- en: Multiplicative weights methods typically refer to a broader class of algorithms
    that includes WEIGHTED-MAJORITY. The outcomes and predictions need not be only
    0 or 1, but can be real numbers, and there can be a loss associated with a particular
    outcome and prediction. The weights can be updated by a multiplicative factor
    that depends on the loss, and the algorithm can, given a set of weights, treat
    them as a distribution on experts and use them to choose an expert to follow in
    each event. Even in these more general settings, bounds similar to Theorem 33.4
    hold.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercises**'
  prefs: []
  type: TYPE_NORMAL
- en: '***33.2-1***'
  prefs: []
  type: TYPE_NORMAL
- en: The proof of Lemma 33.3 assumes that some expert never makes a mistake. It is
    possible to generalize the algorithm and analysis to remove this assumption. The
    new algorithm begins in the same way. The set *S* might become empty at some point,
    however. If that ever happens, reset *S* to contain all the experts and continue
    the algorithm. Show that the number of mistakes that this algorithm makes is at
    most *m** ⌈1g *n*⌉.
  prefs: []
  type: TYPE_NORMAL
- en: '***33.2-2***'
  prefs: []
  type: TYPE_NORMAL
- en: Show that ln(1 – *x*) ≥ −*x* – *x*² when 0 < *x* ≤ 1/2\. (*Hint:* Start with
    equation (33.13), group all the terms after the first three, and use equation
    (A.7) on page 1142.)
  prefs: []
  type: TYPE_NORMAL
- en: '***33.2-3***'
  prefs: []
  type: TYPE_NORMAL
- en: Consider a randomized variant of the algorithm given in the proof of Lemma 33.3,
    in which some expert never makes a mistake. At each step, choose an expert *E*[*i*]
    uniformly at random from the set *S* and then make the same predication as *E*[*i*].
    Show that the expected number of mistakes made by this algorithm is ⌈1g *n*⌉.
  prefs: []
  type: TYPE_NORMAL
- en: '***33.2-4***'
  prefs: []
  type: TYPE_NORMAL
- en: Consider a randomized version of WEIGHTED-MAJORITY. The algorithm is the same,
    except for the prediction step, which interprets the weights as a probability
    distribution over the experts and chooses an expert *E*[*i*] according to that
    distribution. It then chooses its prediction to be the same as the prediction
    made by expert *E*[*i*]. Show that, for any 0 < *ϵ* < 1/2, the expected number
    of mistakes made by this algorithm is at most (1 + *ϵ*)*m** + (ln *n*)/*ϵ*.
  prefs: []
  type: TYPE_NORMAL
- en: '[**33.3    Gradient descent**](toc.xhtml#Rh1-196)'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that you have a set {*p*[1], *p*[2], …, *p*[*n*]} of points and you
    want to find the line that best fits these points. For any line ℓ, there is a
    distance *d*[*i*] between each point *p*[*i*] and the line. You want to find the
    line that minimizes some function *f*(*d*[1], …, *d*[*n*]). There are many possible
    choices for the definition of distance and for the function *f*. For example,
    the distance can be the projection distance to the line and the function can be
    the sum of the squares of the distances. This type of problem is common in data
    science and machine learning—the line is the hypothesis that best describes the
    data—where the particular definition of best is determined by the definition of
    distance and the objective *f*. If the definition of distance and the function
    *f* are linear, then we have a linear-programming problem, as discussed in [Chapter
    29](chapter029.xhtml). Although the linear-programming framework captures several
    important problems, many other problems, including various machine-learning problems,
    have objectives and constraints that are not necessarily linear. We need frameworks
    and algorithms to solve such problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we consider the problem of optimizing a continuous function
    and discuss one of the most popular methods to do so: gradient descent. Gradient
    descent is a general method for finding a local minimum of a function *f* : ℝ^(*n*)
    → ℝ, where informally, a local minimum of a function *f* is a point **x** for
    which *f*(**x**) ≤ *f*(**x**′) for all **x**′ that are “near” **x**. When the
    function is convex, it can find a point near the ***global minimizer*** of *f*:
    an *n*-vector argument **x** = (*x*[1], *x*[2], …, *x*[*n*]) such that *f*(**x**)
    is minimum. For the intuitive idea behind gradient descent, imagine being in a
    landscape of hills and valleys, and wanting to get to a low point as quickly as
    possible. You survey the terrain and choose to move in the direction that takes
    you downhill the fastest from your current position. You move in that direction,
    but only for a short while, because as you proceed, the terrain changes and you
    might need to choose a different direction. So you stop, reevaluate the possible
    directions and move another short distance in the steepest downhill direction,
    which might differ from the direction of your previous movement. You continue
    this process until you reach a point from which all directions lead up. Such a
    point is a local minimum.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to make this informal procedure more formal, we need to define the
    gradient of a function, which in the analogy above is a measure of the steepness
    of the various directions. Given a function *f* : ℝ^(*n*) → ℝ, its ***gradient***
    ∇*f* is a function ∇*f* : ℝ^(*n*) → ℝ^(*n*) comprising *n* partial derivatives:
    ![art](images/Art_P1377.jpg). Analogous to the derivative of a function of a single
    variable, the gradient can be viewed as a direction in which the function value
    locally increases the fastest, and the rate of that increase. This view is informal;
    in order to make it formal we would have to define what local means and place
    certain conditions, such as continuity or existence of derivatives, on the function.
    Nevertheless, this view motivates the key step of gradient descent—move in the
    direction opposite to the gradient, by a distance influenced by the magnitude
    of the gradient.'
  prefs: []
  type: TYPE_NORMAL
- en: The general procedure of gradient descent proceeds in steps. You start at some
    initial point **x^((0))**, which is an *n*-vector. At each step *t*, you compute
    the value of the gradient of *f* at point **x^((*t*))**, that is, (∇*f*)(**x^((*t*))**),
    which is also an *n*-vector. You then move in the direction opposite to the gradient
    in each dimension at **x^((*t*))** to arrive at the next point **x^((*t*+1))**,
    which again is an *n*-vector. Because you moved in a monotonically decreasing
    direction in each dimension, you should have that *f*(**x^((*t*+1))**) ≤ *f*(**x^((*t*))**).
    Several details are needed to turn this idea into an actual algorithm. The two
    main details are that you need an initial point and that you need to decide how
    far to move in the direction of the negative gradient. You also need to understand
    when to stop and what you can conclude about the quality of the solution found.
    We will explore these issues further in this section, for both constrained minimization,
    where there are additional constraints on the points, and unconstrained minimization,
    where there are none.
  prefs: []
  type: TYPE_NORMAL
- en: '**Unconstrained gradient descent**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to gain intuition, let’s consider unconstrained gradient descent in
    just one dimension, that is, when *f* is a function of a scalar *x*, so that *f*
    : ℝ → ℝ. In this case, the gradient ∇*f* of *f* is just *f*′(*x*), the derivative
    of *f* with respect to *x*. Consider the function *f* shown in blue in [Figure
    33.3](chapter033.xhtml#Fig_33-3), with minimizer *x** and starting point *x*^((0)).
    The gradient (derivative) *f*′(*x*^((0))), shown in orange, has a negative slope,
    so that a small step from *x*^((0)) in the direction of increasing *x* results
    in a point *x*′ for which *f*(*x*′) < *f*(*x*^((0))). Too large a step, however,
    results in a'
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1378.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 33.3** A function *f* : ℝ → ℝ, shown in blue. Its gradient at point
    *x*^((0)), in orange, has a negative slope, and so a small increase in *x* from
    *x*^((0)) to *x*′ results in *f*(*x*′) < *f*(*x*^((0))). Small increases in *x*
    from *x*^((0)) head toward ![art](images/Art_P1379.jpg), which gives a local minimum.
    Too large an increase in *x* can end up at *x*″, where *f*(*x*″) > *f*(*x*^((0))).
    Small steps starting from *x*^((0)) and going only in the direction of decreasing
    values of *f* cannot end up at the global minimizer *x**.'
  prefs: []
  type: TYPE_NORMAL
- en: point *x*″ for which *f*(*x*″) > *f*(*x*^((0))), so this is a bad idea. Restricting
    ourselves to small steps, where each one has *f*(*x*′) < *f*(*x*), eventually
    results in getting close to point ![art](images/Art_P1380.jpg), which gives a
    local minimum. By taking only small downhill steps, however, gradient descent
    has no chance to get to the global minimizer *x**, given the starting point *x*^((0)).
  prefs: []
  type: TYPE_NORMAL
- en: We draw two observations from this simple example. First, gradient descent converges
    toward a local minimum, and not necessarily a global minimum. Second, the speed
    at which it converges and how it behaves are related to properties of the function,
    to the initial point, and to the step size of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The procedure GRADIENT-DESCENT on the facing page takes as input a function
    *f*, an initial point **x^((0))** ∈ ℝ^(*n*), a fixed step-size multiplier *γ*
    > 0, and a number *T* > 0 of steps to take. Each iteration of the **for** loop
    of lines 2–4 performs a step by computing the *n*-dimensional gradient at point
    **x^((*t*))** and then moving distance *γ* in the opposite direction in the *n*-dimensional
    space. The complexity of computing the gradient depends on the function *f* and
    can sometimes be expensive. Line 3 sums the points visited. After the loop terminates,
    line 6 returns **x-avg**, the average of all the points visited except for the
    last one, **x^((*T*))**. It might seem more natural to return **x^((*T*))**, and
    in fact, in many circumstances, you might prefer to have the function return **x^((*T*))**.
    For the version we will analyze, however, we use **x-avg**.
  prefs: []
  type: TYPE_NORMAL
- en: GRADIENT-DESCENT(*f*, **x^((0))**, *γ*, *T*)
  prefs: []
  type: TYPE_NORMAL
- en: '| 1 | **sum** = 0 | **//** *n*-dimensional vector, initially all 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | **for** *t* = 0 **to** *T* – 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | **sum** = **sum** + **x^((*t*))** | **//** add each of *n* dimensions
    into **sum** |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | **x^((*t*+1)) = x^((*t*)) – γ · (∇*f*)(x^((*t*)))** | **//** (∇*f*)(**x^((*t*))),
    x^((*t*+1))** are *n*-dimensional |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | **x-avg** = **sum**/*T* | **//** divide each of *n* dimensions by *T*
    |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | **return x-avg** |'
  prefs: []
  type: TYPE_TB
- en: '[Figure 33.4](chapter033.xhtml#Fig_33-4) depicts how gradient descent ideally
    runs on a convex 1-dimensional function.^([1](#footnote_1)) We’ll define convexity
    more formally below, but the figure shows that each iteration moves in the direction
    opposite to the gradient, with the distance moved being proportional to the magnitude
    of the gradient. As the iterations proceed, the magnitude of the gradient decreases,
    and thus the distance moved along the horizontal axis decreases. After each iteration,
    the distance to the optimal point **x*** decreases. This ideal behavior is not
    guaranteed to occur in general, but the analysis in the remainder of this section
    formalizes when this behavior occurs and quantifies the number of iterations needed.
    Gradient descent does not always work, however. We have already seen that if the
    function is not convex, gradient descent can converge to a local, rather than
    global, minimum. We have also seen that if the step size is too large, GRADIENT-DESCENT
    can overshoot the minimum and wind up farther away. (It is also possible to overshoot
    the minimum and wind up closer to the optimum.)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Analysis of unconstrained gradient descent for convex functions**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our analysis of gradient descent focuses on convex functions. Inequality (C.29)
    on page 1194 defines a convex function of one variable, as shown in [Figure 33.5](chapter033.xhtml#Fig_33-5).
    We can extend that definition to a function *f* : ℝ^(*n*) → ℝ and say that *f*
    is ***convex*** if for all **x**, **y** ∈ ℝ^(*n*) and for all 0 ≤ λ ≤ 1, we have'
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1381.jpg)'
  prefs: []
  type: TYPE_IMG
- en: (Inequalities (33.18) and (C.29) are the same, except for the dimensions of
    **x** and **y**.) We also assume that our convex functions are closed^([2](#footnote_2))
    and differentiable.
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1382.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 33.4** An example of running gradient descent on a convex function
    *f* : ℝ → ℝ, shown in blue. Beginning at point **x^((0))**, each iteration moves
    in the direction opposite to the gradient, and the distance moved is proportional
    to the magnitude of the gradient. Orange lines represent the negative of the gradient
    at each point, scaled by the step size *γ*. As the iterations proceed, the magnitude
    of the gradient decreases, and the distance moved decreases correspondingly. After
    each iteration, the distance to the optimal point **x*** decreases.'
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1383.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 33.5** A convex function *f* : ℝ → ℝ, shown in blue, with local and
    global minimizer **x***. Because *f* is convex, *f*(*λ***x** + (1 – *λ*)**y**)
    ≤ *λf*(**x**) + (1 – *λ*)*f*(**y**) for any two values **x** and **y** and all
    0 ≤ *λ* ≤ 1, shown for a particular value of *λ*. Here, the orange line segment
    represents all values *λf*(**x**) + (1 – *λ*)*f*(**y**) for 0 ≤ *λ* ≤ 1, and it
    is above the blue line.'
  prefs: []
  type: TYPE_NORMAL
- en: A convex function has the property that any local minimum is also a global minimum.
    To verify this property, consider inequality (33.18), and suppose for the purpose
    of contradiction that **x** is a local minimum but not a global minimum and **y**
    ≠ **x** is a global minimum, so *f*(**y**) < *f*(**x**). Then we have
  prefs: []
  type: TYPE_NORMAL
- en: '| *f*(*λ***x** + (1 – *λ*)**y**) | ≤ | *λf*(**x**) + (1 – *λ*)*f*(**y**) |
    (by inequality (33.18)) |'
  prefs: []
  type: TYPE_TB
- en: '|  | < | *λf*(**x**) + (1 – *λ*)*f*(**x**) |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | = | *f*(**x**). |  |'
  prefs: []
  type: TYPE_TB
- en: Thus, letting approach 1, we see that there is another point near **x**, say
    **x**′, such that *f*(**x**′) < *f*(**x**), so **x** is not a local minimum.
  prefs: []
  type: TYPE_NORMAL
- en: Convex functions have several useful properties. The first property, whose proof
    we leave as Exercise 33.3-1, says that a convex function always lies above its
    tangent hyperplane. In the context of gradient descent, angle brackets denote
    the notation for inner product defined on page 1219 rather than denoting a sequence.
  prefs: []
  type: TYPE_NORMAL
- en: '***Lemma 33.6***'
  prefs: []
  type: TYPE_NORMAL
- en: 'For any convex differentiable function *f* : ℝ^(*n*) → ℝ and for all *x*, *y*
    ∈ ℝ^(*n*), we have ≤ *f*(**x**) ≤ *f*(**y**) + 〈(∇*f*)(**x**), **x** – **y**〉.'
  prefs: []
  type: TYPE_NORMAL
- en: ▪
  prefs: []
  type: TYPE_NORMAL
- en: The second property, which Exercise 33.3-2 asks you to prove, is a repeated
    application of the definition of convexity in inequality (33.18).
  prefs: []
  type: TYPE_NORMAL
- en: '***Lemma 33.7***'
  prefs: []
  type: TYPE_NORMAL
- en: 'For any convex function *f* : ℝ^(*n*) → ℝ, for any integer *T* ≥ 1, and for
    all **x^((0))**, …, **x^((*T*–1))** ∈ ℝ^(*n*), we have'
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1384.jpg)'
  prefs: []
  type: TYPE_IMG
- en: ▪
  prefs: []
  type: TYPE_NORMAL
- en: The left-hand side of inequality (33.19) is the value of *f* at the vector **x-avg**
    that GRADIENT-DESCENT returns.
  prefs: []
  type: TYPE_NORMAL
- en: We now proceed to analyze GRADIENT-DESCENT. It might not return the exact global
    minimizer **x***. We use an error bound *ϵ*, and we want to choose *T* so that
    *f*(**x-avg**) – *f*(**x***) ≤ *ϵ* at termination. The value of *ϵ* depends on
    the number *T* of iterations and two additional values. First, since you expect
    it to be better to start close to the global minimizer, *ϵ* is a function of
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1385.jpg)'
  prefs: []
  type: TYPE_IMG
- en: the euclidean norm (or distance, defined on page 1219) of the difference between
    **x^((0))** and **x***. The error bound *ϵ* is also a function of a quantity we
    call *L*, which is an upper bound on the magnitude ∥(∇*f*)(**x**)∥ of the gradient,
    so that
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1386.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where **x** ranges over all the points **x^((0))**, …, **x^((*T*–1))** whose
    gradients are computed by GRADIENT-DESCENT. Of course, we don’t know the values
    of *L* and *R*, but for now let’s assume that we do. We’ll discuss later how to
    remove these assumptions. The analysis of GRADIENT-DESCENT is summarized in the
    following theorem.
  prefs: []
  type: TYPE_NORMAL
- en: '***Theorem 33.8***'
  prefs: []
  type: TYPE_NORMAL
- en: Let **x*** ∈ ℝ^(*n*) be the minimizer of a convex function *f*, and suppose
    that an execution of GRADIENT-DESCENT(*f*, **x^((0))**, *γ*, *T*) returns **x-avg**,
    where ![art](images/Art_P1387.jpg) and *R* and *L* are defined in equations (33.20)
    and (33.21). Let ![art](images/Art_P1388.jpg). Then we have *f*(**x-avg**) – *f*(**x***)
    ≤ *ϵ*.
  prefs: []
  type: TYPE_NORMAL
- en: ▪
  prefs: []
  type: TYPE_NORMAL
- en: We now prove this theorem. We do not give an absolute bound on how much progress
    each iteration makes. Instead, we use a potential function, as in [Section 16.3](chapter016.xhtml#Sec_16.3).
    Here, we define a potential Φ(*t*) after computing **x^((*t*))**, such that Φ(*t*)
    ≥ 0 for *t* = 0, …, *T*. We define the ***amortized progress*** in the iteration
    that computes **x^((*t*))** as
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1389.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Along with including the change in potential (Φ(*t* + 1) – Φ(*t*)), equation
    (33.22) also subtracts the minimum value *f*(**x***) because ultimately, you care
    not about the values *f*(**x^((*t*))**) but about how close they are to *f*(**x***).
    Suppose that we can show that *p*(*t*) ≤ *B* for some value *B* and *t* = 0, …,
    *T* – 1\. Then we can substitute for *p*(*t*) using equation (33.22), giving
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1390.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Summing inequality (33.23) over *t* = 0, …, *T* – 1 yields
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1391.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Observing that we have a telescoping series on the right and regrouping terms,
    we have that
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1392.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Dividing by *T* and dropping the positive term Φ(*T*) gives
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1393.jpg)'
  prefs: []
  type: TYPE_IMG
- en: and thus we have
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1394.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In other words, if we can show that *p*(*t*) ≤ *B* for some value *B* and choose
    a potential function where Φ(0) is not too large, then inequality (33.25) tells
    us how close the function value *f*(**x-avg**) is to the function value *f*(**x***)
    after *T* iterations. That is, we can set the error bound *ϵ* to *B* + Φ(0)/*T*.
  prefs: []
  type: TYPE_NORMAL
- en: In order to bound the amortized progress, we need to come up with a concrete
    potential function. Define the potential function Φ(*t*) by
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1395.jpg)'
  prefs: []
  type: TYPE_IMG
- en: that is, the potential function is proportional to the square of the distance
    between the current point and the minimizer **x***. With this potential function
    in hand, the next lemma provides a bound on the amortized progress made in any
    iteration of GRADIENT-DESCENT.
  prefs: []
  type: TYPE_NORMAL
- en: '***Lemma 33.9***'
  prefs: []
  type: TYPE_NORMAL
- en: Let **x*** ∈ ℝ^(*n*) be the minimizer of a convex function *f*, and consider
    an execution of GRADIENT-DESCENT(*f*, **x^((0))**, *γ*, *T*). Then for each point
    **x^((*t*))** computed by the procedure, we have that
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1396.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '***Proof***   We first bound the potential change Φ(*t* + 1) – Φ(*t*). Using
    the definition of Φ(*t*) from equation (33.26), we have'
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1397.jpg)'
  prefs: []
  type: TYPE_IMG
- en: From line 4 in GRADIENT-DESCENT, we know that
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1398.jpg)'
  prefs: []
  type: TYPE_IMG
- en: and so we would like to rewrite equation (33.27) to have **x^((*t*+1))** – **x^((*t*))**
    terms. As Exercise 33.3-3 asks you to prove, for any two vectors **a**, **b**
    ∈ ℝ^(*n*), we have
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1399.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Letting **a** = **x^((*t*))** – **x*** and **b** = **x^((*t*+1))** – **x^((*t*))**,
    we can write the right-hand side of equation (33.27) as ![art](images/Art_P1400.jpg).
    Then we can express the potential change as
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1401.jpg)'
  prefs: []
  type: TYPE_IMG
- en: and thus we have
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1402.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can now proceed to bound *p*(*t*). By the bound on the potential change from
    inequality (33.31), and using the definition of *L* (inequality (33.21)), we have
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1403.jpg)'
  prefs: []
  type: TYPE_IMG
- en: ▪
  prefs: []
  type: TYPE_NORMAL
- en: sult in the following theorem
  prefs: []
  type: TYPE_NORMAL
- en: Having bounded the amortized progress in one step, we now analyze the entire
    GRADIENT-DESCENT procedure, completing the proof of Theorem 33.8.
  prefs: []
  type: TYPE_NORMAL
- en: '***Proof of Theorem 33.8*** Inequality (33.25) tells us that if we have an
    upper bound of *B* for *p*(*t*), then we also have the bound *f*(**x-avg**) –
    *f*(**x***) ≤ *B* + Φ(0)/*T*. By equations (33.20) and (33.26), we have that Φ(0)
    = *R*²/(2*γ*). Lemma 33.9 gives us the upper bound of *B* = *γL*²/2, and so we
    have'
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1404.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Our choice of ![art](images/Art_P1405.jpg) in the statement of Theorem 33.8
    balances the two terms, and we obtain
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1406.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Since we chose ![art](images/Art_P1407.jpg) in the theorem statement, the proof
    is complete.
  prefs: []
  type: TYPE_NORMAL
- en: ▪
  prefs: []
  type: TYPE_NORMAL
- en: Continuing under the assumption that we know *R* (from equation (33.20)) and
    *L* (from inequality (33.21)), we can think of the analysis in a slightly different
    way. We can presume that we have a target accuracy *ϵ* and then compute the number
    of iterations needed. That is, we can solve ![art](images/Art_P1408.jpg) for *T*,
    obtaining *T* = *R*²*L*²/*ϵ*². The number of iterations thus depends on the square
    of *R* and *L* and, most importantly, on 1/*ϵ*². (The definition of *L* from inequality
    (33.21) depends on *T*, but we may know an upper bound on *L* that doesn’t depend
    on the particular value of *T*.) Thus, if you want to halve your error bound,
    you need to run four times as many iterations.
  prefs: []
  type: TYPE_NORMAL
- en: It is quite possible that we don’t really know *R* and *L*, since you’d need
    to know **x*** in order to know *R* (since *R* = ∥**x^((0))** – **x***∥), and
    you might not have an explicit upper bound on the gradient, which would provide
    *L*. You can, however, interpret the analysis of gradient descent as a proof that
    there is some step size for which the procedure makes progress toward the minimum.
    You can then compute a step size for which *f*(**x^((*t*))**) – *f*(**x^((*t*+1))**)
    is large enough. In fact, not having a fixed step size multiplier can actually
    help in practice, as you are free to use any step size *s* that achieves sufficient
    decrease in the value of *f*. You can search for a step size that achieves a large
    decrease via a binary-search-like routine, which is often called ***line search***.
    For a given function *f* and step size *s*, define the function *g*(**x^((*t*))**,
    *s*) = *f*(**x^((*t*))**) – *s*(∇*f*)(**x^((*t*))**). Start with a small step
    size *s* for which *g*(**x^((*t*))**, *s*) ≤ *f*(**x^((*t*))**). Then repeatedly
    double *s* until *g*(**x^((*t*))**, 2*s*) ≥ *g*(**x^((*t*))**, *s*), and then
    perform a binary search in the interval [*s*, 2*s*]. This procedure can produce
    a step size that achieves a significant decrease in the objective function. In
    other circumstances, however, you may know good upper bounds on *R* and *L*, typically
    from problem-specific information, which can suffice.
  prefs: []
  type: TYPE_NORMAL
- en: The dominant computational step in each iteration of the **for** loop of lines
    2–4 is computing the gradient. The complexity of computing and evaluating a gradient
    varies widely, depending on the application at hand. We’ll discuss several applications
    later.
  prefs: []
  type: TYPE_NORMAL
- en: '**Constrained gradient descent**'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can adapt gradient descent for constrained minimization to minimize a closed
    convex function *f*(**x**), subject to the additional requirement that **x** ∈
    *K*, where *K* is a closed convex body. A ***body*** *K* ⊆ ℝ^(*n*) is ***convex***
    if for all **x**, **y** ∈ *K*, the convex combination *λ***x**+(1–*λ*)**y** ∈
    *K* for all 0 ≤ *λ* ≤ 1\. A ***closed*** convex body contains its limit points.
    Somewhat surprisingly, restricting to the constrained problem does not significantly
    increase the number of iterations of gradient descent. The idea is that you run
    the same algorithm, but in each iteration, check whether the current point **x^((*t*))**
    is still within the convex body *K*. If it is not, just move to the closest point
    in *K*. Moving to the closest point is known as ***projection***. We formally
    define the projection ∏[*K*](**x**) of a point **x** in *n* dimensions onto a
    convex body *K* as the point **y** ∈ *K* such that ∥**x** – **y**∥ = min {∥**x**
    – **z**∥ : *z* ∈ *K*}. If we have **x** ∈ *K*, then ∏[*K*](**x**) = **x**.'
  prefs: []
  type: TYPE_NORMAL
- en: This one change yields the procedure GRADIENT-DESCENT-CONSTRAINED, in which
    line 4 of GRADIENT-DESCENT is replaced by two lines. It assumes that **x^((0))**
    ∈ *K*. Line 4 of GRADIENT-DESCENT-CONSTRAINED moves in the direction of the negative
    gradient, and line 5 projects back onto *K*. The lemma that follows helps to show
    that when **x*** ∈ *K*, if the projection step in line 5 moves from a point outside
    of *K* to a point in *K*, it cannot be moving away from **x***.
  prefs: []
  type: TYPE_NORMAL
- en: GRADIENT-DESCENT-CONSTRAINED(*f*, **x^((0))**, *γ*, *T*, *K*)
  prefs: []
  type: TYPE_NORMAL
- en: '| 1 | **sum** = 0 | **//** *n*-dimensional vector, initially all 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | **for** *t* = 0 **to** *T* – 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | **sum** = **sum** + **x^((*t*))** | **//** add each of *n* dimensions
    into **sum** |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | **x′**^((*t*+1)) = **x^((*t*))** – *γ* · (∇*f*)(**x^((*t*))**) | **//**
    (∇*f*)(**x^((*t*))**), **x′^((*t*+1))** are *n*-dimensional |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | **x^((*t*+1))** = ∏[*K*](**x^((*t*+1))**) | **//** project onto *K* |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | **x-avg** = **sum**/*T* | **//** divide each of *n* dimensions by *T*
    |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | **return x-avg** |'
  prefs: []
  type: TYPE_TB
- en: '***Lemma 33.10***'
  prefs: []
  type: TYPE_NORMAL
- en: Consider a convex body *K* ⊆ ℝ^(*n*) and points **a** ∈ *K* and **b**′ ∈ ℝ^(*n*).
    Let **b** = ∏[*K*](**b**′). Then ∥**b** – **a**∥² ≤ ∥**b**′ – **a**∥².
  prefs: []
  type: TYPE_NORMAL
- en: '***Proof***   If **b**′ ∈ *K*, then **b** = **b**′ and the claim is true. Otherwise,
    **b**′ ≠ **b**, and as [Figure 33.6](chapter033.xhtml#Fig_33-6) shows, we can
    extend the line segment between **b** and **b**′ to a line ℓ. Let **c** be the
    projection of **a** onto ℓ. Point **c** may or may not be in *K*, and if **a**
    is on the boundary of *K*, then **c** could coincide with **b**. If **c** coincides
    with **b** (part (c) of the figure), then **abb**′ is a right triangle, and so
    ∥**b** – **a**∥² ≤ ∥**b**′ – **a**∥². If **c** does not coincide with **b** (parts
    (a) and (b) of the figure), then because of convexity, the angle ∠**abb**′ must
    be obtuse. Because angle ∠**abb**′ is obtuse, **b** lies between **c** and **b**′
    on ℓ. Furthermore, because **c** is the projection of **a** onto line ℓ, **acb**
    and **acb**′ must be right triangles. By the Pythagorean theorem, we have that
    ∥b′ – **a**∥² = ∥**a** – **c**∥²+∥**c** – **b**′∥² and ∥**b** – **a**∥² = ∥**a**
    – **c**∥²+∥**c** – **b**∥². Subtracting these two equations gives ∥**b**′ – **a**∥²
    – ∥**b** – **a**∥² = ∥**c** – **b**′∥² – ∥**c** – **b**∥². Because **b** is between
    **c** and **b**′, we must have ∥**c** – **b**′∥² ≥ ∥**c** – **b**∥², and thus
    ∥**b**′ – **a**∥² – ∥**b** – **a**∥² ≥ 0\. The lemma follows.'
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1409.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 33.6** Projecting a point **b**′ outside the convex body *K* to the
    closest point **b** = ∏[*K*](**b**′) in *K*. Line ℓ is the line containing **b**
    and **b**′, and point **c** is the projection of **a** onto ℓ. **(a)** When **c**
    is in *K*. **(b)** When **c** is not in *K*. **(c)** When **a** is on the boundary
    of *K* and **c** coincides with **b**.'
  prefs: []
  type: TYPE_NORMAL
- en: ▪
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now repeat the entire proof for the unconstrained case and obtain the
    same bounds. Lemma 33.10 with **a** = **x***, **b** = **x^((*t*+1))**, and **b**′
    = **x′^((*t*+1))** tells us that ∥**x^((*t*+1))**–**x***∥² ≤ ∥**x′^((*t*+1))**–**x***∥².
    We can therefore derive an upper bound that matches inequality (33.31). We continue
    to define Φ(*t*) as in equation (33.26), but noting that **x^((*t*+1))**, computed
    in line 5 of GRADIENT-DESCENT-CONSTRAINED, has a different meaning here from in
    inequality (33.31):'
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1410.jpg)'
  prefs: []
  type: TYPE_IMG
- en: With the same upper bound on the change in the potential function as in equation
    (33.30), the entire proof of Lemma 33.9 can proceed as before. We can therefore
    conclude that the procedure GRADIENT-DESCENT-CONSTRAINED has the same asymptotic
    complexity as GRADIENT-DESCENT. We summarize this result in the following theorem.
  prefs: []
  type: TYPE_NORMAL
- en: '***Theorem 33.11***'
  prefs: []
  type: TYPE_NORMAL
- en: Let *K* ⊆ ℝ^(*n*) be a convex body, **x*** ∈ ℝ^(*n*) be the minimizer of a convex
    function *f* over *K*, and ![art](images/Art_P1411.jpg), where *R* and *L* are
    defined in equations (33.20) and (33.21). Suppose that the vector **x-avg** is
    returned by an execution of GRADIENT-DESCENT-CONSTRAINED(*f*, **x^((0))**, *γ*,
    *T*, *K*). Let ![art](images/Art_P1412.jpg). Then we have *f*(**x-avg**) – *f*(**x***)
    ≤ *ϵ*.
  prefs: []
  type: TYPE_NORMAL
- en: ▪
  prefs: []
  type: TYPE_NORMAL
- en: '**Applications of gradient descent**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Gradient descent has many applications to minimizing functions and is widely
    used in optimization and machine learning. Here we sketch how it can be used to
    solve linear systems. Then we discuss an application to machine learning: prediction
    using linear regression.'
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 28](chapter028.xhtml), we saw how to use Gaussian elimination to
    solve a system of linear equations *A***x** = **b**, thereby computing **x** =
    *A*^(−1)**b**. If *A* is an *n* × *n* matrix and **b** is a length-*n* vector,
    then the running time of Gaussian elimination is Θ(*n*³), which for large matrices
    might be prohibitively expensive. If an approximate solution is acceptable, however,
    you can use gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s see how to use gradient descent as a roundabout—and admittedly
    inefficient—way to solve for *x* in the scalar equation *ax* = *b*, where *a*,
    *x*, *b* ∈ ℝ. This equation is equivalent to *ax* – *b* = 0\. If *ax* – *b* is
    the derivative of a convex function *f*(*x*), then *ax* – *b* = 0 for the value
    of *x* that minimizes *f*(*x*). Given *f*(*x*), gradient descent can then determine
    this minimizer. Of course, *f*(*x*) is just the integral of *ax* – *b*, that is,
    ![art](images/Art_P1413.jpg), which is convex if *a* ≥ 0\. Therefore, one way
    to solve *ax* = *b* for *a* ≥ 0 is to find the minimizer for ![art](images/Art_P1414.jpg)
    via gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: We now generalize this idea to higher dimensions, where using gradient descent
    may actually lead to a faster algorithm. One *n*-dimensional analog is the function
    ![art](images/Art_P1415.jpg), where *A* is an *n* × *n* matrix. The gradient of
    *f* with respect to **x** is the function *A***x** – **b**. To find the value
    of **x** that minimizes *f*, we set the gradient of *f* to 0 and solve for **x**.
    Solving *A***x**–**b** = 0 for **x**, we obtain **x** = *A*^(−1)**b**, Thus, minimizing
    *f*(**x**) is equivalent to solving *A***x** = **b**. If *f*(**x**) is convex,
    then gradient descent can approximately compute this minimum.
  prefs: []
  type: TYPE_NORMAL
- en: 'A 1-dimensional function is convex when its second derivative is positive.
    The equivalent definition for a multidimensional function is that it is convex
    when its Hessian matrix is positive-semidefinite (see page 1222 for a definition),
    where the ***Hessian matrix*** (∇²*f*)(**x**) of a function *f*(**x**) is the
    matrix in which entry (*i*, *j*) is the partial derivative of *f* with respect
    to *i* and *j*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1416.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Analogous to the 1-dimensional case, the Hessian of *f* is just *A*, and so
    if *A* is a positive-semidefinite matrix, then we can use gradient descent to
    find a point **x** where *A***x** ≈ **b**. If *R* and *L* are not too large, then
    this method is faster than using Gaussian elimination.
  prefs: []
  type: TYPE_NORMAL
- en: '**Gradient descent in machine learning**'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a concrete example of supervised learning for prediction, suppose that you
    want to predict whether a patient will develop heart disease. For each of *m*
    patients, you have *n* different attributes. For example, you might have *n* =
    4 and the four pieces of data are age, height, blood pressure, and number of close
    family members with heart disease. Denote the data for patient *i* as a vector
    **x^((*i*))** ∈ ℝ^(*n*), with ![art](images/Art_P1417.jpg) giving the *j*th entry
    in vector **x^((*i*))**. The ***label*** of patient *i* is denoted by a scalar
    *y*^((*i*)) ∈ ℝ, signifying the severity of the patient’s heart disease. The hypothesis
    should capture a relationship between the **x^((*i*))** values and *y*^((*i*)).
    For this example, we make the modeling assumption that the relationship is linear,
    and therefore the goal is to compute the “best” linear relationship between the
    **x^((*i*))** values and *y*^((*i*)): a linear function *f* : ℝ^(*n*) → ℝ such
    that *f*(**x^((*i*))**) ≈ *y*^((*i*)) for each patient *i*. Of course, no such
    function may exist, but you would like one that comes as close as possible. A
    linear function *f* can be defined by a vector of weights **w** = (*w*[0], *w*[1],
    …, *w*[*n*]), with'
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1418.jpg)'
  prefs: []
  type: TYPE_IMG
- en: When evaluating a machine-learning model, you need to measure how close each
    value *f*(**x^((*i*))**) is to its corresponding label *y*^((*i*)). In this example,
    we define the error *e*^((*i*)) ∈ ℝ associated with patient *i* as *e*^((*i*))
    = *f*(**x^((*i*))**) – *y*^((*i*)). The objective function we choose is to minimize
    the sum of squares of the errors, which is
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1419.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The objective function is typically called the ***loss function***, and the
    ***least-squares error*** given by equation (33.33) is just one example of many
    possible loss functions. The goal is then, given the **x^((*i*))** and *y*^((*i*))
    values, to compute the weights *w*[0], *w*[1], …, *w*[*n*] so as to minimize the
    loss function in equation (33.33). The variables here are the weights *w*[0],
    *w*[1], …, *w*[*n*] and not the **x^((*i*))** or *y*^((*i*)) values.
  prefs: []
  type: TYPE_NORMAL
- en: This particular objective is sometimes known as a ***least-squares fit***, and
    the problem of finding a linear function to fit data and minimize the least-squares
    error is called ***linear regression***. Finding a least-squares fit is also addressed
    in [Section 28.3](chapter028.xhtml#Sec_28.3).
  prefs: []
  type: TYPE_NORMAL
- en: When the function *f* is linear, the loss function defined in equation (33.33)
    is convex, because it is the sum of squares of linear functions, which are themselves
    convex. Therefore, we can apply gradient descent to compute a set of weights to
    approximately minimize the least-squares error. The concrete goal of learning
    is to be able to make predictions on new data. Informally, if the features are
    all reported in the same units and are from the same range (perhaps from being
    normalized), then the weights tend to have a natural interpretation because the
    features of the data that are better predictors of the label have a larger associated
    weight. For example, you would expect that, after normalization, the weight associated
    with the number of family members with heart disease would be larger than the
    weight associated with height.
  prefs: []
  type: TYPE_NORMAL
- en: The computed weights form a model of the data. Once you have a model, you can
    make predictions, so that given new data, you can predict its label. In our example,
    given a new patient **x**′ who is not part of the original training data set,
    you would still hope to predict the chance that the new patient develops heart
    disease. You can do so by computing the label *f*(**x**′), incorporating the weights
    computed by gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: For this linear-regression problem, the objective is to minimize the expression
    in equation (33.33), which is a quadratic in each of the *n*+1 weights *w*[*j*].
    Thus, entry *j* in the gradient is linear in *w*[*j*]. Exercise 33.3-5 asks you
    to explicitly compute the gradient and see that it can be computed in *O*(*nm*)
    time, which is linear in the input size. Compared with the exact method of solving
    equation (33.33) in [Chapter 28](chapter028.xhtml), which needs to invert a matrix,
    gradient descent is typically much faster.
  prefs: []
  type: TYPE_NORMAL
- en: '[Section 33.1](chapter033.xhtml#Sec_33.1) briefly discussed regularization—the
    idea that a complicated hypothesis should be penalized in order to avoid overfitting
    the training data. Regularization often involves adding a term to the objective
    function, but it can also be achieved by adding a constraint. One way to regularize
    this example would be to explicitly limit the norm of the weights, adding a constraint
    that ∥**w**∥ ≤ *B* for some bound *B* > 0\. (Recall again that the components
    of the vector **w** are the variables in the present application.) Adding this
    constraint controls the complexity of the model, as the number of values *w*[*j*]
    that can have large absolute value is now limited.'
  prefs: []
  type: TYPE_NORMAL
- en: In order to run GRADIENT-DESCENT-CONSTRAINED for any problem, you need to implement
    the projection step, as well as to compute bounds on *R* and *L*. We conclude
    this section by describing these calculations for gradient descent with the constraint
    ∥**w**∥ ≤ *B*. First, consider the projection step in line 5\. Suppose that the
    update in line 4 results in a vector **w**′. The projection is implemented by
    computing ∏[*k*](**w**′) where *K* is defined by ∥**w**∥ ≤ *B*. This particular
    projection can be accomplished by simply scaling **w**′, since we know that closest
    point in *K* to **w**′ must be the point along the vector whose norm is exactly
    *B*. The amount *z* by which we need to scale **w**′ to hit the boundary of *K*
    is the solution to the equation *z* ∥**w**′∥ = *B*, which is solved by *z* = *B*/∥**w**′∥.
    Hence line 5 is implemented by computing **w** = **w**′*B*/∥**w**′∥. Because we
    always have ∥**w**∥ ≤ *B*, Exercise 33.3-6 asks you to show that the upper bound
    on the magnitude *L* of the gradient is *O*(*B*). We also get a bound on *R*,
    as follows. By the constraint ∥**w**∥ ≤ *B*, we know that both ∥**w^((0))**∥ ≤
    *B* and ∥**w***∥ ≤ *B*, and thus ∥**w^((0))** – **w***∥ ≤ 2*B*. Using the definition
    of *R* in equation (33.20), we have *R* = *O*(*B*). The bound ![art](images/Art_P1420.jpg)
    on the accuracy of the solution after *T* iterations in Theorem 33.11 becomes
    ![art](images/Art_P1421.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercises**'
  prefs: []
  type: TYPE_NORMAL
- en: '***33.3-1***'
  prefs: []
  type: TYPE_NORMAL
- en: Prove Lemma 33.6\. Start from the definition of a convex function given in equation
    (33.18). (*Hint:* You can prove the statement when *n* = 1 first. The proof for
    general values of *n* is similar.)
  prefs: []
  type: TYPE_NORMAL
- en: '***33.3-2***'
  prefs: []
  type: TYPE_NORMAL
- en: Prove Lemma 33.7.
  prefs: []
  type: TYPE_NORMAL
- en: '***33.3-3***'
  prefs: []
  type: TYPE_NORMAL
- en: Prove equation (33.29). (*Hint:* The proof for *n* = 1 dimension is straightforward.
    The proof for general values of *n* dimensions follows along similar lines.)
  prefs: []
  type: TYPE_NORMAL
- en: '***33.3-4***'
  prefs: []
  type: TYPE_NORMAL
- en: Show that the function *f* in equation (33.32) is a convex function of the variables
    *w*[0], *w*[1], …, *w*[*n*].
  prefs: []
  type: TYPE_NORMAL
- en: '***33.3-5***'
  prefs: []
  type: TYPE_NORMAL
- en: Compute the gradient of expression (33.33) and explain how to evaluate the gradient
    in *O*(*nm*) time.
  prefs: []
  type: TYPE_NORMAL
- en: '***33.3-6***'
  prefs: []
  type: TYPE_NORMAL
- en: Consider the function *f* defined in equation (33.32), and suppose that you
    have a bound ∥**w**∥ ≤ *B*, as is considered in the discussion on regularization.
    Show that *L* = *O*(*B*) in this case.
  prefs: []
  type: TYPE_NORMAL
- en: '***33.3-7***'
  prefs: []
  type: TYPE_NORMAL
- en: Equation (33.2) on page 1009 gives a function that, when minimized, gives an
    optimal solution to the *k*-means problem. Explain how to use gradient descent
    to solve the *k*-means problem.
  prefs: []
  type: TYPE_NORMAL
- en: '**Problems**'
  prefs: []
  type: TYPE_NORMAL
- en: '***33-1     Newton’s method***'
  prefs: []
  type: TYPE_NORMAL
- en: 'Gradient descent iteratively moves closer to a desired value (the minimum)
    of a function. Another algorithm in this spirit is known as ***Newton’s method***,
    which is an iterative algorithm that finds the root of a function. Here, we consider
    Newton’s method which, given a function *f* : ℝ → ℝ, finds a value *x** such that
    *f*(*x** ) = 0\. The algorithm moves through a series of points *x*^((0)), *x*^((1)),
    …. If the algorithm is currently at a point *x*^((*t*)), then to find point *x*^((*t*+1)),
    it first takes the equation of the line tangent to the curve at *x* = *x*^((*t*)),'
  prefs: []
  type: TYPE_NORMAL
- en: '*y* = *f*′(*x*^((*t*)))(*x* – *x*^((*t*))) + *f*(*x*^((*t*))).'
  prefs: []
  type: TYPE_NORMAL
- en: It then uses the *x*-intercept of this line as the next point *x*^((*t*+1)).
  prefs: []
  type: TYPE_NORMAL
- en: '***a.*** Show that the algorithm described above can be summarized by the update
    rule'
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1422.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We restrict our attention to some domain *I* and assume that *f*′(*x*) ≠ 0 for
    all *x* ∈ *I* and that *f*″(*x*) is continuous. We also assume that the starting
    point *x*^((0)) is sufficiently close to *x**, where “sufficiently close” means
    that we can use only the first two terms of the Taylor expansion of *f*(*x**)
    about *x*^((0)), namely
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1423.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where *γ*^((0)) is some value between *x*^((0)) and *x**. If the approximation
    in equation (33.34) holds for *x*^((0)), it also holds for any point closer to
    *x**.
  prefs: []
  type: TYPE_NORMAL
- en: '***b.*** Assume that the function *f* has exactly one point *x** for which
    *f*(*x**) = 0\. Let *ϵ*^((*t*)) = |*x*^((*t*)) – *x**|. Using the Taylor expansion
    in equation (33.34), show that'
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1424.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where *γ*^((*t*)) is some value between *x*^((*t*)) and *x**.
  prefs: []
  type: TYPE_NORMAL
- en: '***c.*** If'
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P1425.jpg)'
  prefs: []
  type: TYPE_IMG
- en: for some constant *c* and *ϵ*^((0)) < 1, then we say that the function *f* has
    ***quadratic convergence***, since the error decreases quadratically. Assuming
    that *f* has quadratic convergence, how many iterations are needed to find a root
    of *f*(*x*) to an accuracy of *δ*? Your answer should include *δ*.
  prefs: []
  type: TYPE_NORMAL
- en: '***d.*** Suppose you wish to find a root of the function *f*(*x*) = (*x* –
    3)², which is also the minimizer, and you start at *x*^((0)) = 3.5\. Compare the
    number of iterations needed by gradient descent to find the minimizer and Newton’s
    method to find the root.'
  prefs: []
  type: TYPE_NORMAL
- en: '***33-2     Hedge***'
  prefs: []
  type: TYPE_NORMAL
- en: Another variant in the multiplicative-weights framework is known as HEDGE. It
    differs from WEIGHTED MAJORITY in two ways. First, HEDGE makes the prediction
    randomly—in iteration *t*, it assigns a probability ![art](images/Art_P1426.jpg)
    to expert *E*[*i*], where ![art](images/Art_P1427.jpg). It then chooses an expert
    *E*[*i*′] according to this probability distribution and predicts according to
    *E*[*i*′]. Second, the update rule is different. If an expert makes a mistake,
    line 16 updates that expert’s weight by the rule ![art](images/Art_P1428.jpg),
    for some 0 < *ϵ* < 1\. Show that the expected number of mistakes made by HEDGE,
    running for *T* rounds, is at most *m** + (ln *n*)/*ϵ* + *ϵT*.
  prefs: []
  type: TYPE_NORMAL
- en: '***33-3     Nonoptimality of Lloyd’s procedure in one dimension***'
  prefs: []
  type: TYPE_NORMAL
- en: Give an example to show that even in one dimension, Lloyd’s procedure for finding
    clusters does not always return an optimum result. That is, Lloyd’s procedure
    may terminate and return as a result a set *C* of clusters that does not minimize
    *f*(*S*, *C*), even when *S* is a set of points on a line.
  prefs: []
  type: TYPE_NORMAL
- en: '***33-4     Stochastic gradient descent***'
  prefs: []
  type: TYPE_NORMAL
- en: Consider the problem described in [Section 33.3](chapter033.xhtml#Sec_33.3)
    of fitting a line *f*(*x*) = *ax* + *b* to a given set of point/value pairs *S*
    = {(*x*[1], *y*[1]), …, (*x*[*T*], *y*[*T*])} by optimizing the choice of the
    parameters *a* and *b* using gradient descent to find a best least-squares fit.
    Here we consider the case where *x* is a real-valued variable, rather than a vector.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that you are not given the point/value pairs in *S* all at once, but
    only one at a time in an online manner. Furthermore, the points are given in random
    order. That is, you know that there are *n* points, but in iteration *t* you are
    given only (*x*[*i*], *y*[*i*]) where *i* is independently and randomly chosen
    from {1, …, *T*}.
  prefs: []
  type: TYPE_NORMAL
- en: You can use gradient descent to compute an estimate to the function. As each
    point (*x*[*i*], *y*[*i*]) is considered, you can update the current values of
    *a* and *b* by taking the derivative with respect to *a* and *b* of the term of
    the objective function depending on (*x*[*i*], *y*[*i*]). Doing so gives you a
    stochastic estimate of the gradient, and you can then take a small step in the
    opposite direction.
  prefs: []
  type: TYPE_NORMAL
- en: Give pseudcode to implement this variant of gradient descent. What would the
    expected value of the error be as a function of *T*, *L*, and *R*? (*Hint:* Replicate
    the analysis of GRADIENT-DESCENT in [Section 33.3](chapter033.xhtml#Sec_33.3)
    for this variant.)
  prefs: []
  type: TYPE_NORMAL
- en: This procedure and its variants are known as ***stochastic gradient descent***.
  prefs: []
  type: TYPE_NORMAL
- en: '**Chapter notes**'
  prefs: []
  type: TYPE_NORMAL
- en: For a general introduction to artificial intelligence, we recommend Russell
    and Norvig [[391](bibliography001.xhtml#endnote_391)]. For a general introduction
    to machine learning, we recommend Murphy [[340](bibliography001.xhtml#endnote_340)].
  prefs: []
  type: TYPE_NORMAL
- en: Lloyd’s procedure for the *k*-means problem was first proposed by Lloyd [[304](bibliography001.xhtml#endnote_304)]
    and also later by Forgy [[151](bibliography001.xhtml#endnote_151)]. It is sometimes
    called “Lloyd’s algorithm” or the “Lloyd-Forgy algorithm.” Although Mahajan et
    al. [[310](bibliography001.xhtml#endnote_310)] showed that finding an optimal
    clustering is NP-hard, even in the plane, Kanungo et al. [[241](bibliography001.xhtml#endnote_241)]
    have shown that there is an approximation algorithm for the *k*-means problem
    with approximation ratio 9 + *ϵ*, for any *ϵ* > 0.
  prefs: []
  type: TYPE_NORMAL
- en: The multiplicative-weights method is surveyed by Arora, Hazan, and Kale [[25](bibliography001.xhtml#endnote_25)].
    The main idea of updating weights based on feedback has been rediscovered many
    times. One early use is in game theory, where Brown defined “Fictitious Play”
    [[74](bibliography001.xhtml#endnote_74)] and conjectured its convergence to the
    value of a zero-sum game. The convergence properties were established by Robinson
    [[382](bibliography001.xhtml#endnote_382)].
  prefs: []
  type: TYPE_NORMAL
- en: In machine learning, the first use of multiplicative weights was by Littlestone
    in the Winnow algorithm [[300](bibliography001.xhtml#endnote_300)], which was
    later extended by Littlestone and Warmuth to the weighted-majority algorithm described
    in [Section 33.2](chapter033.xhtml#Sec_33.2) [[301](bibliography001.xhtml#endnote_301)].
    This work is closely connected to the boosting algorithm, originally due to Freund
    and Shapire [[159](bibliography001.xhtml#endnote_159)]. The multiplicative-weights
    idea is also closely related to several more general optimization algorithms,
    including the perceptron algorithm [[328](bibliography001.xhtml#endnote_328)]
    and algorithms for optimization problems such as packing linear programs [[177](bibliography001.xhtml#endnote_177),
    [359](bibliography001.xhtml#endnote_359)].
  prefs: []
  type: TYPE_NORMAL
- en: The treatment of gradient descent in this chapter draws heavily on the unpublished
    manuscript of Bansal and Gupta [[35](bibliography001.xhtml#endnote_35)]. They
    emphasize the idea of using a potential function and using ideas from amortized
    analysis to explain gradient descent. Other presentations and analyses of gradient
    descent include works by Bubeck [[75](bibliography001.xhtml#endnote_75)], Boyd
    and Vanderberghe [[69](bibliography001.xhtml#endnote_69)], and Nesterov [[343](bibliography001.xhtml#endnote_343)].
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent is known to converge faster when functions obey stronger properties
    than general convexity. For example, a function *f* is ***α-strongly convex***
    if *f*(**y**) ≥ *f*(**x**) + 〈(∇*f*)(**x**), (**y** – **x**)〉 + *α*∥**y** – **x**∥
    for all **x**, **y** ∈ ℝ^(*n*). In this case, GRADIENT-DESCENT can use a variable
    step size and return **x^((*T*))**. The step size at step *t* becomes *γ*[*t*]
    = 1/(*α*(*t* + 1)), and the procedure returns a point such that *f*(**x-avg**)
    – *f*(**x***) ≤ *L*²/(*α*(*T* + 1)). This convergence is better than that of Theorem
    33.8 because the number of iterations needed is linear, rather than quadratic,
    in the desired error parameter *ϵ*, and because the performance is independent
    of the initial point.
  prefs: []
  type: TYPE_NORMAL
- en: Another case in which gradient descent can be shown to perform better than the
    analysis in [Section 33.3](chapter033.xhtml#Sec_33.3) suggests is for smooth convex
    functions. We say that a function is ***β-smooth*** if ![art](images/Art_P1429.jpg).
    This inequality goes in the opposite direction from the one for *≈*-strong convexity.
    Better bounds on gradient descent are possible here as well.
  prefs: []
  type: TYPE_NORMAL
- en: '[¹](#footnote_ref_1) Although the curve in [Figure 33.4](chapter033.xhtml#Fig_33-4)
    looks concave, according to the definition of convexity that we’ll see below,
    the function *f* in the figure is convex.'
  prefs: []
  type: TYPE_NORMAL
- en: '[²](#footnote_ref_2) A function *f* : ℝ^(*n*) → ℝ is closed if, for each *α*
    ∈ ℝ, the set {**x** ∈ dom(*f*) : *f*(**x**) ≤ *α*} is closed, where dom(*f*) is
    the domain of *f*.'
  prefs: []
  type: TYPE_NORMAL
