- en: '[**15        Greedy Algorithms**](toc.xhtml#chap-15)'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms for optimization problems typically go through a sequence of steps,
    with a set of choices at each step. For many optimization problems, using dynamic
    programming to determine the best choices is overkill, and simpler, more efficient
    algorithms will do. A ***greedy algorithm*** always makes the choice that looks
    best at the moment. That is, it makes a locally optimal choice in the hope that
    this choice leads to a globally optimal solution. This chapter explores optimization
    problems for which greedy algorithms provide optimal solutions. Before reading
    this chapter, you should read about dynamic programming in [Chapter 14](chapter014.xhtml),
    particularly [Section 14.3](chapter014.xhtml#Sec_14.3).
  prefs: []
  type: TYPE_NORMAL
- en: 'Greedy algorithms do not always yield optimal solutions, but for many problems
    they do. We first examine, in [Section 15.1](chapter015.xhtml#Sec_15.1), a simple
    but nontrivial problem, the activity-selection problem, for which a greedy algorithm
    efficiently computes an optimal solution. We’ll arrive at the greedy algorithm
    by first considering a dynamic-programming approach and then showing that an optimal
    solution can result from always making greedy choices. [Section 15.2](chapter015.xhtml#Sec_15.2)
    reviews the basic elements of the greedy approach, giving a direct approach for
    proving greedy algorithms correct. [Section 15.3](chapter015.xhtml#Sec_15.3) presents
    an important application of greedy techniques: designing data-compression (Huffman)
    codes. Finally, [Section 15.4](chapter015.xhtml#Sec_15.4) shows that in order
    to decide which blocks to replace when a miss occurs in a cache, the “furthest-in-future”
    strategy is optimal if the sequence of block accesses is known in advance.'
  prefs: []
  type: TYPE_NORMAL
- en: The greedy method is quite powerful and works well for a wide range of problems.
    Later chapters will present many algorithms that you can view as applications
    of the greedy method, including minimum-spanning-tree algorithms ([Chapter 21](chapter021.xhtml)),
    Dijkstra’s algorithm for shortest paths from a single source ([Section 22.3](chapter022.xhtml#Sec_22.3)),
    and a greedy set-covering heuristic ([Section 35.3](chapter035.xhtml#Sec_35.3)).
    Minimum-spanning-tree algorithms furnish a classic example of the greedy method.
    Although you can read this chapter and [Chapter 21](chapter021.xhtml) independently
    of each other, you might find it useful to read them together.
  prefs: []
  type: TYPE_NORMAL
- en: '[**15.1    An activity-selection problem**](toc.xhtml#Rh1-88)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our first example is the problem of scheduling several competing activities
    that require exclusive use of a common resource, with a goal of selecting a maximum-size
    set of mutually compatible activities. Imagine that you are in charge of scheduling
    a conference room. You are presented with a set *S* = {*a*[1], *a*[2], … , *a[n]*}
    of *n* proposed ***activities*** that wish to reserve the conference room, and
    the room can serve only one activity at a time. Each activity *a[i]* has a ***start
    time*** *s[i]* and a ***finish time*** *f[i]*, where 0 ≤ *s[i]* < *f[i]* < ∞.
    If selected, activity *a[i]* takes place during the half-open time interval [*s[i]*,
    *f[i]*). Activities *a[i]* and *a[j]* are ***compatible*** if the intervals [*s[i]*,
    *f[i]*) and [*s[j]*, *f[j]*) do not overlap. That is, *a[i]* and *a[j]* are compatible
    if *s[i]* ≥ *f[j]* or *s[j]* ≥ *f[i]*. (Assume that if your staff needs time to
    change over the room from one activity to the next, the changeover time is built
    into the intervals.) In the ***activity-selection problem***, your goal is to
    select a maximum-size subset of mutually compatible activities. Assume that the
    activities are sorted in monotonically increasing order of finish time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P485.jpg)'
  prefs: []
  type: TYPE_IMG
- en: (We’ll see later the advantage that this assumption provides.) For example,
    consider the set of activities in [Figure 15.1](chapter015.xhtml#Fig_15-1). The
    subset {*a*[3], *a*[9], *a*[11]} consists of mutually compatible activities. It
    is not a maximum subset, however, since the subset {*a*[1], *a*[4], *a*[8], *a*[11]}
    is larger. In fact, {*a*[1], *a*[4], *a*[8], *a*[11]} is a largest subset of mutually
    compatible activities, and another largest subset is {*a*[2], *a*[4], *a*[9],
    *a*[11]}.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll see how to solve this problem, proceeding in several steps. First we’ll
    explore a dynamic-programming solution, in which you consider several choices
    when determining which subproblems to use in an optimal solution. We’ll then observe
    that you need to consider only one choice—the greedy choice—and that when you
    make the greedy choice, only one subproblem remains. Based on these observations,
    we’ll develop a recursive greedy algorithm to solve the activity-selection problem.
    Finally, we’ll complete the process of developing a greedy solution by converting
    the recursive algorithm to an iterative one. Although the steps we go through
    in this section are slightly more involved than is typical when developing a greedy
    algorithm, they illustrate the relationship between greedy algorithms and dynamic
    programming.
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P486.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 15.1** A set {*a*[1], *a*[2], … , *a*[11]} of activities. Activity
    *a[i]* has start time *s[i]* and finish time *f[i]*.'
  prefs: []
  type: TYPE_NORMAL
- en: '**The optimal substructure of the activity-selection problem**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s verify that the activity-selection problem exhibits optimal substructure.
    Denote by *S[ij]* the set of activities that start after activity *a[i]* finishes
    and that finish before activity *a[j]* starts. Suppose that you want to find a
    maximum set of mutually compatible activities in *S[ij]*, and suppose further
    that such a maximum set is *A[ij]*, which includes some activity *a[k]*. By including
    *a[k]* in an optimal solution, you are left with two subproblems: finding mutually
    compatible activities in the set *S[ik]* (activities that start after activity
    *a[i]* finishes and that finish before activity *a[k]* starts) and finding mutually
    compatible activities in the set *S[kj]* (activities that start after activity
    *a[k]* finishes and that finish before activity *a[j]* starts). Let *A[ik]* =
    *A[ij]* ∩ *S[ik]* and *A[kj]* = *A[ij]* ∩ *S[kj]*, so that *A[ik]* contains the
    activities in *A[ij]* that finish before *a[k]* starts and *A[kj]* contains the
    activities in *A[ij]* that start after *a[k]* finishes. Thus, we have *A[ij]*
    = *A[ik]* ∪ {*a[k]*} ∪ *A[kj]*, and so the maximum-size set *A[ij]* of mutually
    compatible activities in *S[ij]* consists of |*A[ij]* | = |*A[ik]*| + |*A[kj]*
    | + 1 activities.'
  prefs: []
  type: TYPE_NORMAL
- en: The usual cut-and-paste argument shows that an optimal solution *A[ij]* must
    also include optimal solutions to the two subproblems for *S[ik]* and *S[kj]*.
    If you could find a set ![art](images/Akj.jpg) of mutually compatible activities
    in *S[kj]* where ![art](images/Akj1.jpg), then you could use ![art](images/Akj.jpg),
    rather than *A[kj]*, in a solution to the subproblem for *S[ij]*. You would have
    constructed a set of ![art](images/Akj2.jpg) mutually compatible activities, which
    contradicts the assumption that *A[ij]* is an optimal solution. A symmetric argument
    applies to the activities in *S[ik]*.
  prefs: []
  type: TYPE_NORMAL
- en: This way of characterizing optimal substructure suggests that you can solve
    the activity-selection problem by dynamic programming. Let’s denote the size of
    an optimal solution for the set *S[ij]* by *c*[*i*, *j*]. Then, the dynamic-programming
    approach gives the recurrence
  prefs: []
  type: TYPE_NORMAL
- en: '*c*[*i*, *j*] = *c*[*i*, *k*] + *c*[*k*, *j*] + 1.'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, if you do not know that an optimal solution for the set *S[ij]* includes
    activity *a[k]*, you must examine all activities in *S[ij]* to find which one
    to choose, so that
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P487.jpg)'
  prefs: []
  type: TYPE_IMG
- en: You can then develop a recursive algorithm and memoize it, or you can work bottom-up
    and fill in table entries as you go along. But you would be overlooking another
    important characteristic of the activity-selection problem that you can use to
    great advantage.
  prefs: []
  type: TYPE_NORMAL
- en: '**Making the greedy choice**'
  prefs: []
  type: TYPE_NORMAL
- en: 'What if you could choose an activity to add to an optimal solution without
    having to first solve all the subproblems? That could save you from having to
    consider all the choices inherent in recurrence (15.2). In fact, for the activity-selection
    problem, you need to consider only one choice: the greedy choice.'
  prefs: []
  type: TYPE_NORMAL
- en: What is the greedy choice for the activity-selection problem? Intuition suggests
    that you should choose an activity that leaves the resource available for as many
    other activities as possible. Of the activities you end up choosing, one of them
    must be the first one to finish. Intuition says, therefore, choose the activity
    in *S* with the earliest finish time, since that leaves the resource available
    for as many of the activities that follow it as possible. (If more than one activity
    in *S* has the earliest finish time, then choose any such activity.) In other
    words, since the activities are sorted in monotonically increasing order by finish
    time, the greedy choice is activity *a*[1]. Choosing the first activity to finish
    is not the only way to think of making a greedy choice for this problem. Exercise
    15.1-3 asks you to explore other possibilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you make the greedy choice, you have only one remaining subproblem to
    solve: finding activities that start after *a*[1] finishes. Why don’t you have
    to consider activities that finish before *a*[1] starts? Because *s*[1] < *f*[1],
    and because *f*[1] is the earliest finish time of any activity, no activity can
    have a finish time less than or equal to *s*[1]. Thus, all activities that are
    compatible with activity *a*[1] must start after *a*[1] finishes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, we have already established that the activity-selection problem
    exhibits optimal substructure. Let *S[k]* = {*a[i]* ∈ *S* : *s[i]* ≥ *f[k]*} be
    the set of activities that start after activity *a[k]* finishes. If you make the
    greedy choice of activity *a*[1], then *S*[1] remains as the only subproblem to
    solve.^([1](#footnote_1)) Optimal substructure says that if *a*[1] belongs to
    an optimal solution, then an optimal solution to the original problem consists
    of activity *a*[1] and all the activities in an optimal solution to the subproblem
    *S*[1].'
  prefs: []
  type: TYPE_NORMAL
- en: 'One big question remains: Is this intuition correct? Is the greedy choice—in
    which you choose the first activity to finish—always part of some optimal solution?
    The following theorem shows that it is.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Theorem 15.1***'
  prefs: []
  type: TYPE_NORMAL
- en: Consider any nonempty subproblem *S[k]*, and let *a[m]* be an activity in *S[k]*
    with the earliest finish time. Then *a[m]* is included in some maximum-size subset
    of mutually compatible activities of *S[k]*.
  prefs: []
  type: TYPE_NORMAL
- en: '***Proof***   Let *A[k]* be a maximum-size subset of mutually compatible activities
    in *S[k]*, and let *a[j]* be the activity in *A[k]* with the earliest finish time.
    If *a[j]* = *a[m]*, we are done, since we have shown that *a[m]* belongs to some
    maximum-size subset of mutually compatible activities of *S[k]*. If *a[j]* ≠ *a[m]*,
    let the set ![art](images/Art_P488.jpg) be *A[k]* but substituting *a[m]* for
    *a[j]*. The activities in ![art](images/Art_P490.jpg) are compatible, which follows
    because the activities in *A[k]* are compatible, *a[j]* is the first activity
    in *A[k]* to finish, and *f[m]* ≤ *f[j]*. Since ![art](images/Art_P491.jpg), we
    conclude that ![art](images/Art_P492.jpg) is a maximum-size subset of mutually
    compatible activities of *S[k]*, and it includes *a[m]*.'
  prefs: []
  type: TYPE_NORMAL
- en: ▪
  prefs: []
  type: TYPE_NORMAL
- en: Although you might be able to solve the activity-selection problem with dynamic
    programming, Theorem 15.1 says that you don’t need to. Instead, you can repeatedly
    choose the activity that finishes first, keep only the activities compatible with
    this activity, and repeat until no activities remain. Moreover, because you always
    choose the activity with the earliest finish time, the finish times of the activities
    that you choose must strictly increase. You can consider each activity just once
    overall, in monotonically increasing order of finish times.
  prefs: []
  type: TYPE_NORMAL
- en: 'An algorithm to solve the activity-selection problem does not need to work
    bottom-up, like a table-based dynamic-programming algorithm. Instead, it can work
    top-down, choosing an activity to put into the optimal solution that it constructs
    and then solving the subproblem of choosing activities from those that are compatible
    with those already chosen. Greedy algorithms typically have this top-down design:
    make a choice and then solve a subproblem, rather than the bottom-up technique
    of solving subproblems before making a choice.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A recursive greedy algorithm**'
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know you can bypass the dynamic-programming approach and instead
    use a top-down, greedy algorithm, let’s see a straightforward, recursive procedure
    to solve the activity-selection problem. The procedure RECURSIVE-ACTIVITY-SELECTOR
    on the following page takes the start and finish times of the activities, represented
    as arrays *s* and *f*,^([2](#footnote_2)) the index *k* that defines the subproblem
    *S[k]* it is to solve, and the size *n* of the original problem. It returns a
    maximum-size set of mutually compatible activities in *S[k]*. The procedure assumes
    that the *n* input activities are already ordered by monotonically increasing
    finish time, according to equation (15.1). If not, you can first sort them into
    this order in *O*(*n* lg *n*) time, breaking ties arbitrarily. In order to start,
    add the fictitious activity *a*[0] with *f*[0] = 0, so that subproblem *S*[0]
    is the entire set of activities *S*. The initial call, which solves the entire
    problem, is RECURSIVE-ACTIVITY-SELECTOR (*s*, *f*, 0, *n*).
  prefs: []
  type: TYPE_NORMAL
- en: RECURSIVE-ACTIVITY-SELECTOR (*s*, *f*, *k*, *n*)
  prefs: []
  type: TYPE_NORMAL
- en: '| 1 | *m* = *k* + 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | **while** *m* ≤ *n* and *s*[*m*] < *f* [*k*] | **//** find the first
    activity in *S[k]* to finish |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | *m* = *m* + 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | **if** *m* ≤ *n* |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | **return** {*a[m]*} ∪ RECURSIVE-ACTIVITY-SELECTOR (*s*, *f*, *m*, *n*)
    |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | **else return** ∅ |'
  prefs: []
  type: TYPE_TB
- en: '[Figure 15.2](chapter015.xhtml#Fig_15-2) shows how the algorithm operates on
    the activities in [Figure 15.1](chapter015.xhtml#Fig_15-1). In a given recursive
    call RECURSIVE-ACTIVITY-SELECTOR (*s*, *f*, *k*, *n*), the **while** loop of lines
    2–3 looks for the first activity in *S[k]* to finish. The loop examines *a*[*k*+1],
    *a*[*k*+2], … , *a[n]*, until it finds the first activity *a[m]* that is compatible
    with *a[k]*, which means that *s[m]* ≥ *f[k]*. If the loop terminates because
    it finds such an activity, line 5 returns the union of {*a[m]*} and the maximum-size
    subset of *S[m]* returned by the recursive call RECURSIVE-ACTIVITY-SELECTOR (*s*,
    *f*, *m*, *n*). Alternatively, the loop may terminate because *m* > *n*, in which
    case the procedure has examined all activities in *S[k]* without finding one that
    is compatible with *a[k]*. In this case, *S[k]* = ∅, and so line 6 returns ∅.'
  prefs: []
  type: TYPE_NORMAL
- en: Assuming that the activities have already been sorted by finish times, the running
    time of the call RECURSIVE-ACTIVITY-SELECTOR (*s*, *f*, 0, *n*) is Θ(*n*). To
    see why, observe that over all recursive calls, each activity is examined exactly
    once in the **while** loop test of line 2\. In particular, activity *a[i]* is
    examined in the last call made in which *k* < *i*.
  prefs: []
  type: TYPE_NORMAL
- en: '**An iterative greedy algorithm**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The recursive procedure can be converted to an iterative one because the procedure
    RECURSIVE-ACTIVITY-SELECTOR is almost “tail recursive” (see Problem 7-5): it ends
    with a recursive call to itself followed by a union operation. It is usually a
    straightforward task to transform a tail-recursive procedure to an iterative form.
    In fact, some compilers for certain programming languages perform this task automatically.'
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P493.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 15.2** The operation of RECURSIVE-ACTIVITY-SELECTOR on the 11 activities
    from [Figure 15.1](chapter015.xhtml#Fig_15-1). Activities considered in each recursive
    call appear between horizontal lines. The fictitious activity *a*[0] finishes
    at time 0, and the initial call RECURSIVE-ACTIVITY-SELECTOR (*s*, *f*, 0, 11),
    selects activity *a*[1]. In each recursive call, the activities that have already
    been selected are blue, and the activity shown in tan is being considered. If
    the starting time of an activity occurs before the finish time of the most recently
    added activity (the arrow between them points left), it is rejected. Otherwise
    (the arrow points directly up or to the right), it is selected. The last recursive
    call, RECURSIVE-ACTIVITY-SELECTOR (*s*, *f*, 11, 11), returns ∅. The resulting
    set of selected activities is {*a*[1], *a*[4], *a*[8], *a*[11]}.'
  prefs: []
  type: TYPE_NORMAL
- en: The procedure GREEDY-ACTIVITY-SELECTOR is an iterative version of the procedure
    RECURSIVE-ACTIVITY-SELECTOR. It, too, assumes that the input activities are ordered
    by monotonically increasing finish time. It collects selected activities into
    a set *A* and returns this set when it is done.
  prefs: []
  type: TYPE_NORMAL
- en: GREEDY-ACTIVITY-SELECTOR (*s*, *f*, *n*)
  prefs: []
  type: TYPE_NORMAL
- en: '| 1 | *A* = {*a*[1]} |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | *k* = 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | **for** *m* = 2 **to** *n* |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | **if** *s*[*m*] ≥ *f* [*k*] | **//** is *a[m]* in *S[k]*? |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | *A* = *A* ∪ {*a[m]*} | **//** yes, so choose it |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | *k* = *m* | **//** and continue from there |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | **return** *A* |'
  prefs: []
  type: TYPE_TB
- en: The procedure works as follows. The variable *k* indexes the most recent addition
    to *A*, corresponding to the activity *a[k]* in the recursive version. Since the
    procedure considers the activities in order of monotonically increasing finish
    time, *f[k]* is always the maximum finish time of any activity in *A*. That is,
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P494.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Lines 1–2 select activity *a*[1], initialize *A* to contain just this activity,
    and initialize *k* to index this activity. The **for** loop of lines 3–6 finds
    the earliest activity in *S[k]* to finish. The loop considers each activity *a[m]*
    in turn and adds *a[m]* to *A* if it is compatible with all previously selected
    activities. Such an activity is the earliest in *S[k]* to finish. To see whether
    activity *a[m]* is compatible with every activity currently in *A*, it suffices
    by equation (15.3) to check (in line 4) that its start time *s[m]* is not earlier
    than the finish time *f[k]* of the activity most recently added to *A*. If activity
    *a[m]* is compatible, then lines 5–6 add activity *a[m]* to *A* and set *k* to
    *m*. The set *A* returned by the call GREEDY-ACTIVITY-SELECTOR (*s*, *f*) is precisely
    the set returned by the initial call RECURSIVE-ACTIVITY-SELECTOR (*s*, *f*, 0,
    *n*).
  prefs: []
  type: TYPE_NORMAL
- en: Like the recursive version, GREEDY-ACTIVITY-SELECTOR schedules a set of *n*
    activities in Θ(*n*) time, assuming that the activities were already sorted initially
    by their finish times.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercises**'
  prefs: []
  type: TYPE_NORMAL
- en: '***15.1-1***'
  prefs: []
  type: TYPE_NORMAL
- en: Give a dynamic-programming algorithm for the activity-selection problem, based
    on recurrence (15.2). Have your algorithm compute the sizes *c*[*i*, *j*] as defined
    above and also produce the maximum-size subset of mutually compatible activities.
    Assume that the inputs have been sorted as in equation (15.1). Compare the running
    time of your solution to the running time of GREEDY-ACTIVITY-SELECTOR.
  prefs: []
  type: TYPE_NORMAL
- en: '***15.1-2***'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that instead of always selecting the first activity to finish, you instead
    select the last activity to start that is compatible with all previously selected
    activities. Describe how this approach is a greedy algorithm, and prove that it
    yields an optimal solution.
  prefs: []
  type: TYPE_NORMAL
- en: '***15.1-3***'
  prefs: []
  type: TYPE_NORMAL
- en: Not just any greedy approach to the activity-selection problem produces a maximum-size
    set of mutually compatible activities. Give an example to show that the approach
    of selecting the activity of least duration from among those that are compatible
    with previously selected activities does not work. Do the same for the approaches
    of always selecting the compatible activity that overlaps the fewest other remaining
    activities and always selecting the compatible remaining activity with the earliest
    start time.
  prefs: []
  type: TYPE_NORMAL
- en: '***15.1-4***'
  prefs: []
  type: TYPE_NORMAL
- en: You are given a set of activities to schedule among a large number of lecture
    halls, where any activity can take place in any lecture hall. You wish to schedule
    all the activities using as few lecture halls as possible. Give an efficient greedy
    algorithm to determine which activity should use which lecture hall.
  prefs: []
  type: TYPE_NORMAL
- en: (This problem is also known as the ***interval-graph coloring problem***. It
    is modeled by an interval graph whose vertices are the given activities and whose
    edges connect incompatible activities. The smallest number of colors required
    to color every vertex so that no two adjacent vertices have the same color corresponds
    to finding the fewest lecture halls needed to schedule all of the given activities.)
  prefs: []
  type: TYPE_NORMAL
- en: '***15.1-5***'
  prefs: []
  type: TYPE_NORMAL
- en: Consider a modification to the activity-selection problem in which each activity
    *a[i]* has, in addition to a start and finish time, a value *v[i]*. The objective
    is no longer to maximize the number of activities scheduled, but instead to maximize
    the total value of the activities scheduled. That is, the goal is to choose a
    set *A* of compatible activities such that ![art](images/Art_P495.jpg) is maximized.
    Give a polynomial-time algorithm for this problem.
  prefs: []
  type: TYPE_NORMAL
- en: '[**15.2    Elements of the greedy strategy**](toc.xhtml#Rh1-89)'
  prefs: []
  type: TYPE_NORMAL
- en: A greedy algorithm obtains an optimal solution to a problem by making a sequence
    of choices. At each decision point, the algorithm makes the choice that seems
    best at the moment. This heuristic strategy does not always produce an optimal
    solution, but as in the activity-selection problem, sometimes it does. This section
    discusses some of the general properties of greedy methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process that we followed in [Section 15.1](chapter015.xhtml#Sec_15.1) to
    develop a greedy algorithm was a bit more involved than is typical. It consisted
    of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Determine the optimal substructure of the problem.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Develop a recursive solution. (For the activity-selection problem, we formulated
    recurrence (15.2), but bypassed developing a recursive algorithm based solely
    on this recurrence.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Show that if you make the greedy choice, then only one subproblem remains.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prove that it is always safe to make the greedy choice. (Steps 3 and 4 can occur
    in either order.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Develop a recursive algorithm that implements the greedy strategy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert the recursive algorithm to an iterative algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These steps highlighted in great detail the dynamic-programming underpinnings
    of a greedy algorithm. For example, the first cut at the activity-selection problem
    defined the subproblems *S[ij]*, where both *i* and *j* varied. We then found
    that if you always make the greedy choice, you can restrict the subproblems to
    be of the form *S[k]*.
  prefs: []
  type: TYPE_NORMAL
- en: 'An alternative approach is to fashion optimal substructure with a greedy choice
    in mind, so that the choice leaves just one subproblem to solve. In the activity-selection
    problem, start by dropping the second subscript and defining subproblems of the
    form *S[k]*. Then prove that a greedy choice (the first activity *a[m]* to finish
    in *S[k]*), combined with an optimal solution to the remaining set *S[m]* of compatible
    activities, yields an optimal solution to *S[k]*. More generally, you can design
    greedy algorithms according to the following sequence of steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Cast the optimization problem as one in which you make a choice and are left
    with one subproblem to solve.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prove that there is always an optimal solution to the original problem that
    makes the greedy choice, so that the greedy choice is always safe.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Demonstrate optimal substructure by showing that, having made the greedy choice,
    what remains is a subproblem with the property that if you combine an optimal
    solution to the subproblem with the greedy choice you have made, you arrive at
    an optimal solution to the original problem.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Later sections of this chapter will use this more direct process. Nevertheless,
    beneath every greedy algorithm, there is almost always a more cumbersome dynamic-programming
    solution.
  prefs: []
  type: TYPE_NORMAL
- en: How can you tell whether a greedy algorithm will solve a particular optimization
    problem? No way works all the time, but the greedy-choice property and optimal
    substructure are the two key ingredients. If you can demonstrate that the problem
    has these properties, then you are well on the way to developing a greedy algorithm
    for it.
  prefs: []
  type: TYPE_NORMAL
- en: '**Greedy-choice property**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first key ingredient is the ***greedy-choice property***: you can assemble
    a globally optimal solution by making locally optimal (greedy) choices. In other
    words, when you are considering which choice to make, you make the choice that
    looks best in the current problem, without considering results from subproblems.'
  prefs: []
  type: TYPE_NORMAL
- en: Here is where greedy algorithms differ from dynamic programming. In dynamic
    programming, you make a choice at each step, but the choice usually depends on
    the solutions to subproblems. Consequently, you typically solve dynamic-programming
    problems in a bottom-up manner, progressing from smaller subproblems to larger
    subproblems. (Alternatively, you can solve them top down, but memoizing. Of course,
    even though the code works top down, you still must solve the subproblems before
    making a choice.) In a greedy algorithm, you make whatever choice seems best at
    the moment and then solve the subproblem that remains. The choice made by a greedy
    algorithm may depend on choices so far, but it cannot depend on any future choices
    or on the solutions to subproblems. Thus, unlike dynamic programming, which solves
    the subproblems before making the first choice, a greedy algorithm makes its first
    choice before solving any subproblems. A dynamic-programming algorithm proceeds
    bottom up, whereas a greedy strategy usually progresses top down, making one greedy
    choice after another, reducing each given problem instance to a smaller one.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, you need to prove that a greedy choice at each step yields a globally
    optimal solution. Typically, as in the case of Theorem 15.1, the proof examines
    a globally optimal solution to some subproblem. It then shows how to modify the
    solution to substitute the greedy choice for some other choice, resulting in one
    similar, but smaller, subproblem.
  prefs: []
  type: TYPE_NORMAL
- en: You can usually make the greedy choice more efficiently than when you have to
    consider a wider set of choices. For example, in the activity-selection problem,
    assuming that the activities were already sorted in monotonically increasing order
    by finish times, each activity needed to be examined just once. By preprocessing
    the input or by using an appropriate data structure (often a priority queue),
    you often can make greedy choices quickly, thus yielding an efficient algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '**Optimal substructure**'
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in [Chapter 14](chapter014.xhtml), a problem exhibits ***optimal substructure***
    if an optimal solution to the problem contains within it optimal solutions to
    subproblems. This property is a key ingredient of assessing whether dynamic programming
    applies, and it’s also essential for greedy algorithms. As an example of optimal
    substructure, recall how [Section 15.1](chapter015.xhtml#Sec_15.1) demonstrated
    that if an optimal solution to subproblem *S[ij]* includes an activity *a[k]*,
    then it must also contain optimal solutions to the subproblems *S[ik]* and *S[kj]*.
    Given this optimal substructure, we argued that if you know which activity to
    use as *a[k]*, you can construct an optimal solution to *S[ij]* by selecting *a[k]*
    along with all activities in optimal solutions to the subproblems *S[ik]* and
    *S[kj]*. This observation of optimal substructure gave rise to the recurrence
    (15.2) that describes the value of an optimal solution.
  prefs: []
  type: TYPE_NORMAL
- en: You will usually use a more direct approach regarding optimal substructure when
    applying it to greedy algorithms. As mentioned above, you have the luxury of assuming
    that you arrived at a subproblem by having made the greedy choice in the original
    problem. All you really need to do is argue that an optimal solution to the subproblem,
    combined with the greedy choice already made, yields an optimal solution to the
    original problem. This scheme implicitly uses induction on the subproblems to
    prove that making the greedy choice at every step produces an optimal solution.
  prefs: []
  type: TYPE_NORMAL
- en: '**Greedy versus dynamic programming**'
  prefs: []
  type: TYPE_NORMAL
- en: Because both the greedy and dynamic-programming strategies exploit optimal substructure,
    you might be tempted to generate a dynamic-programming solution to a problem when
    a greedy solution suffices or, conversely, you might mistakenly think that a greedy
    solution works when in fact a dynamic-programming solution is required. To illustrate
    the subtle differences between the two techniques, let’s investigate two variants
    of a classical optimization problem.
  prefs: []
  type: TYPE_NORMAL
- en: The ***0-1 knapsack problem*** is the following. A thief robbing a store wants
    to take the most valuable load that can be carried in a knapsack capable of carrying
    at most *W* pounds of loot. The thief can choose to take any subset of *n* items
    in the store. The *i*th item is worth *v[i]* dollars and weighs *w[i]* pounds,
    where *v[i]* and *w[i]* are integers. Which items should the thief take? (We call
    this the 0-1 knapsack problem because for each item, the thief must either take
    it or leave it behind. The thief cannot take a fractional amount of an item or
    take an item more than once.)
  prefs: []
  type: TYPE_NORMAL
- en: In the ***fractional knapsack problem***, the setup is the same, but the thief
    can take fractions of items, rather than having to make a binary (0-1) choice
    for each item. You can think of an item in the 0-1 knapsack problem as being like
    a gold ingot and an item in the fractional knapsack problem as more like gold
    dust.
  prefs: []
  type: TYPE_NORMAL
- en: Both knapsack problems exhibit the optimal-substructure property. For the 0-1
    problem, if the most valuable load weighing at most *W* pounds includes item *j*,
    then the remaining load must be the most valuable load weighing at most *W* −
    *w[j]* pounds that the thief can take from the *n* − 1 original items excluding
    item *j*. For the comparable fractional problem, if if the most valuable load
    weighing at most *W* pounds includes weight *w* of item *j*, then the remaining
    load must be the most valuable load weighing at most *W* − *w* pounds that the
    thief can take from the *n* − 1 original items plus *w[j]* − *w* pounds of item
    *j*.
  prefs: []
  type: TYPE_NORMAL
- en: Although the problems are similar, a greedy strategy works to solve the fractional
    knapsack problem, but not the 0-1 problem. To solve the fractional problem, first
    compute the value per pound *v[i]*/*w[i]* for each item. Obeying a greedy strategy,
    the thief begins by taking as much as possible of the item with the greatest value
    per pound. If the supply of that item is exhausted and the thief can still carry
    more, then the thief takes as much as possible of the item with the next greatest
    value per pound, and so forth, until reaching the weight limit *W*. Thus, by sorting
    the items by value per pound, the greedy algorithm runs in *O*(*n* lg *n*) time.
    You are asked to prove that the fractional knapsack problem has the greedy-choice
    property in Exercise 15.2-1.
  prefs: []
  type: TYPE_NORMAL
- en: To see that this greedy strategy does not work for the 0-1 knapsack problem,
    consider the problem instance illustrated in [Figure 15.3(a)](chapter015.xhtml#Fig_15-3).
    This example has three items and a knapsack that can hold 50 pounds. Item 1 weighs
    10 pounds and is worth $60\. Item 2 weighs 20 pounds and is worth $100\. Item
    3 weighs 30 pounds and is worth $120\. Thus, the value per pound of item 1 is
    $6 per pound, which is greater than the value per pound of either item 2 ($5 per
    pound) or item 3 ($4 per pound). The greedy strategy, therefore, would take item
    1 first. As you can see from the case analysis in [Figure 15.3(b)](chapter015.xhtml#Fig_15-3),
    however, the optimal solution takes items 2 and 3, leaving item 1 behind. The
    two possible solutions that take item 1 are both suboptimal.
  prefs: []
  type: TYPE_NORMAL
- en: For the comparable fractional problem, however, the greedy strategy, which takes
    item 1 first, does yield an optimal solution, as shown in [Figure 15.3(c)](chapter015.xhtml#Fig_15-3).
    Taking item 1 doesn’t work in the 0-1 problem, because the thief is unable to
    fill the knapsack to capacity, and the empty space lowers the effective value
    per pound of the load. In the 0-1 problem, when you consider whether to include
    an item in the knapsack, you must compare the solution to the subproblem that
    includes the item with the solution to the subproblem that excludes the item before
    you can make the choice. The problem formulated in this way gives rise to many
    overlapping subproblems—a hallmark of dynamic programming, and indeed, as Exercise
    15.2-2 asks you to show, you can use dynamic programming to solve the 0-1 problem.
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P496.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 15.3** An example showing that the greedy strategy does not work for
    the 0-1 knapsack problem. **(a)** The thief must select a subset of the three
    items shown whose weight must not exceed 50 pounds. **(b)** The optimal subset
    includes items 2 and 3\. Any solution with item 1 is suboptimal, even though item
    1 has the greatest value per pound. **(c)** For the fractional knapsack problem,
    taking the items in order of greatest value per pound yields an optimal solution.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercises**'
  prefs: []
  type: TYPE_NORMAL
- en: '***15.2-1***'
  prefs: []
  type: TYPE_NORMAL
- en: Prove that the fractional knapsack problem has the greedy-choice property.
  prefs: []
  type: TYPE_NORMAL
- en: '***15.2-2***'
  prefs: []
  type: TYPE_NORMAL
- en: Give a dynamic-programming solution to the 0-1 knapsack problem that runs in
    *O*(*n W*) time, where *n* is the number of items and *W* is the maximum weight
    of items that the thief can put in the knapsack.
  prefs: []
  type: TYPE_NORMAL
- en: '***15.2-3***'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that in a 0-1 knapsack problem, the order of the items when sorted by
    increasing weight is the same as their order when sorted by decreasing value.
    Give an efficient algorithm to find an optimal solution to this variant of the
    knapsack problem, and argue that your algorithm is correct.
  prefs: []
  type: TYPE_NORMAL
- en: '***15.2-4***'
  prefs: []
  type: TYPE_NORMAL
- en: Professor Gekko has always dreamed of inline skating across North Dakota. The
    professor plans to cross the state on highway U.S. 2, which runs from Grand Forks,
    on the eastern border with Minnesota, to Williston, near the western border with
    Montana. The professor can carry two liters of water and can skate *m* miles before
    running out of water. (Because North Dakota is relatively flat, the professor
    does not have to worry about drinking water at a greater rate on uphill sections
    than on flat or downhill sections.) The professor will start in Grand Forks with
    two full liters of water. The professor has an official North Dakota state map,
    which shows all the places along U.S. 2 to refill water and the distances between
    these locations.
  prefs: []
  type: TYPE_NORMAL
- en: The professor’s goal is to minimize the number of water stops along the route
    across the state. Give an efficient method by which the professor can determine
    which water stops to make. Prove that your strategy yields an optimal solution,
    and give its running time.
  prefs: []
  type: TYPE_NORMAL
- en: '***15.2-5***'
  prefs: []
  type: TYPE_NORMAL
- en: Describe an efficient algorithm that, given a set {*x*[1], *x*[2], … , *x[n]*}
    of points on the real line, determines the smallest set of unit-length closed
    intervals that contains all of the given points. Argue that your algorithm is
    correct.
  prefs: []
  type: TYPE_NORMAL
- en: ★ ***15.2-6***
  prefs: []
  type: TYPE_NORMAL
- en: Show how to solve the fractional knapsack problem in *O*(*n*) time.
  prefs: []
  type: TYPE_NORMAL
- en: '***15.2-7***'
  prefs: []
  type: TYPE_NORMAL
- en: You are given two sets *A* and *B*, each containing *n* positive integers. You
    can choose to reorder each set however you like. After reordering, let *a[i]*
    be the *i*th element of set *A*, and let *b[i]* be the *i*th element of set *B*.
    You then receive a payoff of ![art](images/Art_P497.jpg). Give an algorithm that
    maximizes your payoff. Prove that your algorithm maximizes the payoff, and state
    its running time, omitting the time for reordering the sets.
  prefs: []
  type: TYPE_NORMAL
- en: '[**15.3    Huffman codes**](toc.xhtml#Rh1-90)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Huffman codes compress data well: savings of 20% to 90% are typical, depending
    on the characteristics of the data being compressed. The data arrive as a sequence
    of characters. Huffman’s greedy algorithm uses a table giving how often each character
    occurs (its frequency) to build up an optimal way of representing each character
    as a binary string.'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that you have a 100,000-character data file that you wish to store compactly
    and you know that the 6 distinct characters in the file occur with the frequencies
    given by [Figure 15.4](chapter015.xhtml#Fig_15-4). The character a occurs 45,000
    times, the character b occurs 13,000 times, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'You have many options for how to represent such a file of information. Here,
    we consider the problem of designing a ***binary character code*** (or ***code***
    for short) in which each character is represented by a unique binary string, which
    we call a ***codeword***. If you use a ***fixed-length code***, you need ⌈lg *n*⌉
    bits to represent *n* ≥ 2 characters. For 6 characters, therefore, you need 3
    bits: a = 000, b = 001, c = 010, d = 011, e = 100, and f = 101\. This method requires
    300,000 bits to encode the entire file. Can you do better?'
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P498.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 15.4** A character-coding problem. A data file of 100,000 characters
    contains only the characters a–f, with the frequencies indicated. With each character
    represented by a 3-bit codeword, encoding the file requires 300,000 bits. With
    the variable-length code shown, the encoding requires only 224,000 bits.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A ***variable-length code*** can do considerably better than a fixed-length
    code. The idea is simple: give frequent characters short codewords and infrequent
    characters long codewords. [Figure 15.4](chapter015.xhtml#Fig_15-4) shows such
    a code. Here, the 1-bit string 0 represents a, and the 4-bit string 1100 represents
    f. This code requires'
  prefs: []
  type: TYPE_NORMAL
- en: (45 · 1 + 13 · 3 + 12 · 3 + 16 · 3 + 9 · 4 + 5 · 4) · 1,000 = 224,000 bits
  prefs: []
  type: TYPE_NORMAL
- en: to represent the file, a savings of approximately 25%. In fact, this is an optimal
    character code for this file, as we shall see.
  prefs: []
  type: TYPE_NORMAL
- en: '**Prefix-free codes**'
  prefs: []
  type: TYPE_NORMAL
- en: We consider here only codes in which no codeword is also a prefix of some other
    codeword. Such codes are called ***prefix-free codes***. Although we won’t prove
    it here, a prefix-free code can always achieve the optimal data compression among
    any character code, and so we suffer no loss of generality by restricting our
    attention to prefix-free codes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Encoding is always simple for any binary character code: just concatenate the
    codewords representing each character of the file. For example, with the variable-length
    prefix-free code of [Figure 15.4](chapter015.xhtml#Fig_15-4), the 4-character
    file face has the encoding 1100 · 0 · 100 · 1101 = 110001001101, where “·” denotes
    concatenation.'
  prefs: []
  type: TYPE_NORMAL
- en: Prefix-free codes are desirable because they simplify decoding. Since no codeword
    is a prefix of any other, the codeword that begins an encoded file is unambiguous.
    You can simply identify the initial codeword, translate it back to the original
    character, and repeat the decoding process on the remainder of the encoded file.
    In our example, the string 100011001101 parses uniquely as 100 · 0 · 1100 · 1101,
    which decodes to cafe.
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P499.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 15.5** Trees corresponding to the coding schemes in [Figure 15.4](chapter015.xhtml#Fig_15-4).
    Each leaf is labeled with a character and its frequency of occurrence. Each internal
    node is labeled with the sum of the frequencies of the leaves in its subtree.
    All frequencies are in thousands. **(a)** The tree corresponding to the fixed-length
    code a = 000, b = 001, c = 010, d = 011, e = 100, f = 101\. **(b)** The tree corresponding
    to the optimal prefix-free code a = 0, b = 101, c = 100, d = 111, e = 1101, f
    = 1100.'
  prefs: []
  type: TYPE_NORMAL
- en: The decoding process needs a convenient representation for the prefix-free code
    so that you can easily pick off the initial codeword. A binary tree whose leaves
    are the given characters provides one such representation. Interpret the binary
    codeword for a character as the simple path from the root to that character, where
    0 means “go to the left child” and 1 means “go to the right child.” [Figure 15.5](chapter015.xhtml#Fig_15-5)
    shows the trees for the two codes of our example. Note that these are not binary
    search trees, since the leaves need not appear in sorted order and internal nodes
    do not contain character keys.
  prefs: []
  type: TYPE_NORMAL
- en: 'An optimal code for a file is always represented by a *full* binary tree, in
    which every nonleaf node has two children (see Exercise 15.3-2). The fixed-length
    code in our example is not optimal since its tree, shown in [Figure 15.5(a)](chapter015.xhtml#Fig_15-5),
    is not a full binary tree: it contains codewords beginning with 10, but none beginning
    with 11\. Since we can now restrict our attention to full binary trees, we can
    say that if *C* is the alphabet from which the characters are drawn and all character
    frequencies are positive, then the tree for an optimal prefix-free code has exactly
    |*C* | leaves, one for each letter of the alphabet, and exactly |*C* | − 1 internal
    nodes (see Exercise B.5-3 on page 1175).'
  prefs: []
  type: TYPE_NORMAL
- en: Given a tree *T* corresponding to a prefix-free code, we can compute the number
    of bits required to encode a file. For each character *c* in the alphabet *C*,
    let the attribute *c*.*freq* denote the frequency of *c* in the file and let *d[T]*(*c*)
    denote the depth of *c*’s leaf in the tree. Note that *d[T]* (*c*) is also the
    length of the codeword for character *c*. The number of bits required to encode
    a file is thus
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P500.jpg)'
  prefs: []
  type: TYPE_IMG
- en: which we define as the ***cost*** of the tree *T*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Constructing a Huffman code**'
  prefs: []
  type: TYPE_NORMAL
- en: Huffman invented a greedy algorithm that constructs an optimal prefix-free code,
    called a ***Huffman code*** in his honor. In line with our observations in [Section
    15.2](chapter015.xhtml#Sec_15.2), its proof of correctness relies on the greedy-choice
    property and optimal substructure. Rather than demonstrating that these properties
    hold and then developing pseudocode, we present the pseudocode first. Doing so
    will help clarify how the algorithm makes greedy choices.
  prefs: []
  type: TYPE_NORMAL
- en: The procedure HUFFMAN assumes that *C* is a set of *n* characters and that each
    character *c* ∈ *C* is an object with an attribute *c*.*freq* giving its frequency.
    The algorithm builds the tree *T* corresponding to an optimal code in a bottom-up
    manner. It begins with a set of |*C* | leaves and performs a sequence of |*C*
    | − 1 “merging” operations to create the final tree. The algorithm uses a min-priority
    queue *Q*, keyed on the *freq* attribute, to identify the two least-frequent objects
    to merge together. The result of merging two objects is a new object whose frequency
    is the sum of the frequencies of the two objects that were merged.
  prefs: []
  type: TYPE_NORMAL
- en: HUFFMAN(*C*)
  prefs: []
  type: TYPE_NORMAL
- en: '|   1 | *n* = &#124;*C* &#124; |   |'
  prefs: []
  type: TYPE_TB
- en: '|   2 | *Q* = *C* |   |'
  prefs: []
  type: TYPE_TB
- en: '|   3 | **for** *i* = 1 **to** *n* − 1 |   |'
  prefs: []
  type: TYPE_TB
- en: '|   4 | allocate a new node *z* |   |'
  prefs: []
  type: TYPE_TB
- en: '|   5 | *x* = EXTRACT-MIN(*Q*) |   |'
  prefs: []
  type: TYPE_TB
- en: '|   6 | *y* = EXTRACT-MIN(*Q*) |   |'
  prefs: []
  type: TYPE_TB
- en: '|   7 | *z*.*left* = *x* |   |'
  prefs: []
  type: TYPE_TB
- en: '|   8 | *z*.*right* = *y* |   |'
  prefs: []
  type: TYPE_TB
- en: '|   9 | *z*.*freq* = *x*.*freq* + *y*.*freq* |   |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | INSERT(*Q*, *z*) |   |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | **return** EXTRACT-MIN(*Q*) | **//** the root of the tree is the only
    node left |'
  prefs: []
  type: TYPE_TB
- en: For our example, Huffman’s algorithm proceeds as shown in [Figure 15.6](chapter015.xhtml#Fig_15-6).
    Since the alphabet contains 6 letters, the initial queue size is *n* = 6, and
    5 merge steps build the tree. The final tree represents the optimal prefix-free
    code. The codeword for a letter is the sequence of edge labels on the simple path
    from the root to the letter.
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P501.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 15.6** The steps of Huffman’s algorithm for the frequencies given
    in [Figure 15.4](chapter015.xhtml#Fig_15-4). Each part shows the contents of the
    queue sorted into increasing order by frequency. Each step merges the two trees
    with the lowest frequencies. Leaves are shown as rectangles containing a character
    and its frequency. Internal nodes are shown as circles containing the sum of the
    frequencies of their children. An edge connecting an internal node with its children
    is labeled 0 if it is an edge to a left child and 1 if it is an edge to a right
    child. The codeword for a letter is the sequence of labels on the edges connecting
    the root to the leaf for that letter. **(a)** The initial set of *n* = 6 nodes,
    one for each letter. **(b)–(e)** Intermediate stages. **(f)** The final tree.'
  prefs: []
  type: TYPE_NORMAL
- en: The HUFFMAN procedure works as follows. Line 2 initializes the min-priority
    queue *Q* with the characters in *C*. The **for** loop in lines 3–10 repeatedly
    extracts the two nodes *x* and *y* of lowest frequency from the queue and replaces
    them in the queue with a new node *z* representing their merger. The frequency
    of *z* is computed as the sum of the frequencies of *x* and *y* in line 9\. The
    node *z* has *x* as its left child and *y* as its right child. (This order is
    arbitrary. Switching the left and right child of any node yields a different code
    of the same cost.) After *n* − 1 mergers, line 11 returns the one node left in
    the queue, which is the root of the code tree.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm produces the same result without the variables *x* and *y*, assigning
    the values returned by the EXTRACT-MIN calls directly to *z*.*left* and *z*.*right*
    in lines 7 and 8, and changing line 9 to *z*.*freq* = *z*.*left*.*freq*+*z*.*right*.*freq*.
    We’ll use the node names *x* and *y* in the proof of correctness, however, so
    we leave them in.
  prefs: []
  type: TYPE_NORMAL
- en: The running time of Huffman’s algorithm depends on how the min-priority queue
    *Q* is implemented. Let’s assume that it’s implemented as a binary min-heap (see
    [Chapter 6](chapter006.xhtml)). For a set *C* of *n* characters, the BUILD-MIN-HEAP
    procedure discussed in [Section 6.3](chapter006.xhtml#Sec_6.3) can initialize
    *Q* in line 2 in *O*(*n*) time. The **for** loop in lines 3–10 executes exactly
    *n* − 1 times, and since each heap operation runs in *O*(lg *n*) time, the loop
    contributes *O*(*n* lg *n*) to the running time. Thus, the total running time
    of HUFFMAN on a set of *n* characters is *O*(*n* lg *n*).
  prefs: []
  type: TYPE_NORMAL
- en: '**Correctness of Huffman’s algorithm**'
  prefs: []
  type: TYPE_NORMAL
- en: To prove that the greedy algorithm HUFFMAN is correct, we’ll show that the problem
    of determining an optimal prefix-free code exhibits the greedy-choice and optimal-substructure
    properties. The next lemma shows that the greedy-choice property holds.
  prefs: []
  type: TYPE_NORMAL
- en: '***Lemma 15.2 (Optimal prefix-free codes have the greedy-choice property)***'
  prefs: []
  type: TYPE_NORMAL
- en: Let *C* be an alphabet in which each character *c* ∈ *C* has frequency *c*.*freq*.
    Let *x* and *y* be two characters in *C* having the lowest frequencies. Then there
    exists an optimal prefix-free code for *C* in which the codewords for *x* and
    *y* have the same length and differ only in the last bit.
  prefs: []
  type: TYPE_NORMAL
- en: '***Proof***   The idea of the proof is to take the tree *T* representing an
    arbitrary optimal prefix-free code and modify it to make a tree representing another
    optimal prefix-free code such that the characters *x* and *y* appear as sibling
    leaves of maximum depth in the new tree. In such a tree, the codewords for *x*
    and *y* have the same length and differ only in the last bit.'
  prefs: []
  type: TYPE_NORMAL
- en: Let *a* and *b* be any two characters that are sibling leaves of maximum depth
    in *T*. Without loss of generality, assume that *a*.*freq* ≤ *b*.*freq* and *x*.*freq*
    ≤ *y*.*freq*. Since *x*.*freq* and *y*.*freq* are the two lowest leaf frequencies,
    in order, and *a*.*freq* and *b*.*freq* are two arbitrary frequencies, in order,
    we have *x*.*freq* ≤ *a*.*freq* and *y*.*freq* ≤ *b*.*freq*.
  prefs: []
  type: TYPE_NORMAL
- en: In the remainder of the proof, it is possible that we could have *x*.*freq*
    = *a*.*freq* or *y*.*freq* = *b*.*freq*, but *x*.*freq* = *b*.*freq* implies that
    *a*.*freq* = *b*.*freq* = *x*.*freq* = *y*.*freq* (see Exercise 15.3-1), and the
    lemma would be trivially true. Therefore, assume that *x.freq* ≠ *b.freq*, which
    means that *x* ≠ *b*.
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P502.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 15.7** An illustration of the key step in the proof of Lemma 15.2\.
    In the optimal tree *T*, leaves *a* and *b* are two siblings of maximum depth.
    Leaves *x* and *y* are the two characters with the lowest frequencies. They appear
    in arbitrary positions in *T*. Assuming that *x* ≠ *b*, swapping leaves *a* and
    *x* produces tree *T*′, and then swapping leaves *b* and *y* produces tree *T*
    ″. Since each swap does not increase the cost, the resulting tree *T* ″ is also
    an optimal tree.'
  prefs: []
  type: TYPE_NORMAL
- en: As [Figure 15.7](chapter015.xhtml#Fig_15-7) shows, imagine exchanging the positions
    in *T* of *a* and *x* to produce a tree *T*′, and then exchanging the positions
    in *T*′ of *b* and *y* to produce a tree *T*″ in which *x* and *y* are sibling
    leaves of maximum depth. (Note that if *x* = *b* but *y* ≠ *a*, then tree *T*
    ″ does not have *x* and *y* as sibling leaves of maximum depth. Because we assume
    that *x* ≠ *b*, this situation cannot occur.) By equation (15.4), the difference
    in cost between *T* and *T*′ is
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P503.jpg)'
  prefs: []
  type: TYPE_IMG
- en: because both *a*.*freq* − *x*.*freq* and *d[T]* (*a*) − *d[T]* (*x*) are nonnegative.
    More specifically, *a.freq* − *x*.*freq* is nonnegative because *x* is a minimum-frequency
    leaf, and *d[T]* (*a*) − *d[T]* (*x*) is nonnegative because *a* is a leaf of
    maximum depth in *T*. Similarly, exchanging *y* and *b* does not increase the
    cost, and so *B*(*T*′) − *B*(*T* ″) is nonnegative. Therefore, *B*(*T* ″) ≤ *B*(*T*′)
    ≤ *B*(*T*), and since *T* is optimal, we have *B*(*T*) ≤ *B*(*T* ″), which implies
    *B*(*T* ″) = *B*(*T*). Thus, *T* ″ is an optimal tree in which *x* and *y* appear
    as sibling leaves of maximum depth, from which the lemma follows.
  prefs: []
  type: TYPE_NORMAL
- en: ▪
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 15.2 implies that the process of building up an optimal tree by mergers
    can, without loss of generality, begin with the greedy choice of merging together
    those two characters of lowest frequency. Why is this a greedy choice? We can
    view the cost of a single merger as being the sum of the frequencies of the two
    items being merged. Exercise 15.3-4 shows that the total cost of the tree constructed
    equals the sum of the costs of its mergers. Of all possible mergers at each step,
    HUFFMAN chooses the one that incurs the least cost.
  prefs: []
  type: TYPE_NORMAL
- en: The next lemma shows that the problem of constructing optimal prefix-free codes
    has the optimal-substructure property.
  prefs: []
  type: TYPE_NORMAL
- en: '***Lemma 15.3 (Optimal prefix-free codes have the optimal-substructure property)***'
  prefs: []
  type: TYPE_NORMAL
- en: Let *C* be a given alphabet with frequency *c*.*freq* defined for each character
    *c* ∈ *C*. Let *x* and *y* be two characters in *C* with minimum frequency. Let
    *C*′ be the alphabet *C* with the characters *x* and *y* removed and a new character
    *z* added, so that *C*′ = (*C* − {*x*, *y*}) ∪ {*z*}. Define *freq* for all characters
    in *C*′ with the same values as in *C*, along with *z*.*freq* = *x*.*freq* + *y*.*freq*.
    Let *T*′ be any tree representing an optimal prefix-free code for alphabet *C*′.
    Then the tree *T*, obtained from *T*′ by replacing the leaf node for *z* with
    an internal node having *x* and *y* as children, represents an optimal prefix-free
    code for the alphabet *C*.
  prefs: []
  type: TYPE_NORMAL
- en: '***Proof***   We first show how to express the cost *B*(*T*) of tree *T* in
    terms of the cost *B*(*T*′) of tree *T*′, by considering the component costs in
    equation (15.4). For each character *c* ∈ *C* − {*x*, *y*}, we have that *d[T]*
    (*c*) = *d[T′]* (*c*), and hence *c*.*freq · d[T]* (*c*) = *c*.*freq* · *d[T′]*
    (*c*). Since *d[T]* (*x*) = *d[T]* (*y*) = *d[T′]* (*z*) + 1, we have'
  prefs: []
  type: TYPE_NORMAL
- en: '| *x*.*freq* · *d[T]* (*x*) + *y*.*freq* · *d[T]* (*y*) | = | (*x*.*freq* +
    *y*.*freq*)(*d[T′]* (*z*) + 1) |'
  prefs: []
  type: TYPE_TB
- en: '|  | = | *z*.*freq* · *d[T′]*(*z*)+ (*x*.*freq* + *y*.*freq*), |'
  prefs: []
  type: TYPE_TB
- en: from which we conclude that
  prefs: []
  type: TYPE_NORMAL
- en: '*B*(*T*) = *B*(*T*′) + *x*.*freq* + *y*.*freq*'
  prefs: []
  type: TYPE_NORMAL
- en: or, equivalently,
  prefs: []
  type: TYPE_NORMAL
- en: '*B*(*T*′) = *B*(*T*) − *x*.*freq* − *y*.*freq*.'
  prefs: []
  type: TYPE_NORMAL
- en: We now prove the lemma by contradiction. Suppose that *T* does not represent
    an optimal prefix-free code for *C*. Then there exists an optimal tree *T*″ such
    that *B*(*T*″) < *B*(*T*). Without loss of generality (by Lemma 15.2), *T*″ has
    *x* and *y* as siblings. Let *T*^(″′) be the tree *T*″ with the common parent
    of *x* and *y* replaced by a leaf *z* with frequency *z*.*freq* = *x*.*freq* +
    *y*.*freq*. Then
  prefs: []
  type: TYPE_NORMAL
- en: '| *B*(*T*‴) | = | *B*(*T*″) − *x*.*freq* − *y*.*freq* |'
  prefs: []
  type: TYPE_TB
- en: '|  | < | *B*(*T*) − *x*.*freq* − *y*.*freq* |'
  prefs: []
  type: TYPE_TB
- en: '|  | = | *B*(*T*′), |'
  prefs: []
  type: TYPE_TB
- en: yielding a contradiction to the assumption that *T*′ represents an optimal prefix-free
    code for *C*′. Thus, *T* must represent an optimal prefix-free code for the alphabet
    *C*.
  prefs: []
  type: TYPE_NORMAL
- en: ▪
  prefs: []
  type: TYPE_NORMAL
- en: '***Theorem 15.4***'
  prefs: []
  type: TYPE_NORMAL
- en: Procedure HUFFMAN produces an optimal prefix-free code.
  prefs: []
  type: TYPE_NORMAL
- en: '***Proof***   Immediate from Lemmas 15.2 and 15.3.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercises**'
  prefs: []
  type: TYPE_NORMAL
- en: '***15.3-1***'
  prefs: []
  type: TYPE_NORMAL
- en: Explain why, in the proof of Lemma 15.2, if *x*.*freq* = *b*.*freq*, then we
    must have *a*.*freq* = *b*.*freq* = *x*.*freq* = *y*.*freq*.
  prefs: []
  type: TYPE_NORMAL
- en: '***15.3-2***'
  prefs: []
  type: TYPE_NORMAL
- en: Prove that a non-full binary tree cannot correspond to an optimal prefix-free
    code.
  prefs: []
  type: TYPE_NORMAL
- en: '***15.3-3***'
  prefs: []
  type: TYPE_NORMAL
- en: What is an optimal Huffman code for the following set of frequencies, based
    on the first 8 Fibonacci numbers?
  prefs: []
  type: TYPE_NORMAL
- en: a:1 b:1 c:2 d:3 e:5 f:8 g:13 h:21
  prefs: []
  type: TYPE_NORMAL
- en: Can you generalize your answer to find the optimal code when the frequencies
    are the first *n* Fibonacci numbers?
  prefs: []
  type: TYPE_NORMAL
- en: '***15.3-4***'
  prefs: []
  type: TYPE_NORMAL
- en: Prove that the total cost *B*(*T*) of a full binary tree *T* for a code equals
    the sum, over all internal nodes, of the combined frequencies of the two children
    of the node.
  prefs: []
  type: TYPE_NORMAL
- en: '***15.3-5***'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given an optimal prefix-free code on a set *C* of *n* characters, you wish
    to transmit the code itself using as few bits as possible. Show how to represent
    any optimal prefix-free code on *C* using only 2*n* − 1 + *n* ⌈lg *n*⌉ bits. (*Hint*:
    Use 2*n* − 1 bits to specify the structure of the tree, as discovered by a walk
    of the tree.)'
  prefs: []
  type: TYPE_NORMAL
- en: '***15.3-6***'
  prefs: []
  type: TYPE_NORMAL
- en: Generalize Huffman’s algorithm to ternary codewords (i.e., codewords using the
    symbols 0, 1, and 2), and prove that it yields optimal ternary codes.
  prefs: []
  type: TYPE_NORMAL
- en: '***15.3-7***'
  prefs: []
  type: TYPE_NORMAL
- en: 'A data file contains a sequence of 8-bit characters such that all 256 characters
    are about equally common: the maximum character frequency is less than twice the
    minimum character frequency. Prove that Huffman coding in this case is no more
    efficient than using an ordinary 8-bit fixed-length code.'
  prefs: []
  type: TYPE_NORMAL
- en: '***15.3-8***'
  prefs: []
  type: TYPE_NORMAL
- en: 'Show that no lossless (invertible) compression scheme can guarantee that for
    every input file, the corresponding output file is shorter. (*Hint*: Compare the
    number of possible files with the number of possible encoded files.)'
  prefs: []
  type: TYPE_NORMAL
- en: '[**15.4    Offline caching**](toc.xhtml#Rh1-91)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Computer systems can decrease the time to access data by storing a subset of
    the main memory in the ***cache***: a small but faster memory. A cache organizes
    data into ***cache blocks*** typically comprising 32, 64, or 128 bytes. You can
    also think of main memory as a cache for disk-resident data in a virtual-memory
    system. Here, the blocks are called ***pages***, and 4096 bytes is a typical size.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a computer program executes, it makes a sequence of memory requests. Say
    that there are *n* memory requests, to data in blocks *b*[1], *b*[2], … , *b[n]*,
    in that order. The blocks in the access sequence might not be distinct, and indeed,
    any given block is usually accessed multiple times. For example, a program that
    accesses four distinct blocks *p*, *q*, *r*, *s* might make a sequence of requests
    to blocks *s*, *q*, *s*, *q*, *q*, *s*, *p*, *p*, *r*, *s*, *s*, *q*, *p*, *r*,
    *q*. The cache can hold up to some fixed number *k* of cache blocks. It starts
    out empty before the first request. Each request causes at most one block to enter
    the cache and at most one block to be evicted from the cache. Upon a request for
    block *b[i]*, any one of three scenarios may occur:'
  prefs: []
  type: TYPE_NORMAL
- en: Block *b[i]* is already in the cache, due to a previous request for the same
    block. The cache remains unchanged. This situation is known as a ***cache hit***.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Block *b[i]* is not in the cache at that time, but the cache contains fewer
    than *k* blocks. In this case, block *b[i]* is placed into the cache, so that
    the cache contains one more block than it did before the request.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Block *b[i]* is not in the cache at that time and the cache is full: it contains
    *k* blocks. Block *b[i]* is placed into the cache, but before that happens, some
    other block in the cache must be evicted from the cache in order to make room.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The latter two situations, in which the requested block is not already in the
    cache, are called ***cache misses***. The goal is to minimize the number of cache
    misses or, equivalently, to maximize the number of cache hits, over the entire
    sequence of *n* requests. A cache miss that occurs while the cache holds fewer
    than *k* blocks—that is, as the cache is first being filled up—is known as a ***compulsory
    miss***, since no prior decision could have kept the requested block in the cache.
    When a cache miss occurs and the cache is full, ideally the choice of which block
    to evict should allow for the smallest possible number of cache misses over the
    entire sequence of future requests.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, caching is an online problem. That is, the computer has to decide
    which blocks to keep in the cache without knowing the future requests. Here, however,
    let’s consider the offline version of this problem, in which the computer knows
    in advance the entire sequence of *n* requests and the cache size *k*, with a
    goal of minimizing the total number of cache misses.
  prefs: []
  type: TYPE_NORMAL
- en: 'To solve this offline problem, you can use a greedy strategy called ***furthest-in-future***,
    which chooses to evict the block in the cache whose next access in the request
    sequence comes furthest in the future. Intuitively, this strategy makes sense:
    if you’re not going to need something for a while, why keep it around? We’ll show
    that the furthest-in-future strategy is indeed optimal by showing that the offline
    caching problem exhibits optimal substructure and that furthest-in-future has
    the greedy-choice property.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, you might be thinking that since the computer usually doesn’t know the
    sequence of requests in advance, there is no point in studying the offline problem.
    Actually, there is. In some situations, you do know the sequence of requests in
    advance. For example, if you view the main memory as the cache and the full set
    of data as residing on disk (or a solid-state drive), there are algorithms that
    plan out the entire set of reads and writes in advance. Furthermore, we can use
    the number of cache misses produced by an optimal algorithm as a baseline for
    comparing how well online algorithms perform. We’ll do just that in [Section 27.3](chapter027.xhtml#Sec_27.3).
  prefs: []
  type: TYPE_NORMAL
- en: Offline caching can even model real-world problems. For example, consider a
    scenario where you know in advance a fixed schedule of *n* events at known locations.
    Events may occur at a location multiple times, not necessarily consecutively.
    You are managing a group of *k* agents, you need to ensure that you have one agent
    at each location when an event occurs, and you want to minimize the number of
    times that agents have to move. Here, the agents are like the blocks, the events
    are like the requests, and moving an agent is akin to a cache miss.
  prefs: []
  type: TYPE_NORMAL
- en: '**Optimal substructure of offline caching**'
  prefs: []
  type: TYPE_NORMAL
- en: To show that the offline problem exhibits optimal substructure, let’s define
    the subproblem (*C*, *i*) as processing requests for blocks *b[i]*, *b*[*i*+1],
    … , *b[n]* with cache configuration *C* at the time that the request for block
    *b[i]* occurs, that is, *C* is a subset of the set of blocks such that |*C* |
    ≤ *k*. A solution to subproblem (*C*, *i*) is a sequence of decisions that specifies
    which block to evict (if any) upon each request for blocks *b[i]*, *b*[*i*+1],
    … , *b[n]*. An optimal solution to subproblem (*C*, *i*) minimizes the number
    of cache misses.
  prefs: []
  type: TYPE_NORMAL
- en: Consider an optimal solution *S* to subproblem (*C*, *i*), and let *C*′ be the
    contents of the cache after processing the request for block *b[i]* in solution
    *S*. Let *S*′ be the subsolution of *S* for the resulting subproblem (*C*′, *i*
    + 1). If the request for *b[i]* results in a cache hit, then the cache remains
    unchanged, so that *C*′ = *C*. If the request for block *b[i]* results in a cache
    miss, then the contents of the cache change, so that *C*′ ≠ *C*. We claim that
    in either case, *S*′ is an optimal solution to subproblem (*C*′, *i* + 1). Why?
    If *S*′ is not an optimal solution to subproblem (*C*′, *i* + 1), then there exists
    another solution *S*″ to subproblem (*C*′, *i* + 1) that makes fewer cache misses
    than *S*′. Combining *S*″ with the decision of *S* at the request for block *b[i]*
    yields another solution that makes fewer cache misses than *S*, which contradicts
    the assumption that *S* is an optimal solution to subproblem (*C*, *i*).
  prefs: []
  type: TYPE_NORMAL
- en: 'To quantify a recursive solution, we need a little more notation. Let *R*[*C*,*i*]
    be the set of all cache configurations that can immediately follow configuration
    *C* after processing a request for block *b[i]*. If the request results in a cache
    hit, then the cache remains unchanged, so that *R*[*C*,*i*] = {*C* }. If the request
    for *b[i]* results in a cache miss, then there are two possibilities. If the cache
    is not full (|*C* | < *k*), then the cache is filling up and the only choice is
    to insert *b[i]* into the cache, so that *R*[*C*,*i*]= {*C* ∪ {*b[i]*}}. If the
    cache is full (|*C* | = *k*) upon a cache miss, then *R*[*C*,*i*] contains *k*
    potential configurations: one for each candidate block in *C* that could be evicted
    and replaced by block *b[i]*. In this case, *R*[*C*,*i*] = {(*C* − {*x*}) ∪ {*b[i]*}
    : *x* ∈ *C* }. For example, if *C* = {*p*, *q*, *r*}, *k* = 3, and block *s* is
    requested, then *R*[*C*,*i*] = {{*p*, *q*, *s*},{*p*, *r*, *s*},{*q*, *r*, *s*}}.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let *miss*(*C*, *i*) denote the minimum number of cache misses in a solution
    for subproblem (*C*, *i*). Here is a recurrence for *miss*(*C*, *i*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P504.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Greedy-choice property**'
  prefs: []
  type: TYPE_NORMAL
- en: To prove that the furthest-in-future strategy yields an optimal solution, we
    need to show that optimal offline caching exhibits the greedy-choice property.
    Combined with the optimal-substructure property, the greedy-choice property will
    prove that furthest-in-future produces the minimum possible number of cache misses.
  prefs: []
  type: TYPE_NORMAL
- en: '***Theorem 15.5 (Optimal offline caching has the greedy-choice property)***'
  prefs: []
  type: TYPE_NORMAL
- en: Consider a subproblem (*C*, *i*) when the cache *C* contains *k* blocks, so
    that it is full, and a cache miss occurs. When block *b[i]* is requested, let
    *z* = *b[m]* be the block in *C* whose next access is furthest in the future.
    (If some block in the cache will never again be referenced, then consider any
    such block to be block *z*, and add a dummy request for block *z* = *b[m]* = *b*[*n*+1].)
    Then evicting block *z* upon a request for block *b[i]* is included in some optimal
    solution for the subproblem (*C*, *i*).
  prefs: []
  type: TYPE_NORMAL
- en: '***Proof***   Let *S* be an optimal solution to subproblem (*C*, *i*). If *S*
    evicts block *z* upon the request for block *b[i]*, then we are done, since we
    have shown that some optimal solution includes evicting *z*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So now suppose that optimal solution *S* evicts some other block *x* when block
    *b[i]* is requested. We’ll construct another solution *S*′ to subproblem (*C*,
    *i*) which, upon the request for *b[i]*, evicts block *z* instead of *x* and induces
    no more cache misses than *S* does, so that *S*′ is also optimal. Because different
    solutions may yield different cache configurations, denote by *C*[*S*,*j*] the
    configuration of the cache under solution *S* just before the request for some
    block *b[j]*, and likewise for solution *S*′ and *C*[*S*′,*j*]. We’ll show how
    to construct *S*′ with the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: For *j* = *i* + 1, … , *m*, let *D[j]* = *C*[*S*,*j*] ∩ *C*[*S*′,*j*]. Then,
    |*D[j]* | ≥ *k* − 1, so that the cache configurations *C*[*S*,*j*] and *C*[*S*′,*j*]
    differ by at most one block. If they differ, then *C*[*S*,*j*] = *D[j]* ∪ {*z*}
    and *C*[*S*′,*j*] = *D[j]* ∪ {*y*} for some block *y* ≠ *z*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each request of blocks *b[i]*, … , *b*[*m*−1], if solution *S* has a cache
    hit, then solution *S*′ also has a cache hit.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For all *j* > *m*, the cache configurations *C*[*S*,*j*] and *C*[*S*′,*j*] are
    identical.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Over the sequence of requests for blocks *b[i]*, … , *b[m]*, the number of cache
    misses produced by solution *S*′ is at most the number of cache misses produced
    by solution *S*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We’ll prove inductively that these properties hold for each request.
  prefs: []
  type: TYPE_NORMAL
- en: We proceed by induction on *j*, for *j* = *i* +1, … , *m*. For the base case,
    the initial caches *C*[*S*,*i*] and *C*[*S*′,*i*] are identical. Upon the request
    for block *b[i]*, solution *S* evicts *x* and solution *S*′ evicts *z*. Thus,
    cache configurations *C*[*S*,*i*+1] and *C*[*S*′,*i*+1] differ by just one block,
    *C*[*S*,*i*+1] = *D*[*i*+1] ∪ {*z*}, *C*[*S*′,*i*+1] = *D*[*i*+1] ∪ {*x*}, and
    *x* ≠ *z*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The inductive step defines how solution *S*′ behaves upon a request for block
    *b[j]* for *i* + 1 ≤ *j* ≤ *m* − 1\. The inductive hypothesis is that property
    1 holds when *b[j]* is requested. Because *z* = *b[m]* is the block in *C*[*S*,*i*]
    whose next reference is furthest in the future, we know that *b[j]* ≠ *z*. We
    consider several scenarios:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If *C*[*S*,*j*] = *C*[*S*′,*j*] (so that |*D[j]* | = *k*), then solution *S*′
    makes the same decision upon the request for *b[j]* as *S* makes, so that *C*[*S*,*j*+1]
    = *C*[*S*′,*j*+1].
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If |*D[j]*| = *k* − 1 and *b[j]* ∈ *D[j]*, then both caches already contain
    block *b[j]*, and both solutions *S* and *S*′ have cache hits. Therefore, *C*[*S*,*j*+1]
    = *C*[*S*,*j*] and *C*[*S*′,*j*+1] = *C*[*S*′,*j*].
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If |*D*[*j*] | = *k* − 1 and *b[j]* ∉ *D[j]*, then because *C*[*S*,*j*] = *D[j]*
    ∪ {*z*} and *b[j]* ≠ *z*, solution *S* has a cache miss. It evicts either block
    *z* or some block *w* ∈ *D[j]*.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If solution *S* evicts block *z*, then *C*[*S*,*j*+1] = *D[j]* ∪ {*b[j]*}.
    There are two cases, depending on whether *b[j]* = *y*:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If *b[j]* = *y*, then solution *S*′ has a cache hit, so that *C*[*S*′,*j*+1]
    = *C*[*S*′,*j*] = *D[j]* ∪ {*b[j]*}. Thus, *C*[*S*,*j*+1] = *C*[*S*′,*j* +1].
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If *b[j]* ≠ *y*, then solution *S*′ has a cache miss. It evicts block *y*, so
    that *C*[*S*′,*j*+1] = *D[j]* ∪ {*b[j]* }, and again *C*[*S*,*j*+1] = *C*[*S*′,*j*+1].
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If solution *S* evicts some block *w* ∈ *D[j]*, then *C*[*S*,*j*+1] = (*D[j]*
    − {*w*}) ∪ {*b[j]*, *z*}. Once again, there are two cases, depending on whether
    *b[j]* = *y*:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If *b[j]* = *y*, then solution *S*′ has a cache hit, so that *C*[*S*′,*j*+1]
    = *C*[*S*′,*j*] = *D[j]* ∪ {*b[j]*}. Since *w* ∈ *D[j]* and *w* was not evicted
    by solution *S*′, we have *w* ∈ *C*[*S*′,*j* +1]. Therefore, *w* ∉ *D*[*j*+1]
    and *b[j]* ∈ *D*[*j*+1], so that *D*[*j*+1] = (*D[j]* − {*w*}) ∪ {*b[j]* }. Thus,
    *C*[*S*,*j*+1] = *D*[*j*+1] ∪ {*z*},*C*[*S*′,*j*+1] = *D*[*j* +1] ∪ {*w*}, and
    because *w* ≠ *z*, property 1 holds when block *b*[*j*+1] is requested. (In other
    words, block *w* replaces block *y* in property 1.)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If *b[j]* ≠ *y*, then solution *S*′ has a cache miss. It evicts block *w*, so
    that *C*[*S*′,*j* +1] = (*D[j]* − {*w*}) ∪ {*b[j]*, *y*}. Therefore, we have that
    *D*[*j*+1] = (*D[j]* − {*w*}) ∪ {*b[j]* } and so *C*[*S*,*j*+1] = *D*[*j*+1] ∪
    {*z*} and *C*[*S*′,*j*+1] = *D*[*j* +1] ∪ {*y*}.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In the above discussion about maintaining property 1, solution *S* may have
    a cache hit in only the first two cases, and solution *S*′ has a cache hit in
    these cases if and only if *S* does.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If *C*[*S*,*m*] = *C*[*S*′,*m*], then solution *S*′ makes the same decision
    upon the request for block *z* = *b[m]* as *S* makes, so that *C*[*S*,*m*+1] =
    *C*[*S*′,*m*+1]. If *C*[*S*,*m*] ≠ *C*[*S*′,*m*], then by property 1, *C*[*S*,*m*]
    = *D[m]*∪{*z*} and *C*[*S*′,*m*] = *D[m]*∪{*y*}, where *y* ≠ *z*. In this case,
    solution *S* has a cache hit, so that *C*[*S*,*m*+1] = *C*[*S*,*m*] = *D[m]* ∪
    {*z*}. Solution *S*′ evicts block *y* and brings in block *z*, so that *C*[*S*′,*m*+1]
    = *D[m]* ∪ {*z*} = *C*[*S*,*m*+1]. Thus, regardless of whether or not *C*[*S*,*m*]
    = *C*[*S*′,*m*], we have *C*[*S*,*m*+1] = *C*[*S*′,*m*+1], and starting with the
    request for block *b*[*m*+1], solution *S*′ simply makes the same decisions as
    *S*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'By property 2, upon the requests for blocks *b[i]*, … , *b*[*m*−1], whenever
    solution *S* has a cache hit, so does *S*′. Only the request for block *b[m]*
    = *z* remains to be considered. If *S* has a cache miss upon the request for *b[m]*,
    then regardless of whether *S*′ has a cache hit or a cache miss, we are done:
    *S*′ has at most the same number of cache misses as *S*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So now suppose that *S* has a cache hit and *S*′ has a cache miss upon the request
    for *b[m]*. We’ll show that there exists a request for at least one of blocks
    *b*[*i*+1], … , *b*[*m*−1] in which the request results in a cache miss for *S*
    and a cache hit for *S*′, thereby compensating for what happens upon the request
    for block *b[m]*. The proof is by contradiction. Assume that no request for blocks
    *b*[*i*+1], … , *b*[*m*−1] results in a cache miss for *S* and a cache hit for
    *S*′.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We start by observing that once the caches *C*[*S*,*j*] and *C*[*S*′*j*] are
    equal for some *j* > *i*, they remain equal thereafter. Observe also that if *b[m]*
    ∈ *C*[*S*,*m*] and *b[m]* ∉ *C*[*S*′,*m*], then *C*[*S*,*m*] ≠ *C*[*S*′,*m*].
    Therefore, solution *S* cannot have evicted block *z* upon the requests for blocks
    *b[i]*, … , *b*[*m*−1], for if it had, then these two cache configurations would
    be equal. The remaining possibility is that upon each of these requests, we had
    *C*[*S*,*j*] = *D*[*j*] ∪ {*z*}, *C*[*S*′,*j*] = *D[j]* ∪ {*y*} for some block
    *y* ≠ *z*, and solution *S* evicted some block *w* ∈ *D[j]*. Moreover, since none
    of these requests resulted in a cache miss for *S* and a cache hit for *S*^′,
    the case of *b[j]* = *y* never occurred. That is, for every request of blocks
    *b*[*i*+1], … , *b*[*m*−1], the requested block *b[j]* was never the block *y*
    ∈ *C*[*S*′,*j*] − *C*[*S*,*j*]. In these cases, after processing the request,
    we had *C*[*S*′,*j* +1] = *D*[*j* +1] ∪ {*y*}: the difference between the two
    caches did not change. Now, let’s go back to the request for block *b[i]*, where
    afterward, we had *C*[*S*′,*i*+1] = *D*[*i*+1] ∪ {*x*}. Because every succeeding
    request until requesting block *b[m]* did not change the difference between the
    caches, we had *C*[*S*′,*j*] = *D[j]* ∪ {*x*} for *j* = *i* + 1, … , *m*.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: By definition, block *z* = *b[m]* is requested after block *x*. That means at
    least one of blocks *b*[*i*+1], … , *b*[*m*−1] is block *x*. But for *j* = *i*
    + 1, … , *m*, we have *x* ∈ *C*[*S*′,*j*] and *x* ∉ *C*[*S*,*j*], so that at least
    one of these requests had a cache hit for *S*′ and a cache miss for *S*, a contradiction.
    We conclude that if solution *S* has a cache hit and solution *S*′ has a cache
    miss upon the request for block *b[m]*, then some earlier request had the opposite
    result, and so solution *S*′ produces no more cache misses than solution *S*.
    Since *S* is assumed to be optimal, *S*′ is optimal as well.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ▪
  prefs: []
  type: TYPE_NORMAL
- en: Along with the optimal-substructure property, Theorem 15.5 tells us that the
    furthest-in-future strategy yields the minimum number of cache misses.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercises**'
  prefs: []
  type: TYPE_NORMAL
- en: '***15.4-1***'
  prefs: []
  type: TYPE_NORMAL
- en: Write pseudocode for a cache manager that uses the furthest-in-future strategy.
    It should take as input a set *C* of blocks in the cache, the number of blocks
    *k* that the cache can hold, a sequence *b*[1], *b*[2], … , *b[n]* of requested
    blocks, and the index *i* into the sequence for the block *b[i]* being requested.
    For each request, it should print out whether a cache hit or cache miss occurs,
    and for each cache miss, it should also print out which block, if any, is evicted.
  prefs: []
  type: TYPE_NORMAL
- en: '***15.4-2***'
  prefs: []
  type: TYPE_NORMAL
- en: Real cache managers do not know the future requests, and so they often use the
    past to decide which block to evict. The ***least-recently-used***, or ***LRU***,
    strategy evicts the block that, of all blocks currently in the cache, was the
    least recently requested. (You can think of LRU as “furthest-in-past.”) Give an
    example of a request sequence in which the LRU strategy is not optimal, by showing
    that it induces more cache misses than the furthest-in-future strategy does on
    the same request sequence.
  prefs: []
  type: TYPE_NORMAL
- en: '***15.4-3***'
  prefs: []
  type: TYPE_NORMAL
- en: Professor Croesus suggests that in the proof of Theorem 15.5, the last clause
    in property 1 can change to *C*[*S*′,*j*] = *D[j]* ∪ {*x*} or, equivalently, require
    the block *y* given in property 1 to always be the block *x* evicted by solution
    *S* upon the request for block *b[i]*. Show where the proof breaks down with this
    requirement.
  prefs: []
  type: TYPE_NORMAL
- en: '***15.4-4***'
  prefs: []
  type: TYPE_NORMAL
- en: This section has assumed that at most one block is placed into the cache whenever
    a block is requested. You can imagine, however, a strategy in which multiple blocks
    may enter the cache upon a single request. Show that for every solution that allows
    multiple blocks to enter the cache upon each request, there is another solution
    that brings in only one block upon each request and is at least as good.
  prefs: []
  type: TYPE_NORMAL
- en: '**Problems**'
  prefs: []
  type: TYPE_NORMAL
- en: '***15-1     Coin changing***'
  prefs: []
  type: TYPE_NORMAL
- en: Consider the problem of making change for *n* cents using the smallest number
    of coins. Assume that each coin’s value is an integer.
  prefs: []
  type: TYPE_NORMAL
- en: '***a.*** Describe a greedy algorithm to make change consisting of quarters,
    dimes, nickels, and pennies. Prove that your algorithm yields an optimal solution.'
  prefs: []
  type: TYPE_NORMAL
- en: '***b.*** Suppose that the available coins are in denominations that are powers
    of *c*: the denominations are *c*⁰, *c*¹, … , *c^k* for some integers *c >* 1
    and *k* ≥ 1\. Show that the greedy algorithm always yields an optimal solution.'
  prefs: []
  type: TYPE_NORMAL
- en: '***c.*** Give a set of coin denominations for which the greedy algorithm does
    not yield an optimal solution. Your set should include a penny so that there is
    a solution for every value of *n*.'
  prefs: []
  type: TYPE_NORMAL
- en: '***d.*** Give an *O*(*nk*)-time algorithm that makes change for any set of
    *k* different coin denominations using the smallest number of coins, assuming
    that one of the coins is a penny.'
  prefs: []
  type: TYPE_NORMAL
- en: '***15-2     Scheduling to minimize average completion time***'
  prefs: []
  type: TYPE_NORMAL
- en: You are given a set *S* = {*a*[1], *a*[2], … , *a[n]*} of tasks, where task
    *a[i]* requires *p[i]* units of processing time to complete. Let *C[i]* be the
    ***completion time*** of task *a[i]*, that is, the time at which task *a[i]* completes
    processing. Your goal is to minimize the average completion time, that is, to
    minimize ![art](images/Art_P505.jpg). For example, suppose that there are two
    tasks *a*[1] and *a*[2] with *p*[1] = 3 and *p*[2] = 5, and consider the schedule
    in which *a*[2] runs first, followed by *a*[1]. Then we have *C*[2] = 5, *C*[1]
    = 8, and the average completion time is (5 + 8)/2 = 6.5\. If task *a*[1] runs
    first, however, then we have *C*[1] = 3, *C*[2] = 8, and the average completion
    time is (3 + 8)/2 = 5.5.
  prefs: []
  type: TYPE_NORMAL
- en: '***a.*** Give an algorithm that schedules the tasks so as to minimize the average
    completion time. Each task must run nonpreemptively, that is, once task *a[i]*
    starts, it must run continuously for *p[i]* units of time until it is done. Prove
    that your algorithm minimizes the average completion time, and analyze the running
    time of your algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: '***b.*** Suppose now that the tasks are not all available at once. That is,
    each task cannot start until its ***release time*** *b[i]*. Suppose also that
    tasks may be ***preempted***, so that a task can be suspended and restarted at
    a later time. For example, a task *a[i]* with processing time *p[i]* = 6 and release
    time *b[i]* = 1 might start running at time 1 and be preempted at time 4\. It
    might then resume at time 10 but be preempted at time 11, and it might finally
    resume at time 13 and complete at time 15\. Task *a[i]* has run for a total of
    6 time units, but its running time has been divided into three pieces. Give an
    algorithm that schedules the tasks so as to minimize the average completion time
    in this new scenario. Prove that your algorithm minimizes the average completion
    time, and analyze the running time of your algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Chapter notes**'
  prefs: []
  type: TYPE_NORMAL
- en: Much more material on greedy algorithms can be found in Lawler [[276](bibliography001.xhtml#endnote_276)]
    and Papadimitriou and Steiglitz [[353](bibliography001.xhtml#endnote_353)]. The
    greedy algorithm first appeared in the combinatorial optimization literature in
    a 1971 article by Edmonds [[131](bibliography001.xhtml#endnote_131)].
  prefs: []
  type: TYPE_NORMAL
- en: The proof of correctness of the greedy algorithm for the activity-selection
    problem is based on that of Gavril [[179](bibliography001.xhtml#endnote_179)].
  prefs: []
  type: TYPE_NORMAL
- en: Huffman codes were invented in 1952 [[233](bibliography001.xhtml#endnote_233)].
    Lelewer and Hirschberg [[294](bibliography001.xhtml#endnote_294)] surveys data-compression
    techniques known as of 1987.
  prefs: []
  type: TYPE_NORMAL
- en: The furthest-in-future strategy was proposed by Belady [[41](bibliography001.xhtml#endnote_41)],
    who suggested it for virtual-memory systems. Alternative proofs that furthest-in-future
    is optimal appear in articles by Lee et al. [[284](bibliography001.xhtml#endnote_284)]
    and Van Roy [[443](bibliography001.xhtml#endnote_443)].
  prefs: []
  type: TYPE_NORMAL
- en: '[¹](#footnote_ref_1) We sometimes refer to the sets *S[k]* as subproblems rather
    than as just sets of activities. The context will make it clear whether we are
    referring to *S[k]* as a set of activities or as a subproblem whose input is that
    set.'
  prefs: []
  type: TYPE_NORMAL
- en: '[²](#footnote_ref_2) Because the pseudocode takes *s* and *f* as arrays, it
    indexes into them with square brackets rather than with subscripts.'
  prefs: []
  type: TYPE_NORMAL
