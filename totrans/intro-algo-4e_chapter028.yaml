- en: '[**28        Matrix Operations**](toc.xhtml#chap-28)'
  prefs: []
  type: TYPE_NORMAL
- en: Because operations on matrices lie at the heart of scientific computing, efficient
    algorithms for working with matrices have many practical applications. This chapter
    focuses on how to multiply matrices and solve sets of simultaneous linear equations.
    [Appendix D](appendix004.xhtml) reviews the basics of matrices.
  prefs: []
  type: TYPE_NORMAL
- en: '[Section 28.1](chapter028.xhtml#Sec_28.1) shows how to solve a set of linear
    equations using LUP decompositions. Then, [Section 28.2](chapter028.xhtml#Sec_28.2)
    explores the close relationship between multiplying and inverting matrices. Finally,
    [Section 28.3](chapter028.xhtml#Sec_28.3) discusses the important class of symmetric
    positive-definite matrices and shows how to use them to find a least-squares solution
    to an overdetermined set of linear equations.'
  prefs: []
  type: TYPE_NORMAL
- en: One important issue that arises in practice is ***numerical stability***. Because
    actual computers have limits to how precisely they can represent floating-point
    numbers, round-off errors in numerical computations may become amplified over
    the course of a computation, leading to incorrect results. Such computations are
    called ***numerically unstable***. Although we’ll briefly consider numerical stability
    on occasion, we won’t focus on it in this chapter. We refer you to the excellent
    book by Higham [[216](bibliography001.xhtml#endnote_216)] for a thorough discussion
    of stability issues.
  prefs: []
  type: TYPE_NORMAL
- en: '[**28.1    Solving systems of linear equations**](toc.xhtml#Rh1-162)'
  prefs: []
  type: TYPE_NORMAL
- en: Numerous applications need to solve sets of simultaneous linear equations. A
    linear system can be cast as a matrix equation in which each matrix or vector
    element belongs to a field, typically the real numbers ℝ. This section discusses
    how to solve a system of linear equations using a method called LUP decomposition.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process starts with a set of linear equations in *n* unknowns *x*[1], *x*[2],
    … , *x[n]*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P897.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A ***solution*** to the equations (28.1) is a set of values for *x*[1], *x*[2],
    … , *x[n]* that satisfy all of the equations simultaneously. In this section,
    we treat only the case in which there are exactly *n* equations in *n* unknowns.
  prefs: []
  type: TYPE_NORMAL
- en: Next, rewrite equations (28.1) as the matrix-vector equation
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P898.jpg)'
  prefs: []
  type: TYPE_IMG
- en: or, equivalently, letting *A* = (*a[ij]*), *x* = (*x[i]*), and *b* = (*b[i]*),
    as
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P899.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If *A* is nonsingular, it possesses an inverse *A*^(−1), and
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P900.jpg)'
  prefs: []
  type: TYPE_IMG
- en: is the solution vector. We can prove that *x* is the unique solution to equation
    (28.2) as follows. If there are two solutions, *x* and *x*′, then *Ax* = *Ax*′
    = *b* and, letting *I* denote an identity matrix,
  prefs: []
  type: TYPE_NORMAL
- en: '| *x* | = | *Ix* |'
  prefs: []
  type: TYPE_TB
- en: '|  | = | (*A*^(−1)*A*)*x* |'
  prefs: []
  type: TYPE_TB
- en: '|  | = | *A*^(−1)(*Ax*) |'
  prefs: []
  type: TYPE_TB
- en: '|  | = | *A*^(−1)(*Ax*′) |'
  prefs: []
  type: TYPE_TB
- en: '|  | = | (*A*^(−1)*A*)*x*′ |'
  prefs: []
  type: TYPE_TB
- en: '|  | = | *Ix*′ |'
  prefs: []
  type: TYPE_TB
- en: '|  | = | *x*′. |'
  prefs: []
  type: TYPE_TB
- en: This section focuses on the case in which *A* is nonsingular or, equivalently
    (by Theorem D.1 on page 1220), the rank of *A* equals the number *n* of unknowns.
    There are other possibilities, however, which merit a brief discussion. If the
    number of equations is less than the number *n* of unknowns—or, more generally,
    if the rank of *A* is less than *n*—then the system is ***underdetermined***.
    An underdetermined system typically has infinitely many solutions, although it
    may have no solutions at all if the equations are inconsistent. If the number
    of equations exceeds the number *n* of unknowns, the system is ***overdetermined***,
    and there may not exist any solutions. [Section 28.3](chapter028.xhtml#Sec_28.3)
    addresses the important problem of finding good approximate solutions to overdetermined
    systems of linear equations.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s return to the problem of solving the system *Ax* = *b* of *n* equations
    in *n* unknowns. One option is to compute *A*^(−1) and then, using equation (28.3),
    multiply *b* by *A*^(−1), yielding *x* = *A*^(−1)*b*. This approach suffers in
    practice from numerical instability. Fortunately, another approach—LUP decomposition—is
    numerically stable and has the further advantage of being faster in practice.
  prefs: []
  type: TYPE_NORMAL
- en: '**Overview of LUP decomposition**'
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind LUP decomposition is to find three *n* × *n* matrices *L*, *U*,
    and *P* such that
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P901.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: '*L* is a unit lower-triangular matrix,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*U* is an upper-triangular matrix, and'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P* is a permutation matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We call matrices *L*, *U*, and *P* satisfying equation (28.4) an ***LUP decomposition***
    of the matrix *A*. We’ll show that every nonsingular matrix *A* possesses such
    a decomposition.
  prefs: []
  type: TYPE_NORMAL
- en: Computing an LUP decomposition for the matrix *A* has the advantage that linear
    systems can be efficiently solved when they are triangular, as is the case for
    both matrices *L* and *U*. If you have an LUP decomposition for *A*, you can solve
    equation (28.2), *Ax* = *b*, by solving only triangular linear systems, as follows.
    Multiply both sides of *Ax* = *b* by *P*, yielding the equivalent equation *PAx*
    = *Pb*. By Exercise D.1-4 on page 1219, multiplying both sides by a permutation
    matrix amounts to permuting the equations (28.1). By the decomposition (28.4),
    substituting *LU* for *PA* gives
  prefs: []
  type: TYPE_NORMAL
- en: '*LUx* = *Pb*.'
  prefs: []
  type: TYPE_NORMAL
- en: You can now solve this equation by solving two triangular linear systems. Define
    *y* = *Ux*, where *x* is the desired solution vector. First, solve the lower-triangular
    system
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P902.jpg)'
  prefs: []
  type: TYPE_IMG
- en: for the unknown vector *y* by a method called “forward substitution.” Having
    solved for *y*, solve the upper-triangular system
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P903.jpg)'
  prefs: []
  type: TYPE_IMG
- en: for the unknown *x* by a method called “back substitution.” Why does this process
    solve *Ax* = *b*? Because the permutation matrix *P* is invertible (see Exercise
    D.2-3 on page 1223), multiplying both sides of equation (28.4) by *P* ^(−1) gives
    *P*^(−1)*PA* = *P*^(−1)*LU*, so that
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P905.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Hence, the vector *x* that satisfies *Ux* = *y* is the solution to *Ax* = *b*:'
  prefs: []
  type: TYPE_NORMAL
- en: '| *Ax* | = | *P*^(−1)*LUx* | (by equation (28.7)) |'
  prefs: []
  type: TYPE_TB
- en: '|  | = | *P*^(−1)*Ly* | (by equation (28.6)) |'
  prefs: []
  type: TYPE_TB
- en: '|  | = | *P*^(−1)*Pb* | (by equation (28.5)) |'
  prefs: []
  type: TYPE_TB
- en: '|  | = | *b*. |'
  prefs: []
  type: TYPE_TB
- en: The next step is to show how forward and back substitution work and then attack
    the problem of computing the LUP decomposition itself.
  prefs: []
  type: TYPE_NORMAL
- en: '**Forward and back substitution**'
  prefs: []
  type: TYPE_NORMAL
- en: '***Forward substitution*** can solve the lower-triangular system (28.5) in
    Θ(*n*²) time, given *L*, *P*, and *b*. An array *π*[1 : *n*] provides a more compact
    format to represent the permutation *P* than an *n* × *n* matrix that is mostly
    0s. For *i* = 1, 2, … , *n*, the entry *π*[*i*] indicates that *P*[*i,π*[*i*]]
    = 1 and *P[ij]* = 0 for *j* ≠ *π*[*i*]. Thus, *PA* has *a*[*π*[*i*],*j*] in row
    *i* and column *j*, and *Pb* has *b*[*π*[*i*]] as its *i*th element. Since *L*
    is unit lower-triangular, the matrix equation *Ly* = *Pb* is equivalent to the
    *n* equations'
  prefs: []
  type: TYPE_NORMAL
- en: '|    *y*[1] | = | *b*[*π*[1]], |'
  prefs: []
  type: TYPE_TB
- en: '| *l*[21]*y*[1] + *y*[2] | = | *b*[*π*[2]], |'
  prefs: []
  type: TYPE_TB
- en: '| *l*[31]*y*[1] + *l*[32]*y*[2] + *y*[3] | = | *b*[*π*[3]], |'
  prefs: []
  type: TYPE_TB
- en: '|  | ⋮ |  |'
  prefs: []
  type: TYPE_TB
- en: '| *l*[*n*1]*y*[1] + *l*[*n*2]*y*[2] + *l*[*n*3]*y*[3] + ⋯ + *y[n]* | = | *b*[*π*[*n*]].
    |'
  prefs: []
  type: TYPE_TB
- en: The first equation gives *y*[1] = *b*[*π*[1]] directly. Knowing the value of
    *y*[1], you can substitute it into the second equation, yielding
  prefs: []
  type: TYPE_NORMAL
- en: '*y*[2] = *b*[*π*[2]] − *l*[21]*y*[1].'
  prefs: []
  type: TYPE_NORMAL
- en: Next, you can substitute both *y*[1] and *y*[2] into the third equation, obtaining
  prefs: []
  type: TYPE_NORMAL
- en: '*y*[3] = *b*[*π*[3]] − (*l*[31]*y*[1] + *l*[32]*y*[2]).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, you substitute *y*[1], *y*[2], … , *y*[*i*−1] “forward” into the
    *i*th equation to solve for *y[i]*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P906.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Once you’ve solved for *y*, you can solve for *x* in equation (28.6) using ***back
    substitution***, which is similar to forward substitution. This time, you solve
    the *n*th equation first and work backward to the first equation. Like forward
    substitution, this process runs in Θ(*n*²) time. Since *U* is upper-triangular,
    the matrix equation *Ux* = *y* is equivalent to the *n* equations
  prefs: []
  type: TYPE_NORMAL
- en: '| *u*[11]*x*[1] + *u*[12]*x*[2] + ⋯ + | *u*[1,*n*−2]*x*[*n*−2] + | *u*[1,*n*−1]*x*[*n*−1]
    + | *u*[1*n*]*x*[*n*] | = | *y*[1], |'
  prefs: []
  type: TYPE_TB
- en: '| *u*[22]*x*[2] + ⋯ + | *u*[2,*n*−2]*x*[*n*−2] + | *u*[2,*n*−1]*x*[*n*−1] +
    | *u*[2*n*]*x*[*n*] | = | *y*[2], |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  | ⋮ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | *u*[*n*−2,*n*−2]*x*[*n*−2] + | *u*[*n*−2,*n*−1]*x*[*n*−1] + | *u*[*n*−2,*n*]*x*[*n*]
    | = | *y*[*n*−2], |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | *u*[*n*−1,*n*−1]*x*[*n*−1] + | *u*[*n*−1,*n*]*x[n]* | = | *y*[*n*−1],
    |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | *u*[*n,n*]*x[n]* | = | *y[n]*. |'
  prefs: []
  type: TYPE_TB
- en: 'Thus, you can solve for *x[n]*, *x*[*n*−1], … , *x*[1] successively as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| *x[n]* | = | *y[n]*/*u*[*n,n*], |'
  prefs: []
  type: TYPE_TB
- en: '| *x*[*n*−1] | = | (*y*[*n*−1] − *u*[*n*−1,*n*]*x[n]*)/*u*[*n*−1,*n*−1], |'
  prefs: []
  type: TYPE_TB
- en: '| *x*[*n*−2] | = | (*y*[*n*−2] − (*u*[*n*−2,*n*−1]*x*[*n*−1] + *u*[*n*−2,*n*]*x[n]*))/*u*[*n*−2,*n*−2,]
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | ⋮ |  |'
  prefs: []
  type: TYPE_TB
- en: or, in general,
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P907.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Given *P*, *L*, *U*, and *b*, the procedure LUP-SOLVE on the next page solves
    for *x* by combining forward and back substitution. The permutation matrix *P*
    is represented by the array *π*. The procedure first solves for *y* using forward
    substitution in lines 2–3, and then it solves for *x* using backward substitution
    in lines 4–5\. Since the summation within each of the **for** loops includes an
    implicit loop, the running time is Θ(*n*²).
  prefs: []
  type: TYPE_NORMAL
- en: As an example of these methods, consider the system of linear equations defined
    by *Ax* = *b*, where
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P908.jpg)'
  prefs: []
  type: TYPE_IMG
- en: LUP-SOLVE(*L*, *U*, *π*, *b*, *n*)
  prefs: []
  type: TYPE_NORMAL
- en: '| 1 | let *x* and *y* be new vectors of length *n* |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | **for** *i* = 1 **to** *n* |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | ![art](images/Art_P909.jpg) |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | **for** *i* = *n* **downto** 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | ![art](images/Art_P910.jpg) |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | **return** *x* |'
  prefs: []
  type: TYPE_TB
- en: and we want to solve for the unknown *x*. The LUP decomposition is
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P911.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '(You might want to verify that *PA* = *LU*.) Using forward substitution, solve
    *Ly* = *Pb* for *y*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P912.jpg)'
  prefs: []
  type: TYPE_IMG
- en: obtaining
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P913.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'by computing first *y*[1], then *y*[2], and finally *y*[3]. Then, using back
    substitution, solve *Ux* = *y* for *x*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P914.jpg)'
  prefs: []
  type: TYPE_IMG
- en: thereby obtaining the desired answer
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P915.jpg)'
  prefs: []
  type: TYPE_IMG
- en: by computing first *x*[3], then *x*[2], and finally *x*[1].
  prefs: []
  type: TYPE_NORMAL
- en: '**Computing an LU decomposition**'
  prefs: []
  type: TYPE_NORMAL
- en: Given an LUP decomposition for a nonsingular matrix *A*, you can use forward
    and back substitution to solve the system *Ax* = *b* of linear equations. Now
    let’s see how to efficiently compute an LUP decomposition for *A*. We start with
    the simpler case in which *A* is an *n* × *n* nonsingular matrix and *P* is absent
    (or, equivalently, *P* = *I[n]*, the *n* × *n* identity matrix), so that *A* =
    *LU*. We call the two matrices *L* and *U* an ***LU decomposition*** of *A*.
  prefs: []
  type: TYPE_NORMAL
- en: To create an LU decomposition, we’ll use a process known as ***Gaussian elimination***.
    Start by subtracting multiples of the first equation from the other equations
    in order to remove the first variable from those equations. Then subtract multiples
    of the second equation from the third and subsequent equations so that now the
    first and second variables are removed from them. Continue this process until
    the system that remains has an upper-triangular form—this is the matrix *U*. The
    matrix *L* comprises the row multipliers that cause variables to be eliminated.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement this strategy, let’s start with a recursive formulation. The input
    is an *n* × *n* nonsingular matrix *A*. If *n* = 1, then nothing needs to be done:
    just choose *L* = *I*[1] and *U* = *A*. For *n* > 1, break *A* into four parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P916.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where *v* = (*a*[21], *a*[31], … , *a*[*n*1]) is a column (*n*−1)-vector, *w*^T
    = (*a*[12], *a*[13], … , *a*[1*n*])^T is a row (*n* − 1)-vector, and *A*′ is an
    (*n* − 1) × (*n* − 1) matrix. Then, using matrix algebra (verify the equations
    by simply multiplying through), factor *A* as
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P917.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The 0s in the first and second matrices of equation (28.9) are row and column
    (*n* − 1)-vectors, respectively. The term *vw*^T/*a*[11] is an (*n* − 1) × (*n*
    − 1) matrix formed by taking the outer product of *v* and *w* and dividing each
    element of the result by *a*[11]. Thus it conforms in size to the matrix *A*′
    from which it is subtracted. The resulting (*n* − 1) × (*n* − 1) matrix
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P918.jpg)'
  prefs: []
  type: TYPE_IMG
- en: is called the ***Schur complement*** of *A* with respect to *a*[11].
  prefs: []
  type: TYPE_NORMAL
- en: We claim that if *A* is nonsingular, then the Schur complement is nonsingular,
    too. Why? Suppose that the Schur complement, which is (*n* − 1) × (*n* − 1), is
    singular. Then by Theorem D.1, it has row rank strictly less than *n* − 1\. Because
    the bottom *n* − 1 entries in the first column of the matrix
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P919.jpg)'
  prefs: []
  type: TYPE_IMG
- en: are all 0, the bottom *n* − 1 rows of this matrix must have row rank strictly
    less than *n* − 1\. The row rank of the entire matrix, therefore, is strictly
    less than *n*. Applying Exercise D.2-8 on page 1223 to equation (28.9), *A* has
    rank strictly less than *n*, and from Theorem D.1, we derive the contradiction
    that *A* is singular.
  prefs: []
  type: TYPE_NORMAL
- en: Because the Schur complement is nonsingular, it, too, has an LU decomposition,
    which we can find recursively. Let’s say that
  prefs: []
  type: TYPE_NORMAL
- en: '*A*′ − *vw*^T/*a*[11] = *L*′*U*′,'
  prefs: []
  type: TYPE_NORMAL
- en: where *L*′ is unit lower-triangular and *U*′ is upper-triangular. The LU decomposition
    of *A* is then *A* = *LU*, with
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P920.jpg)'
  prefs: []
  type: TYPE_IMG
- en: as shown by
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P921.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Because *L*′ is unit lower-triangular, so is *L*, and because *U*′ is upper-triangular,
    so is *U*.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, if *a*[11] = 0, this method doesn’t work, because it divides by 0\.
    It also doesn’t work if the upper leftmost entry of the Schur complement *A*′
    − *vw*^T/*a*[11] is 0, since the next step of the recursion will divide by it.
    The denominators in each step of LU decomposition are called ***pivots***, and
    they occupy the diagonal elements of the matrix *U*. The permutation matrix *P*
    included in LUP decomposition provides a way to avoid dividing by 0, as we’ll
    see below. Using permutations to avoid division by 0 (or by small numbers, which
    can contribute to numerical instability), is called ***pivoting***.
  prefs: []
  type: TYPE_NORMAL
- en: An important class of matrices for which LU decomposition always works correctly
    is the class of symmetric positive-definite matrices. Such matrices require no
    pivoting to avoid dividing by 0 in the recursive strategy outlined above. We will
    prove this result, as well as several others, in [Section 28.3](chapter028.xhtml#Sec_28.3).
  prefs: []
  type: TYPE_NORMAL
- en: The pseudocode in the procedure LU-DECOMPOSITION follows the recursive strategy,
    except that an iteration loop replaces the recursion. (This transformation is
    a standard optimization for a “tail-recursive” procedure—one whose last operation
    is a recursive call to itself. See Problem 7-5 on page 202.) The procedure initializes
    the matrix *U* with 0s below the diagonal and matrix *L* with 1s on its diagonal
    and 0s above the diagonal. Each iteration works on a square submatrix, using its
    upper leftmost element as the pivot to compute the *v* and *w* vectors and the
    Schur complement, which becomes the square submatrix worked on by the next iteration.
  prefs: []
  type: TYPE_NORMAL
- en: LU-DECOMPOSITION(*A*, *n*)
  prefs: []
  type: TYPE_NORMAL
- en: '|   1 | let *L* and *U* be new *n* × *n* matrices |'
  prefs: []
  type: TYPE_TB
- en: '|   2 | initialize *U* with 0s below the diagonal |'
  prefs: []
  type: TYPE_TB
- en: '|   3 | initialize *L* with 1s on the diagonal and 0s above the diagonal |'
  prefs: []
  type: TYPE_TB
- en: '|   4 | **for** *k* = 1 **to** *n* |'
  prefs: []
  type: TYPE_TB
- en: '|   5 | *u[kk]* = *a[kk]* |'
  prefs: []
  type: TYPE_TB
- en: '|   6 | **for** *i* = *k* + 1 **to** *n* |'
  prefs: []
  type: TYPE_TB
- en: '|   7 | *l[ik]* = *a[ik]*/*a[kk]* | **//** *a[ik]* holds *v[i]* |'
  prefs: []
  type: TYPE_TB
- en: '|   8 | *u[ki]* = *a[ki]* | **//** *a[ki]* holds *w[i]* |'
  prefs: []
  type: TYPE_TB
- en: '|   9 | **for** *i* = *k* + 1 **to** *n* | **//** compute the Schur complement
    … |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | **for** *j* = *k* + 1 **to** *n* |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | *a[ij]* = *a[ij]* − *l[ik]u[kj]* | **//** … and store it back into *A*
    |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | **return** *L* and *U* |'
  prefs: []
  type: TYPE_TB
- en: Each recursive step in the description above takes place in one iteration of
    the outer **for** loop of lines 4–11\. Within this loop, line 5 determines the
    pivot to be *u[kk]* = *a[kk]*. The **for** loop in lines 6–8 (which does not execute
    when *k* = *n*) uses the *v* and *w* vectors to update *L* and *U*. Line 7 determines
    the below-diagonal elements of *L*, storing *v[i]*/*a[kk]* in *l[ik]*, and line
    8 computes the above-diagonal elements of *U*, storing *w[i]* in *u[ki]*. Finally,
    lines 9–11 compute the elements of the Schur complement and store them back into
    the matrix *A*. (There is no need to divide by *a[kk]* in line 11 because that
    already happened when line 7 computed *l[ik]*.) Because line 11 is triply nested,
    LU-DECOMPOSITION runs in Θ(*n*³) time.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 28.1](chapter028.xhtml#Fig_28-1) illustrates the operation of LU-DECOMPOSITION.
    It shows a standard optimization of the procedure that stores the significant
    elements of *L* and *U* in place in the matrix *A*. Each element *a[ij]* corresponds
    to either *l[ij]* (if *i* > *j*) or *u[ij]* (if *i* ≤ *j*), so that the matrix
    *A* holds both *L* and *U* when the procedure terminates. To obtain the pseudocode
    for this optimization from the pseudocode for the LU-DECOMPOSITION procedure,
    just replace each reference to *l* or *u* by *a*. You can verify that this transformation
    preserves correctness.'
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P922.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 28.1** The operation of LU-DECOMPOSITION. **(a)** The matrix *A*.
    **(b)** The result of the first iteration of the outer **for** loop of lines 4–11\.
    The element *a*[11] = 2 highlighted in blue is the pivot, the tan column is *v*/*a*[11],
    and the tan row is *w*^T. The elements of *U* computed thus far are above the
    horizontal line, and the elements of *L* are to the left of the vertical line.
    The Schur complement matrix *A*′ − *vw*^T/*a*[11] occupies the lower right. **(c)**
    The result of the next iteration of the outer **for** loop, on the Schur complement
    matrix from part (b). The element *a*[22] = 4 highlighted in blue is the pivot,
    and the tan column and row are *v*/*a*[22] and *w*^T (in the partitioning of the
    Schur complement), respectively. Lines divide the matrix into the elements of
    *U* computed so far (above), the elements of *L* computed so far (left), and the
    new Schur complement (lower right). **(d)** After the next iteration, the matrix
    *A* is factored. The element 3 in the new Schur complement becomes part of *U*
    when the recursion terminates.) **(e)** The factorization *A* = *LU*.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Computing an LUP decomposition**'
  prefs: []
  type: TYPE_NORMAL
- en: If the diagonal of the matrix given to LU-DECOMPOSITION contains any 0s, then
    the procedure will attempt to divide by 0, which would cause disaster. Even if
    the diagonal contains no 0s, but does have numbers with small absolute values,
    dividing by such numbers can cause numerical instabilities. Therefore, LUP decomposition
    pivots on entries with the largest absolute values that it can find.
  prefs: []
  type: TYPE_NORMAL
- en: In LUP decomposition, the input is an *n* × *n* nonsingular matrix *A*, with
    a goal of finding a permutation matrix *P*, a unit lower-triangular matrix *L*,
    and an upper-triangular matrix *U* such that *PA* = *LU*. Before partitioning
    the matrix *A*, as LU decomposition does, LUP decomposition moves a nonzero element,
    say *a*[*k*1], from somewhere in the first column to the (1, 1) position of the
    matrix. For the greatest numerical stability, LUP decomposition chooses the element
    in the first column with the greatest absolute value as *a*[*k*1]. (The first
    column cannot contain only 0s, for then *A* would be singular, because its determinant
    would be 0, by Theorems D.4 and D.5 on page 1221.) In order to preserve the set
    of equations, LUP decomposition exchanges row 1 with row *k*, which is equivalent
    to multiplying *A* by a permutation matrix *Q* on the left (Exercise D.1-4 on
    page 1219). Thus, the analog to equation (28.8) expresses *QA* as
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P923.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'where *v* = (*a*[21], *a*[31], … , *a*[*n*1]), except that *a*[11] replaces
    *a*[*k*1]; *w*^T = (*a*[*k*2], *a*[*k*3], … , *a[kn]*)^T; and *A*′ is an (*n*
    − 1) × (*n* − 1) matrix. Since *a*[*k*1] ≠ 0, the analog to equation (28.9) guarantees
    no division by 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P924.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Just as in LU decomposition, if *A* is nonsingular, then the Schur complement
    *A*′ − *vw*^T/*a*[*k*1] is nonsingular, too. Therefore, you can recursively find
    an LUP decomposition for it, with unit lower-triangular matrix *L*′, upper-triangular
    matrix *U*′, and permutation matrix *P*′, such that
  prefs: []
  type: TYPE_NORMAL
- en: '*P*′(*A*′ − *vw*^T/*a*[*k*1]) = *L*′*U*′.'
  prefs: []
  type: TYPE_NORMAL
- en: Define
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P925.jpg)'
  prefs: []
  type: TYPE_IMG
- en: which is a permutation matrix, since it is the product of two permutation matrices
    (Exercise D.1-4 on page 1219). This definition of *P* gives
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P926.jpg)'
  prefs: []
  type: TYPE_IMG
- en: which yields the LUP decomposition. Because *L*′ is unit lower-triangular, so
    is *L*, and because *U*′ is upper-triangular, so is *U*.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that in this derivation, unlike the one for LU decomposition, both the
    column vector *v*/*a*[*k*1] and the Schur complement *A*′ − *vw*^T/*a*[*k*1] are
    multiplied by the permutation matrix *P*′. The procedure LUP-DECOMPOSITION gives
    the pseudocode for LUP decomposition.
  prefs: []
  type: TYPE_NORMAL
- en: LUP-DECOMPOSITION(*A*, *n*)
  prefs: []
  type: TYPE_NORMAL
- en: '|   1 | let *π*[1 : *n*] be a new array |'
  prefs: []
  type: TYPE_TB
- en: '|   2 | **for** *i* = 1 **to** *n* |'
  prefs: []
  type: TYPE_TB
- en: '|   3 | *π*[*i*] = *i* | **//** initialize *π* to the identity permutation
    |'
  prefs: []
  type: TYPE_TB
- en: '|   4 | **for** *k* = 1 **to** *n* |'
  prefs: []
  type: TYPE_TB
- en: '|   5 | *p* = 0 |'
  prefs: []
  type: TYPE_TB
- en: '|   6 | **for** *i* = *k* **to** *n* | **//** find largest absolute value in
    column *k* |'
  prefs: []
  type: TYPE_TB
- en: '|   7 | **if** &#124;*a[ik]*&#124; > *p* |'
  prefs: []
  type: TYPE_TB
- en: '|   8 | *p* = &#124;*a[ik]*&#124; |'
  prefs: []
  type: TYPE_TB
- en: '|   9 | *k*′ = *i* | **//** row number of the largest found so far |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | **if** *p* == 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | **error** “singular matrix” |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | exchange *π*[*k*] with *π*[*k*′] |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | **for** *i* = 1 **to** *n* |'
  prefs: []
  type: TYPE_TB
- en: '| 14 | exchange *a[ki]* with *a[k′i]* | **//** exchange rows *k* and *k*′ |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | **for** *i* = *k* + 1 **to** *n* |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | *a[ik]* = *a[ik]*/*a[kk]* |'
  prefs: []
  type: TYPE_TB
- en: '| 17 | **for** *j* = *k* + 1 **to** *n* |'
  prefs: []
  type: TYPE_TB
- en: '| 18 | *a[ij]* = *a[ij]* − *a[ik]a[kj]* | **//** compute *L* and *U* in place
    in *A* |'
  prefs: []
  type: TYPE_TB
- en: Like LU-DECOMPOSITION, the LUP-DECOMPOSITION procedure replaces the recursion
    with an iteration loop. As an improvement over a direct implementation of the
    recursion, the procedure dynamically maintains the permutation matrix *P* as an
    array *π*, where *π*[*i*] = *j* means that the *i*th row of *P* contains a 1 in
    column *j*. The LUP-DECOMPOSITION procedure also implements the improvement mentioned
    earlier, computing *L* and *U* in place in the matrix *A*. Thus, when the procedure
    terminates,
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P927.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 28.2](chapter028.xhtml#Fig_28-2) illustrates how LUP-DECOMPOSITION
    factors a matrix. Lines 2–3 initialize the array *π* to represent the identity
    permutation. The outer **for** loop of lines 4–18 implements the recursion, finding
    an LUP decomposition of the (*n* − *k* + 1) × (*n* − *k* + 1) submatrix whose
    upper left is in row *k* and column *k*. Each time through the outer loop, lines
    5–9 determine the element *a*[*k′k*] with the largest absolute value of those
    in the current first column (column *k*) of the (*n* − *k* + 1) × (*n* − *k* +
    1) submatrix that the procedure is currently working on. If all elements in the
    current first column are 0, lines 10–11 report that the matrix is singular. To
    pivot, line 12 exchanges *π*[*k*′] with *π*[*k*], and lines 13–14 exchange the
    *k*th and *k*′th rows of *A*, thereby making the pivot element *a[kk]*. (The entire
    rows are swapped because in the derivation of the method above, not only is *A*′
    − *vw*^T/*a*[*k*1] multiplied by *P*′, but so is *v*/*a*[*k*1].) Finally, the
    Schur complement is computed by lines 15–18 in much the same way as it is computed
    by lines 6–11 of LU-DECOMPOSITION, except that here the operation is written to
    work in place.'
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P928.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 28.2** The operation of LUP-DECOMPOSITION. **(a)** The input matrix
    *A* with the identity permutation of the rows in yellow on the left. The first
    step of the algorithm determines that the element 5 highlighted in blue in the
    third row is the pivot for the first column. **(b)** Rows 1 and 3 are swapped
    and the permutation is updated. The tan column and row represent *v* and *w*^T.
    **(c)** The vector *v* is replaced by *v*/5, and the lower right of the matrix
    is updated with the Schur complement. Lines divide the matrix into three regions:
    elements of *U* (above), elements of *L* (left), and elements of the Schur complement
    (lower right). **(d)–(f)** The second step. **(g)–(i)** The third step. No further
    changes occur on the fourth (final) step. **(j)** The LUP decomposition *PA* =
    *LU*.'
  prefs: []
  type: TYPE_NORMAL
- en: Because of its triply nested loop structure, LUP-DECOMPOSITION has a running
    time of Θ(*n*³), which is the same as that of LU-DECOMPOSITION. Thus, pivoting
    costs at most a constant factor in time.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercises**'
  prefs: []
  type: TYPE_NORMAL
- en: '***28.1-1***'
  prefs: []
  type: TYPE_NORMAL
- en: Solve the equation
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P929.jpg)'
  prefs: []
  type: TYPE_IMG
- en: by using forward substitution.
  prefs: []
  type: TYPE_NORMAL
- en: '***28.1-2***'
  prefs: []
  type: TYPE_NORMAL
- en: Find an LU decomposition of the matrix
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P930.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '***28.1-3***'
  prefs: []
  type: TYPE_NORMAL
- en: Solve the equation
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P931.jpg)'
  prefs: []
  type: TYPE_IMG
- en: by using an LUP decomposition.
  prefs: []
  type: TYPE_NORMAL
- en: '***28.1-4***'
  prefs: []
  type: TYPE_NORMAL
- en: Describe the LUP decomposition of a diagonal matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '***28.1-5***'
  prefs: []
  type: TYPE_NORMAL
- en: Describe the LUP decomposition of a permutation matrix, and prove that it is
    unique.
  prefs: []
  type: TYPE_NORMAL
- en: '***28.1-6***'
  prefs: []
  type: TYPE_NORMAL
- en: Show that for all *n* ≥ 1, there exists a singular *n* × *n* matrix that has
    an LU decomposition.
  prefs: []
  type: TYPE_NORMAL
- en: '***28.1-7***'
  prefs: []
  type: TYPE_NORMAL
- en: In LU-DECOMPOSITION, is it necessary to perform the outermost **for** loop iteration
    when *k* = *n*? How about in LUP-DECOMPOSITION?
  prefs: []
  type: TYPE_NORMAL
- en: '[**28.2    Inverting matrices**](toc.xhtml#Rh1-163)'
  prefs: []
  type: TYPE_NORMAL
- en: Although you can use equation (28.3) to solve a system of linear equations by
    computing a matrix inverse, in practice you are better off using more numerically
    stable techniques, such as LUP decomposition. Sometimes, however, you really do
    need to compute a matrix inverse. This section shows how to use LUP decomposition
    to compute a matrix inverse. It also proves that matrix multiplication and computing
    the inverse of a matrix are equivalently hard problems, in that (subject to technical
    conditions) an algorithm for one can solve the other in the same asymptotic running
    time. Thus, you can use Strassen’s algorithm (see [Section 4.2](chapter004.xhtml#Sec_4.2))
    for matrix multiplication to invert a matrix. Indeed, Strassen’s original paper
    was motivated by the idea that a set of a linear equations could be solved more
    quickly than by the usual method.
  prefs: []
  type: TYPE_NORMAL
- en: '**Computing a matrix inverse from an LUP decomposition**'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that you have an LUP decomposition of a matrix *A* in the form of three
    matrices *L*, *U*, and *P* such that *PA* = *LU*. Using LUP-SOLVE, you can solve
    an equation of the form *Ax* = *b* in Θ(*n*²) time. Since the LUP decomposition
    depends on *A* but not *b*, you can run LUP-SOLVE on a second set of equations
    of the form *Ax* = *b*′ in Θ(*n*²) additional time. In general, once you have
    the LUP decomposition of *A*, you can solve, in Θ(*kn*²) time, *k* versions of
    the equation *Ax* = *b* that differ only in the vector *b*.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s think of the equation
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P932.jpg)'
  prefs: []
  type: TYPE_IMG
- en: which defines the matrix *X*, the inverse of *A*, as a set of *n* distinct equations
    of the form *Ax* = *b*. To be precise, let *X[i]* denote the *i*th column of *X*,
    and recall that the unit vector *e[i]* is the *i*th column of *I[n]*. You can
    then solve equation (28.11) for *X* by using the LUP decomposition for *A* to
    solve each equation
  prefs: []
  type: TYPE_NORMAL
- en: '*AX[i]* = *e[i]*'
  prefs: []
  type: TYPE_NORMAL
- en: separately for *X[i]*. Once you have the LUP decomposition, you can compute
    each of the *n* columns *X[i]* in Θ(*n*²) time, and so you can compute *X* from
    the LUP decomposition of *A* in Θ(*n*³) time. Since you find the LUP decomposition
    of *A* in Θ(*n*³) time, you can compute the inverse *A*^(−1) of a matrix *A* in
    Θ(*n*³) time.
  prefs: []
  type: TYPE_NORMAL
- en: '**Matrix multiplication and matrix inversion**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s see how the theoretical speedups obtained for matrix multiplication
    translate to speedups for matrix inversion. In fact, we’ll prove something stronger:
    matrix inversion is equivalent to matrix multiplication, in the following sense.
    If *M*(*n*) denotes the time to multiply two *n* × *n* matrices, then a nonsingular
    *n* × *n* matrix can be inverted in *O*(*M*(*n*)) time. Moreover, if *I*(*n*)
    denotes the time to invert a nonsingular *n* × *n* matrix, then two *n* × *n*
    matrices can be multiplied in *O*(*I*(*n*)) time. We prove these results as two
    separate theorems.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Theorem 28.1 (Multiplication is no harder than inversion)***'
  prefs: []
  type: TYPE_NORMAL
- en: If an *n* × *n* matrix can be inverted in *I*(*n*) time, where *I*(*n*) = Ω(*n*²)
    and *I*(*n*) satisfies the regularity condition *I*(3*n*) = *O*(*I*(*n*)), then
    two *n* × *n* matrices can be multiplied in *O*(*I*(*n*)) time.
  prefs: []
  type: TYPE_NORMAL
- en: '***Proof***   Let *A* and *B* be *n* × *n* matrices. To compute their product
    *C* = *AB*, define the 3*n* × 3*n* matrix *D* by'
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P933.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The inverse of *D* is
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P934.jpg)'
  prefs: []
  type: TYPE_IMG
- en: and thus to compute the product *AB*, just take the upper right *n* × *n* submatrix
    of *D*^(−1).
  prefs: []
  type: TYPE_NORMAL
- en: Constructing matrix *D* takes Θ(*n*²) time, which is *O*(*I*(*n*)) from the
    assumption that *I*(*n*) = Ω(*n*²), and inverting *D* takes *O*(*I*(3*n*)) = *O*(*I*(*n*))
    time, by the regularity condition on *I*(*n*). We thus have *M*(*n*) = *O*(*I*(*n*)).
  prefs: []
  type: TYPE_NORMAL
- en: ▪
  prefs: []
  type: TYPE_NORMAL
- en: Note that *I*(*n*) satisfies the regularity condition whenever *I*(*n*) = Θ(*n^c*
    lg^(*d*)*n*) for any constants *c* > 0 and *d* ≥ 0.
  prefs: []
  type: TYPE_NORMAL
- en: The proof that matrix inversion is no harder than matrix multiplication relies
    on some properties of symmetric positive-definite matrices proved in [Section
    28.3](chapter028.xhtml#Sec_28.3).
  prefs: []
  type: TYPE_NORMAL
- en: '***Theorem 28.2 (Inversion is no harder than multiplication)***'
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose that two *n* × *n* real matrices can be multiplied in *M*(*n*) time,
    where *M*(*n*) = Ω(*n*²) and *M*(*n*) satisfies the following two regularity conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: '*M*(*n* + *k*) = *O*(*M*(*n*)) for any *k* in the range 0 ≤ *k* < *n*, and'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*M*(*n*/2) ≤ *cM*(*n*) for some constant *c* < 1/2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then the inverse of any real nonsingular *n*×*n* matrix can be computed in *O*(*M*(*n*))
    time.
  prefs: []
  type: TYPE_NORMAL
- en: '***Proof***   Let *A* be an *n* × *n* matrix with real-valued entries that
    is nonsingular. Assume that *n* is an exact power of 2 (i.e., *n* = 2*^l* for
    some integer *l*); we’ll see at the end of the proof what to do if *n* is not
    an exact power of 2.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the moment, assume that the *n* × *n* matrix *A* is symmetric and positive-definite.
    Partition each of *A* and its inverse *A*^(−1) into four *n*/2 × *n*/2 submatrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P935.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Then, if we let
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P936.jpg)'
  prefs: []
  type: TYPE_IMG
- en: be the Schur complement of *A* with respect to *B* (we’ll see more about this
    form of Schur complement in [Section 28.3](chapter028.xhtml#Sec_28.3)), we have
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P937.jpg)'
  prefs: []
  type: TYPE_IMG
- en: since *AA*^(−1) = *I[n]*, as you can verify by performing the matrix multiplication.
    Because *A* is symmetric and positive-definite, Lemmas 28.4 and 28.5 in [Section
    28.3](chapter028.xhtml#Sec_28.3) imply that *B* and *S* are both symmetric and
    positive-definite. By Lemma 28.3 in [Section 28.3](chapter028.xhtml#Sec_28.3),
    therefore, the inverses *B*^(−1) and *S*^(−1) exist, and by Exercise D.2-6 on
    page 1223, *B*^(−1) and *S*^(−1) are symmetric, so that (*B*^(−1))^T = *B*^(−1)
    and (*S*^(−1))^T = *S*^(−1). Therefore, to compute the submatrices
  prefs: []
  type: TYPE_NORMAL
- en: '| *R* | = | *B*^(−1) + *B*^(−1)*C*^T*S*^(−1)*CB*^(−1), |'
  prefs: []
  type: TYPE_TB
- en: '| *T* | = | −*B*^(−1)*C*^T*S*^(−1), |'
  prefs: []
  type: TYPE_TB
- en: '| *U* | = | −*S*^(−1)*CB*^(−1), and |'
  prefs: []
  type: TYPE_TB
- en: '| *V* | = | *S*^(−1) |'
  prefs: []
  type: TYPE_TB
- en: 'of *A*^(−1), do the following, where all matrices mentioned are *n*/2 × *n*/2:'
  prefs: []
  type: TYPE_NORMAL
- en: Form the submatrices *B*, *C*, *C*^T, and *D* of *A*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Recursively compute the inverse *B*^(−1) of *B*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the matrix product *W* = *CB*^(−1), and then compute its transpose *W*^T,
    which equals *B*^(−1)*C*^T (by Exercise D.1-2 on page 1219 and (*B*^(−1))^T =
    *B*^(−1)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the matrix product *X* = *WC*^T, which equals *CB*^(−1)*C*^T, and then
    compute the matrix *S* = *D* − *X* = *D* − *CB*^(−1)*C*^T.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Recursively compute the inverse *S*^(−1) of *S*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the matrix product *Y* = *S*^(−1)*W*, which equals *S*^(−1)*CB*^(−1),
    and then compute its transpose *Y*^T, which equals *B*^(−1)*C*^T*S*^(−1) (by Exercise
    D.1-2, (*B*^(−1))^T = *B*^(−1), and (*S*^(−1))^T = *S*^(−1)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the matrix product *Z* = *W*^T*Y*, which equals *B*^(−1)*C*^T*S*^(−1)*CB*^(−1).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *R* = *B*^(−1) + *Z*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *T* = −*Y*^T.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *U* = −*Y*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *V* = *S*^(−1).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Thus, to invert an *n*×*n* symmetric positive-definite matrix, invert two *n*/2×*n*/2
    matrices in steps 2 and 5; perform four multiplications of *n*/2 × *n*/2 matrices
    in steps 3, 4, 6, and 7; plus incur an additional cost of *O*(*n*²) for extracting
    submatrices from *A*, inserting submatrices into *A*^(−1), and performing a constant
    number of additions, subtractions, and transposes on *n*/2 × *n*/2 matrices. The
    running time is given by the recurrence
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P938.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The second line follows from the assumption that *M*(*n*) = Ω(*n*²) and from
    the second regularity condition in the statement of the theorem, which implies
    that 4*M*(*n*/2) < 2*M*(*n*). Because *M*(*n*) = Ω(*n*²), case 3 of the master
    theorem (Theorem 4.1) applies to the recurrence (28.15), giving the *O*(*M*(*n*))
    result.
  prefs: []
  type: TYPE_NORMAL
- en: It remains to prove how to obtain the same asymptotic running time for matrix
    multiplication as for matrix inversion when *A* is invertible but not symmetric
    and positive-definite. The basic idea is that for any nonsingular matrix *A*,
    the matrix *A*^T*A* is symmetric (by Exercise D.1-2) and positive-definite (by
    Theorem D.6 on page 1222). The trick, then, is to reduce the problem of inverting
    *A* to the problem of inverting *A*^T*A*.
  prefs: []
  type: TYPE_NORMAL
- en: The reduction is based on the observation that when *A* is an *n* × *n* nonsingular
    matrix, we have
  prefs: []
  type: TYPE_NORMAL
- en: '*A*^(−1) = (*A*^T*A*)^(−1)*A*^T,'
  prefs: []
  type: TYPE_NORMAL
- en: since ((*A*^T*A*)^(−1)*A*^T)*A* = (*A*^T*A*)^(−1)(*A*^T*A*) = *I[n]* and a matrix
    inverse is unique. Therefore, to compute *A*^(−1), first multiply *A*^T by *A*
    to obtain *A*^T*A*, then invert the symmetric positive-definite matrix *A*^T*A*
    using the above divide-and-conquer algorithm, and finally multiply the result
    by *A*^T. Each of these three steps takes *O*(*M*(*n*)) time, and thus any nonsingular
    matrix with real entries can be inverted in *O*(*M*(*n*)) time.
  prefs: []
  type: TYPE_NORMAL
- en: The above proof assumed that *A* is an *n* × *n* matrix, where *n* is an exact
    power of 2\. If *n* is not an exact power of 2, then let *k* < *n* be such that
    *n* + *k* is an exact power of 2, and define the (*n* + *k*) × (*n* + *k*) matrix
    *A*′ as
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P939.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Then the inverse of *A*′ is
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P940.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Apply the method of the proof to *A*′ to compute the inverse of *A*′, and take
    the first *n* rows and *n* columns of the result as the desired answer *A*^(−1).
    The first regularity condition on *M*(*n*) ensures that enlarging the matrix in
    this way increases the running time by at most a constant factor.
  prefs: []
  type: TYPE_NORMAL
- en: ▪
  prefs: []
  type: TYPE_NORMAL
- en: The proof of Theorem 28.2 suggests how to solve the equation *Ax* = *b* by using
    LU decomposition without pivoting, so long as *A* is nonsingular. Let *y* = *A*^T*b*.
    Multiply both sides of the equation *Ax* = *b* by *A*^T, yielding (*A*^T*A*)*x*
    = *A*^T*b* = *y*. This transformation doesn’t affect the solution *x*, since *A*^T
    is invertible. Because *A*^T*A* is symmetric positive-definite, it can be factored
    by computing an LU decomposition. Then, use forward and back substitution to solve
    for *x* in the equation (*A*^T*A*)*x* = *y*. Although this method is theoretically
    correct, in practice the procedure LUP-DECOMPOSITION works much better. LUP decomposition
    requires fewer arithmetic operations by a constant factor, and it has somewhat
    better numerical properties.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercises**'
  prefs: []
  type: TYPE_NORMAL
- en: '***28.2-1***'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let *M*(*n*) be the time to multiply two *n* × *n* matrices, and let *S*(*n*)
    denote the time required to square an *n* × *n* matrix. Show that multiplying
    and squaring matrices have essentially the same difficulty: an *M*(*n*)-time matrix-multiplication
    algorithm implies an *O*(*M*(*n*))-time squaring algorithm, and an *S*(*n*)-time
    squaring algorithm implies an *O*(*S*(*n*))-time matrix-multiplication algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: '***28.2-2***'
  prefs: []
  type: TYPE_NORMAL
- en: Let *M*(*n*) be the time to multiply two *n* × *n* matrices. Show that an *M*(*n*)-time
    matrix-multiplication algorithm implies an *O*(*M*(*n*))-time LUP-decomposition
    algorithm. (The LUP decomposition your method produces need not be the same as
    the result produced by the LUP-DECOMPOSITION procedure.)
  prefs: []
  type: TYPE_NORMAL
- en: '***28.2-3***'
  prefs: []
  type: TYPE_NORMAL
- en: Let *M*(*n*) be the time to multiply two *n* × *n* boolean matrices, and let
    *T*(*n*) be the time to find the transitive closure of an *n* × *n* boolean matrix.
    (See [Section 23.2](chapter023.xhtml#Sec_23.2).) Show that an *M*(*n*)-time boolean
    matrix-multiplication algorithm implies an *O*(*M*(*n*) lg *n*)-time transitive-closure
    algorithm, and a *T*(*n*)-time transitive-closure algorithm implies an *O*(*T*
    (*n*))-time boolean matrix-multiplication algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '***28.2-4***'
  prefs: []
  type: TYPE_NORMAL
- en: Does the matrix-inversion algorithm based on Theorem 28.2 work when matrix elements
    are drawn from the field of integers modulo 2? Explain.
  prefs: []
  type: TYPE_NORMAL
- en: ★ ***28.2-5***
  prefs: []
  type: TYPE_NORMAL
- en: Generalize the matrix-inversion algorithm of Theorem 28.2 to handle matrices
    of complex numbers, and prove that your generalization works correctly. (*Hint:*
    Instead of the transpose of *A*, use the ***conjugate transpose*** *A**, which
    you obtain from the transpose of *A* by replacing every entry with its complex
    conjugate. Instead of symmetric matrices, consider ***Hermitian*** matrices, which
    are matrices *A* such that *A* = *A**.)
  prefs: []
  type: TYPE_NORMAL
- en: '[**28.3    Symmetric positive-definite matrices and least-squares approximation**](toc.xhtml#Rh1-164)'
  prefs: []
  type: TYPE_NORMAL
- en: Symmetric positive-definite matrices have many interesting and desirable properties.
    An *n* × *n* matrix *A* is ***symmetric positive-definite*** if *A* = *A*^T(*A*
    is symmetric) and *x*^T*Ax* > 0 for all *n*-vectors *x* ≠ 0 (*A* is positive-definite).
    Symmetric positive-definite matrices are nonsingular, and an LU decomposition
    on them will not divide by 0\. This section proves these and several other important
    properties of symmetric positive-definite matrices. We’ll also see an interesting
    application to curve fitting by a least-squares approximation.
  prefs: []
  type: TYPE_NORMAL
- en: The first property we prove is perhaps the most basic.
  prefs: []
  type: TYPE_NORMAL
- en: '***Lemma 28.3***'
  prefs: []
  type: TYPE_NORMAL
- en: Any positive-definite matrix is nonsingular.
  prefs: []
  type: TYPE_NORMAL
- en: '***Proof***   Suppose that a matrix *A* is singular. Then by Corollary D.3
    on page 1221, there exists a nonzero vector *x* such that *Ax* = 0\. Hence, *x*^T*Ax*
    = 0, and *A* cannot be positive-definite.'
  prefs: []
  type: TYPE_NORMAL
- en: ▪
  prefs: []
  type: TYPE_NORMAL
- en: The proof that an LU decomposition on a symmetric positive-definite matrix *A*
    won’t divide by 0 is more involved. We begin by proving properties about certain
    submatrices of *A*. Define the *k*th ***leading submatrix*** of *A* to be the
    matrix *A[k]* consisting of the intersection of the first *k* rows and first *k*
    columns of *A*.
  prefs: []
  type: TYPE_NORMAL
- en: '***Lemma 28.4***'
  prefs: []
  type: TYPE_NORMAL
- en: If *A* is a symmetric positive-definite matrix, then every leading submatrix
    of *A* is symmetric and positive-definite.
  prefs: []
  type: TYPE_NORMAL
- en: '***Proof***   Since *A* is symmetric, each leading submatrix *A[k]* is also
    symmetric. We’ll prove that *A[k]* is positive-definite by contradiction. If *A[k]*
    is not positive-definite, then there exists a *k*-vector *x[k]* ≠ 0 such that
    ![art](images/Art_P941.jpg). Let *A* be *n* × *n*, and'
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P942.jpg)'
  prefs: []
  type: TYPE_IMG
- en: for submatrices *B* (which is (*n*−*k*)×*k*) and *C* (which is (*n*−*k*)×(*n*−*k*)).
    Define the *n*-vector ![art](images/Art_P943.jpg), where *n* − *k* 0s follow *x[k]*.
    Then we have
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P944.jpg)'
  prefs: []
  type: TYPE_IMG
- en: which contradicts *A* being positive-definite.
  prefs: []
  type: TYPE_NORMAL
- en: ▪
  prefs: []
  type: TYPE_NORMAL
- en: We now turn to some essential properties of the Schur complement. Let *A* be
    a symmetric positive-definite matrix, and let *A[k]* be a leading *k* × *k* submatrix
    of *A*. Partition *A* once again according to equation (28.16). Equation (28.10)
    generalizes to define the ***Schur complement*** *S* of *A* with respect to *A[k]*
    as
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P945.jpg)'
  prefs: []
  type: TYPE_IMG
- en: (By Lemma 28.4, *A[k]* is symmetric and positive-definite, and therefore, ![art](images/Art_P946.jpg)
    exists by Lemma 28.3, and *S* is well defined.) The earlier definition (28.10)
    of the Schur complement is consistent with equation (28.17) by letting *k* = 1.
  prefs: []
  type: TYPE_NORMAL
- en: The next lemma shows that the Schur-complement matrices of symmetric positive-definite
    matrices are themselves symmetric and positive-definite. We used this result in
    Theorem 28.2, and its corollary will help prove that LU decomposition works for
    symmetric positive-definite matrices.
  prefs: []
  type: TYPE_NORMAL
- en: '***Lemma 28.5 (Schur complement lemma)***'
  prefs: []
  type: TYPE_NORMAL
- en: If *A* is a symmetric positive-definite matrix and *A[k]* is a leading *k* ×
    *k* submatrix of *A*, then the Schur complement *S* of *A* with respect to *A[k]*
    is symmetric and positive-definite.
  prefs: []
  type: TYPE_NORMAL
- en: '***Proof***   Because *A* is symmetric, so is the submatrix *C*. By Exercise
    D.2-6 on page 1223, the product ![art](images/BA.jpg) is symmetric. Since *C*
    and ![art](images/BA.jpg) are symmetric, then by Exercise D.1-1 on page 1219,
    so is *S*.'
  prefs: []
  type: TYPE_NORMAL
- en: It remains to show that *S* is positive-definite. Consider the partition of
    *A* given in equation (28.16). For any nonzero vector *x*, we have *x*^T*Ax* >
    0 by the assumption that *A* is positive-definite. Let the subvectors *y* and
    *z* consist of the first *k* and last *n* − *k* elements in *x*, respectively,
    and thus they are compatible with *A[k]* and *C*, respectively. Because ![art](images/Art_P947.jpg)
    exists, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P948.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This last equation, which you can verify by multiplying through, amounts to
    “completing the square” of the quadratic form. (See Exercise 28.3-2.)
  prefs: []
  type: TYPE_NORMAL
- en: Since *x*^T*Ax* > 0 holds for any nonzero *x*, pick any nonzero *z* and then
    choose ![art](images/Art_P949.jpg), which causes the first term in equation (28.18)
    to vanish, leaving
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P950.jpg)'
  prefs: []
  type: TYPE_IMG
- en: as the value of the expression. For any *z* ≠ 0, we therefore have *z*^T*Sz*
    = *x*^T*Ax* > 0, and thus *S* is positive-definite.
  prefs: []
  type: TYPE_NORMAL
- en: ▪
  prefs: []
  type: TYPE_NORMAL
- en: '***Corollary 28.6***'
  prefs: []
  type: TYPE_NORMAL
- en: LU decomposition of a symmetric positive-definite matrix never causes a division
    by 0.
  prefs: []
  type: TYPE_NORMAL
- en: '***Proof***   Let *A* be an *n* × *n* symmetric positive-definite matrix. In
    fact, we’ll prove a stronger result than the statement of the corollary: every
    pivot is strictly positive. The first pivot is *a*[11]. Let *e*[1] be the length-*n*
    unit vector ( 1 0 0 ⋯ 0 )^T, so that ![art](images/Art_P951.jpg), which is positive
    because *e*[1] is nonzero and *A* is positive definite. Since the first step of
    LU decomposition produces the Schur complement of *A* with respect to *A*[1] =
    (*a*[11]), Lemma 28.5 implies by induction that all pivots are positive.'
  prefs: []
  type: TYPE_NORMAL
- en: ▪
  prefs: []
  type: TYPE_NORMAL
- en: '**Least-squares approximation**'
  prefs: []
  type: TYPE_NORMAL
- en: One important application of symmetric positive-definite matrices arises in
    fitting curves to given sets of data points. You are given a set of *m* data points
  prefs: []
  type: TYPE_NORMAL
- en: (*x*[1], *y*[1]), (*x*[2], *y*[2]), … , (*x[m]*, *y[m]*),
  prefs: []
  type: TYPE_NORMAL
- en: where you know that the *y[i]* are subject to measurement errors. You wish to
    determine a function *F*(*x*) such that the approximation errors
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P952.jpg)'
  prefs: []
  type: TYPE_IMG
- en: are small for *i* = 1, 2, … , *m*. The form of the function *F* depends on the
    problem at hand. Let’s assume that it has the form of a linearly weighted sum
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P953.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where the number *n* of summands and the specific ***basis functions*** *f[j]*
    are chosen based on knowledge of the problem at hand. A common choice is *f[j]*(*x*)
    = *x*^(*j*−1), which means that
  prefs: []
  type: TYPE_NORMAL
- en: '*F*(*x*) = *c*[1] + *c*[2]*x* + *c*[3]*x*² + ⋯ + *c*[*n*]*x*^(*n*−1)'
  prefs: []
  type: TYPE_NORMAL
- en: is a polynomial of degree *n* − 1 in *x*. Thus, if you are given *m* data points
    (*x*[1], *y*[1]), (*x*[2], *y*[2]), … , (*x[m]*, *y[m]*), you need to calculate
    *n* coefficients *c*[1], *c*[2], … , *c[n]* that minimize the approximation errors
    *η*[1], *η*[2], … , *η[m]*.
  prefs: []
  type: TYPE_NORMAL
- en: By choosing *n* = *m*, you can calculate each *y[i]* *exactly* in equation (28.19).
    Such a high-degree polynomial *F* “fits the noise” as well as the data, however,
    and generally gives poor results when used to predict *y* for previously unseen
    values of *x*. It is usually better to choose *n* significantly smaller than *m*
    and hope that by choosing the coefficients *c[j]* well, you can obtain a function
    *F* that finds the significant patterns in the data points without paying undue
    attention to the noise. Some theoretical principles exist for choosing *n*, but
    they are beyond the scope of this text. In any case, once you choose a value of
    *n* that is less than *m*, you end up with an overdetermined set of equations
    whose solution you wish to approximate. Let’s see how to do so.
  prefs: []
  type: TYPE_NORMAL
- en: Let
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P954.jpg)'
  prefs: []
  type: TYPE_IMG
- en: denote the matrix of values of the basis functions at the given points, that
    is, *a[ij]* = *f[j]*(*x[i]*). Let *c* = (*c[k]*) denote the desired *n*-vector
    of coefficients. Then,
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P955.jpg)'
  prefs: []
  type: TYPE_IMG
- en: is the *m*-vector of “predicted values” for *y*. Thus,
  prefs: []
  type: TYPE_NORMAL
- en: '*η* = *Ac* − *y*'
  prefs: []
  type: TYPE_NORMAL
- en: is the *m*-vector of ***approximation errors***.
  prefs: []
  type: TYPE_NORMAL
- en: To minimize approximation errors, let’s minimize the norm of the error vector
    *η*, which gives a ***least-squares solution***, since
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P956.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Because
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P957.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'to minimize ∥*η*∥, differentiate ∥*η*∥² with respect to each *c[k]* and then
    set the result to 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P958.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The *n* equations (28.20) for *k* = 1, 2, … , *n* are equivalent to the single
    matrix equation
  prefs: []
  type: TYPE_NORMAL
- en: (*Ac* − *y*)^T *A* = 0
  prefs: []
  type: TYPE_NORMAL
- en: or, equivalently (using Exercise D.1-2 on page 1219), to
  prefs: []
  type: TYPE_NORMAL
- en: '*A*^T(*Ac* − *y*) = 0,'
  prefs: []
  type: TYPE_NORMAL
- en: which implies
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P959.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In statistics, equation (28.21) is called the ***normal equation***. The matrix
    *A*^T*A* is symmetric by Exercise D.1-2, and if *A* has full column rank, then
    by Theorem D.6 on page 1222, *A*^T*A* is positive-definite as well. Hence, (*A*^T*A*)^(−1)
    exists, and the solution to equation (28.21) is
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P960.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where the matrix *A*^+ = ((*A*^T*A*)^(−1)*A*^T) is the ***pseudoinverse*** of
    the matrix *A*. The pseudoinverse naturally generalizes the notion of a matrix
    inverse to the case in which *A* is not square. (Compare equation (28.22) as the
    approximate solution to *Ac* = *y* with the solution *A*^(−1)*b* as the exact
    solution to *Ax* = *b*.)
  prefs: []
  type: TYPE_NORMAL
- en: As an example of producing a least-squares fit, suppose that you have five data
    points
  prefs: []
  type: TYPE_NORMAL
- en: '| (*x*[1], *y*[1]) | = | (−1, 2), |'
  prefs: []
  type: TYPE_TB
- en: '| (*x*[2], *y*[2]) | = | (1, 1), |'
  prefs: []
  type: TYPE_TB
- en: '| (*x*[3], *y*[3]) | = | (2, 1), |'
  prefs: []
  type: TYPE_TB
- en: '| (*x*[4], *y*[4]) | = | (3, 0), |'
  prefs: []
  type: TYPE_TB
- en: '| (*x*[5], *y*[5]) | = | (5, 3), |'
  prefs: []
  type: TYPE_TB
- en: shown as orange dots in [Figure 28.3](chapter028.xhtml#Fig_28-3), and you want
    to fit these points with a quadratic polynomial
  prefs: []
  type: TYPE_NORMAL
- en: '*F*(*x*) = *c*[1] + *c*[2]*x* + *c*[3]*x*².'
  prefs: []
  type: TYPE_NORMAL
- en: Start with the matrix of basis-function values
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P961.jpg)'
  prefs: []
  type: TYPE_IMG
- en: whose pseudoinverse is
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P962.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 28.3** The least-squares fit of a quadratic polynomial to the set
    of five data points {(−1, 2), (1, 1), (2, 1), (3, 0), (5, 3)}. The orange dots
    are the data points, and the blue dots are their estimated values predicted by
    the polynomial *F*(*x*) = 1.2 − 0.757*x* + 0.214*x*², the quadratic polynomial
    that minimizes the sum of the squared errors, plotted in blue. Each orange line
    shows the error for one data point.'
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P963.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Multiplying *y* by *A*^+ gives the coefficient vector
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P964.jpg)'
  prefs: []
  type: TYPE_IMG
- en: which corresponds to the quadratic polynomial
  prefs: []
  type: TYPE_NORMAL
- en: '*F*(*x*) = 1.200 − 0.757*x* + 0.214*x*²'
  prefs: []
  type: TYPE_NORMAL
- en: as the closest-fitting quadratic to the given data, in a least-squares sense.
  prefs: []
  type: TYPE_NORMAL
- en: As a practical matter, you would typically solve the normal equation (28.21)
    by multiplying *y* by *A*^T and then finding an LU decomposition of *A*^T*A*.
    If *A* has full rank, the matrix *A*^T*A* is guaranteed to be nonsingular, because
    it is symmetric and positive-definite. (See Exercise D.1-2 and Theorem D.6.)
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P965.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 28.4** A least-squares fit of a curve of the form'
  prefs: []
  type: TYPE_NORMAL
- en: '*c*[1] + *c*[2]*x* + *c*[3]*x*² + *c*[4] sin(2*πx*) + *c*[5] cos(2*πx*)'
  prefs: []
  type: TYPE_NORMAL
- en: for the carbon-dioxide concentrations measured in Mauna Loa, Hawaii from 1990^([1](#footnote_1))
    to 2019, where *x* is the number of years elapsed since 1990\. This curve is the
    famous “Keeling curve,” illustrating curve-fitting to nonpolynomial formulas.
    The sine and cosine terms allow modeling of seasonal variations in CO[2] concentrations.
    The red curve shows the measured CO[2] concentrations. The best fit, shown in
    black, has the form
  prefs: []
  type: TYPE_NORMAL
- en: 352.83 + 1.39*x* + 0.02*x*² + 2.83 sin(2*πx*) − 0.94 cos(2*πx*).
  prefs: []
  type: TYPE_NORMAL
- en: 'We close this section with an example in [Figure 28.4](chapter028.xhtml#Fig_28-4),
    illustrating that a curve can also fit a nonpolynomial function. The curve confirms
    one aspect of climate change: that carbon dioxide (CO[2]) concentrations have
    steadily increased over a period of 29 years. Linear and quadratic terms model
    the annual increase, and sine and cosine terms model seasonal variations.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercises**'
  prefs: []
  type: TYPE_NORMAL
- en: '***28.3-1***'
  prefs: []
  type: TYPE_NORMAL
- en: Prove that every diagonal element of a symmetric positive-definite matrix is
    positive.
  prefs: []
  type: TYPE_NORMAL
- en: '***28.3-2***'
  prefs: []
  type: TYPE_NORMAL
- en: Let ![art](images/Art_P966.jpg) be a 2 × 2 symmetric positive-definite matrix.
    Prove that its determinant *ac* − *b*² is positive by “completing the square”
    in a manner similar to that used in the proof of Lemma 28.5.
  prefs: []
  type: TYPE_NORMAL
- en: '***28.3-3***'
  prefs: []
  type: TYPE_NORMAL
- en: Prove that the maximum element in a symmetric positive-definite matrix lies
    on the diagonal.
  prefs: []
  type: TYPE_NORMAL
- en: '***28.3-4***'
  prefs: []
  type: TYPE_NORMAL
- en: Prove that the determinant of each leading submatrix of a symmetric positive-definite
    matrix is positive.
  prefs: []
  type: TYPE_NORMAL
- en: '***28.3-5***'
  prefs: []
  type: TYPE_NORMAL
- en: Let *A[k]* denote the *k*th leading submatrix of a symmetric positive-definite
    matrix *A*. Prove that det(*A[k]*)/det(*A*[*k*−1]) is the *k*th pivot during LU
    decomposition, where, by convention, det(*A*[0]) = 1.
  prefs: []
  type: TYPE_NORMAL
- en: '***28.3-6***'
  prefs: []
  type: TYPE_NORMAL
- en: Find the function of the form
  prefs: []
  type: TYPE_NORMAL
- en: '*F*(*x*) = *c*[1] + *c*[2]*x* lg *x* + *c*[3]*e^x*'
  prefs: []
  type: TYPE_NORMAL
- en: that is the best least-squares fit to the data points
  prefs: []
  type: TYPE_NORMAL
- en: (1, 1), (2, 1), (3, 3), (4, 8).
  prefs: []
  type: TYPE_NORMAL
- en: '***28.3-7***'
  prefs: []
  type: TYPE_NORMAL
- en: 'Show that the pseudoinverse *A*^+ satisfies the following four equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '| *AA*^+*A* | = | *A*, |'
  prefs: []
  type: TYPE_TB
- en: '| *A*^+*AA*^+ | = | *A*^+, |'
  prefs: []
  type: TYPE_TB
- en: '| (*AA*^+)^T | = | *AA*^+, |'
  prefs: []
  type: TYPE_TB
- en: '| (*A*^+*A*)^T | = | *A*^+*A*. |'
  prefs: []
  type: TYPE_TB
- en: '**Problems**'
  prefs: []
  type: TYPE_NORMAL
- en: '***28-1     Tridiagonal systems of linear equations***'
  prefs: []
  type: TYPE_NORMAL
- en: Consider the tridiagonal matrix
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P967.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '***a.*** Find an LU decomposition of *A*.'
  prefs: []
  type: TYPE_NORMAL
- en: '***b.*** Solve the equation *Ax* = ( 1 1 1 1 1 )^T by using forward and back
    substitution.'
  prefs: []
  type: TYPE_NORMAL
- en: '***c.*** Find the inverse of *A*.'
  prefs: []
  type: TYPE_NORMAL
- en: '***d.*** Show how to solve the equation *Ax* = *b* for any *n* × *n* symmetric
    positive-definite, tridiagonal matrix *A* and any *n*-vector *b* in *O*(*n*) time
    by performing an LU decomposition. Argue that any method based on forming *A*^(−1)
    is asymptotically more expensive in the worst case.'
  prefs: []
  type: TYPE_NORMAL
- en: '***e.*** Show how to solve the equation *Ax* = *b* for any *n* × *n* nonsingular,
    tridiagonal matrix *A* and any *n*-vector *b* in *O*(*n*) time by performing an
    LUP decomposition.'
  prefs: []
  type: TYPE_NORMAL
- en: '***28-2     Splines***'
  prefs: []
  type: TYPE_NORMAL
- en: 'A practical method for interpolating a set of points with a curve is to use
    ***cubic splines***. You are given a set {(*x[i]*, *y[i]*) : *i* = 0, 1, … , *n*}
    of *n* + 1 point-value pairs, where *x*[0] < *x*[1] < ⋯ < *x[n]*. Your goal is
    to fit a piecewise-cubic curve (spline) *f*(*x*) to the points. That is, the curve
    *f*(*x*) is made up of *n* cubic polynomials *f[i]*(*x*) = *a[i]* + *b[i]x* +
    *c*[*i*]*x*² + *d*[*i*]*x*³ for *i* = 0, 1, … , *n* − 1, where if *x* falls in
    the range *x[i]* ≤ *x* ≤ *x*[*i*+1], then the value of the curve is given by *f*(*x*)
    = *f[i]*(*x* − *x[i]*). The points *x[i]* at which the cubic polynomials are “pasted”
    together are called ***knots***. For simplicity, assume that *x[i]* = *i* for
    *i* = 0, 1, … , *n*.'
  prefs: []
  type: TYPE_NORMAL
- en: To ensure continuity of *f*(*x*), require that
  prefs: []
  type: TYPE_NORMAL
- en: '| *f*(*x[i]*) | = | *f[i]*(0) | = | *y[i]*, |'
  prefs: []
  type: TYPE_TB
- en: '| *f*(*x*[*i*+1]) | = | *f[i]*(1) | = | *y*[*i*+1] |'
  prefs: []
  type: TYPE_TB
- en: 'for *i* = 0, 1, … , *n* − 1\. To ensure that *f*(*x*) is sufficiently smooth,
    also require the first derivative to be continuous at each knot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P968.jpg)'
  prefs: []
  type: TYPE_IMG
- en: for *i* = 0, 1, … , *n* − 2.
  prefs: []
  type: TYPE_NORMAL
- en: '***a.*** Suppose that for *i* = 0, 1, … , *n*, in addition to the point-value
    pairs {(*x[i]*, *y[i]*)}, you are also given the first derivative *D[i]* = *f*′(*x[i]*)
    at each knot. Express each coefficient *a[i]*, *b[i]*, *c[i]*, and *d[i]* in terms
    of the values *y[i]*, *y*[*i*+1], *D[i]*, and *D*[*i*+1]. (Remember that *x[i]*
    = *i*.) How quickly can you compute the 4*n* coefficients from the point-value
    pairs and first derivatives?'
  prefs: []
  type: TYPE_NORMAL
- en: 'The question remains of how to choose the first derivatives of *f*(*x*) at
    the knots. One method is to require the second derivatives to be continuous at
    the knots:'
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P969.jpg)'
  prefs: []
  type: TYPE_IMG
- en: for *i* = 0, 1, … , *n*−2\. At the first and last knots, assume that ![art](images/Art_P970.jpg)
    and ![art](images/Art_P971.jpg). These assumptions make *f*(*x*) a ***natural***
    cubic spline.
  prefs: []
  type: TYPE_NORMAL
- en: '***b.*** Use the continuity constraints on the second derivative to show that
    for *i* = 1, 2, … , *n* − 1,'
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P972.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '***c.*** Show that'
  prefs: []
  type: TYPE_NORMAL
- en: '![art](images/Art_P973.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '***d.*** Rewrite equations (28.23)–(28.25) as a matrix equation involving the
    vector *D* = (*D*[0] *D*[1] *D*[2] ⋯ *D[n]*)^T of unknowns. What attributes does
    the matrix in your equation have?'
  prefs: []
  type: TYPE_NORMAL
- en: '***e.*** Argue that a natural cubic spline can interpolate a set of *n* + 1
    point-value pairs in *O*(*n*) time (see Problem 28-1).'
  prefs: []
  type: TYPE_NORMAL
- en: '***f.*** Show how to determine a natural cubic spline that interpolates a set
    of *n* + 1 points (*x[i]*, *y[i]*) satisfying *x*[0] < *x*[1] < ⋯ < *x[n]*, even
    when *x[i]* is not necessarily equal to *i*. What matrix equation must your method
    solve, and how quickly does your algorithm run?'
  prefs: []
  type: TYPE_NORMAL
- en: '**Chapter notes**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Many excellent texts describe numerical and scientific computation in much
    greater detail than we have room for here. The following are especially readable:
    George and Liu [[180](bibliography001.xhtml#endnote_180)], Golub and Van Loan
    [[192](bibliography001.xhtml#endnote_192)], Press, Teukolsky, Vetterling, and
    Flannery [[365](bibliography001.xhtml#endnote_365), [366](bibliography001.xhtml#endnote_366)],
    and Strang [[422](bibliography001.xhtml#endnote_422), [423](bibliography001.xhtml#endnote_423)].'
  prefs: []
  type: TYPE_NORMAL
- en: Golub and Van Loan [[192](bibliography001.xhtml#endnote_192)] discuss numerical
    stability. They show why det(*A*) is not necessarily a good indicator of the stability
    of a matrix *A*, proposing instead to use ∥*A*∥[∞] ∥*A*^(−1)∥[∞], where ![art](images/Art_P974.jpg).
    They also address the question of how to compute this value without actually computing
    *A*^(−1).
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian elimination, upon which the LU and LUP decompositions are based, was
    the first systematic method for solving linear systems of equations. It was also
    one of the earliest numerical algorithms. Although it was known earlier, its discovery
    is commonly attributed to C. F. Gauss (1777–1855). In his famous paper [[424](bibliography001.xhtml#endnote_424)],
    Strassen showed that an *n*×*n* matrix can be inverted in *O*(*n*^(lg 7)) time.
    Winograd [[460](bibliography001.xhtml#endnote_460)] originally proved that matrix
    multiplication is no harder than matrix inversion, and the converse is due to
    Aho, Hopcroft, and Ullman [[5](bibliography001.xhtml#endnote_5)].
  prefs: []
  type: TYPE_NORMAL
- en: Another important matrix decomposition is the ***singular value decomposition***,
    or ***SVD***. The SVD factors an *m* × *n* matrix *A* into ![art](images/Art_P975.jpg),
    where Σ is an *m*×*n* matrix with nonzero values only on the diagonal, *Q*[1]
    is *m*×*m* with mutually orthonormal columns, and *Q*[2] is *n* × *n*, also with
    mutually orthonormal columns. Two vectors are ***orthonormal*** if their inner
    product is 0 and each vector has a norm of 1\. The books by Strang [[422](bibliography001.xhtml#endnote_422),
    [423](bibliography001.xhtml#endnote_423)] and Golub and Van Loan [[192](bibliography001.xhtml#endnote_192)]
    contain good treatments of the SVD.
  prefs: []
  type: TYPE_NORMAL
- en: Strang [[423](bibliography001.xhtml#endnote_423)] has an excellent presentation
    of symmetric positive-definite matrices and of linear algebra in general.
  prefs: []
  type: TYPE_NORMAL
- en: '[¹](#footnote_ref_1) The year in which *Introduction to Algorithms* was first
    published.'
  prefs: []
  type: TYPE_NORMAL
