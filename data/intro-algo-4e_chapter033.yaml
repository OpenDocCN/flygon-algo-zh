- en: '[**33        Machine-Learning Algorithms**](toc.xhtml#chap-33)'
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: '[**33        机器学习算法**](toc.xhtml#chap-33)'
- en: Machine learning may be viewed as a subfield of artificial intelligence. Broadly
    speaking, artificial intelligence aims to enable computers to carry out complex
    perception and information-processing tasks with human-like performance. The field
    of AI is vast and uses many different algorithmic methods.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习可以被视为人工智能的一个子领域。广义上讲，人工智能旨在使计算机能够以类似人类的性能执行复杂的感知和信息处理任务。人工智能领域广泛而使用许多不同的算法方法。
- en: Machine learning is rich and fascinating, with strong ties to statistics and
    optimization. Technology today produces enormous amounts of data, providing myriad
    opportunities for machine-learning algorithms to formulate and test hypotheses
    about patterns within the data. These hypotheses can then be used to make predictions
    about the characteristics or classifications in new data. Because machine learning
    is particularly good with challenging tasks involving uncertainty, where observed
    data follows unknown rules, it has markedly transformed fields such as medicine,
    advertising, and speech recognition.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是丰富而迷人的，与统计学和优化学有着密切的联系。当今技术产生了大量的数据，为机器学习算法提供了无数机会来制定和测试关于数据内部模式的假设。这些假设可以用来对新数据的特征或分类进行预测。由于机器学习在涉及不确定性的挑战性任务中表现特别出色，观察到的数据遵循未知规则，它已经显著改变了医学、广告和语音识别等领域。
- en: 'This chapter presents three important machine-learning algorithms: *k*-means
    clustering, multiplicative weights, and gradient descent. You can view each of
    these tasks as a learning problem, whereby an algorithm uses the data collected
    so far to produce a hypothesis that describes the regularities learned and/or
    makes predictions about new data. The boundaries of machine learning are imprecise
    and evolving—some might say that the *k*-means clustering algorithm should be
    called “data science” and not “machine learning,” and gradient descent, though
    an immensely important algorithm for machine learning, also has a multitude of
    applications outside of machine learning (most notably for optimization problems).'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了三种重要的机器学习算法：*k*-means聚类、乘法权重和梯度下降。您可以将这些任务中的每一个视为一个学习问题，其中算法��用迄今收集的数据来产生描述所学规律和/或对新数据进行预测的假设。机器学习的边界不明确且不断发展——有些人可能会说*k*-means聚类算法应该被称为“数据科学”，而不是“机器学习”，梯度下降虽然是机器学习中极其重要的算法，但在机器学习之外也有许多应用（尤其是用于优化问题）。
- en: Machine learning typically starts with a ***training phase*** followed by a
    ***prediction phase*** in which predictions are made about new data. For ***online
    learning***, the training and prediction phases are intermingled. The training
    phase takes as input ***training data***, where each input data point has an associated
    output or ***label***; the label might be a category name or some real-valued
    attribute. It then produces as an output one or more ***hypotheses*** about how
    the labels depend on the attributes of the input data points. Hypotheses can take
    many forms, typically some type of formula or algorithm. The learning algorithm
    used is often a form of gradient descent. The prediction phase then uses the hypothesis
    on new data in order to make ***predictions*** regarding the labels of new data
    points.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习通常从***训练阶段***开始，然后是***预测阶段***，在预测阶段对新数据进行预测。对于***在线学习***，训练和预测阶段是交织在一起的。训练阶段以***训练数据***为输入，其中每个输入数据点都有一个相关的输出或***标签***；标签可能是一个类别名称或一些实值属性。然后，它产生一个或多个关于标签如何依赖于输入数据点属性的***假设***。假设可以采用许多形式，通常是某种类型的公式或算法。通常使用的学习算法是梯度下降的一种形式。然后，在预测阶段，使用假设对新数据进行预测，以便对新数据点的标签进行***预测***。
- en: The type of learning just described is known as ***supervised learning***, since
    it starts with a set of inputs that are each labeled. As an example, consider
    a machine-learning algorithm to recognize spam emails. The training data comprises
    a collection of emails, each of which is labeled either “spam” or “not spam.”
    The machine-learning algorithm frames a hypothesis, possibly a rule of the form
    “if an email has one of a set of words, then it is likely to be spam.” Or it might
    learn rules that assign a spam score to each word and then evaluates a document
    by the sum of the spam scores of its constituent words, so that a document with
    a total score above a certain threshold value is classified as spam. The machine-learning
    algorithm can then predict whether a new email is spam or not.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 刚刚描述的学习类型被称为***监督学习***，因为它从一组已标记的输入开始。举个例子，考虑一个用于识别垃圾邮件的机器学习算法。训练数据包括一组电子邮件，每封邮件都被标记为“垃圾邮件”或“非垃圾邮件”。机器学习算法构建一个假设，可能是一个规则，比如“如果一封邮件包含一组词中的一个，那么它很可能是垃圾邮件”。或者它可能学习将每个词分配一个垃圾邮件分数的规则，然后通过其组成词的垃圾邮件分数之和来评估文档，以便将总分超过某个阈值的文档分类为垃圾邮件。然后，机器学习算法可以预测新邮件是否是垃圾邮件。
- en: A second form of machine learning is ***unsupervised learning***, where the
    training data is unlabeled, as in the clustering problem of [Section 33.1](chapter033.xhtml#Sec_33.1).
    Here the machine-learning algorithm produces hypotheses regarding the centers
    of groups of input data points.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的第二种形式是***无监督学习***，其中训练数据没有标记，就像[第33.1节](chapter033.xhtml#Sec_33.1)中的聚类问题一样。在这里，机器学习算法产生关于输入数据点组的中心的假设。
- en: A third form of machine learning (not covered further here) is ***reinforcement
    learning***, where the machine-learning algorithm takes actions in an environment,
    receives feedback for those actions from the environment, and then updates its
    model of the environment based on the feedback. The learner is in an environment
    that has some state, and the actions of the learner have an effect on that state.
    Reinforcement learning is a natural choice for situations such as game playing
    or operating a self-driving car.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的第三种形式（此处不再详细介绍）是***强化学习***，其中机器学习算法在环境中采取行动，从环境中获得这些行动的反馈，然后根据反馈更新其对环境的模型。学习者处于具有某种状态的环境中，学习者的行动会影响该状态。强化学习在游戏玩法或操作自动驾驶汽车等情况下是一个自然的选择。
- en: 'Sometimes the goal in a supervised machine-learning application is not making
    accurate predictions of labels for new examples, but rather performing causal
    ***inference***: finding an explanatory model that describes how the various features
    of an input data point affect its associated label. Finding a model that fits
    a given set of training data well can be tricky. It may involve sophisticated
    optimization methods that need to balance between producing a hypothesis that
    fits the data well and producing a hypothesis that is simple.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，在监督式机器学习应用中，目标不是为新示例进行准确的标签预测，而是进行因果***推断***：找到一个解释性模型，描述输入数据点的各个特征如何影响其相关标签。找到一个适合给定训练数据集的模型可能会很棘手。这可能涉及需要在产生适合数据的假设和简单假设之间取得平衡的复杂优化方法。
- en: 'This chapter focuses on three problem domains: finding hypotheses that group
    the input data points well (using a clustering algorithm), learning which predictors
    (experts) to rely upon for making predictions in an online learning problem (using
    the multiplicative-weights algorithm), and fitting a model to data (using gradient
    descent).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点关注三个问题领域：找到很好地将输入数据点分组的假设（使用聚类算法）、学习依赖于在线学习问题中的哪些预测器（专家）进行预测（使用乘法权重算法）以及将模型拟合到数据中（使用梯度下降）。
- en: '[Section 33.1](chapter033.xhtml#Sec_33.1) considers the clustering problem:
    how to divide a given set of *n* training data points into a given number *k*
    of groups, or “clusters,” based on a measure of how similar (or more accurately,
    how dissimilar) points are to each other. The approach is iterative, beginning
    with an arbitrary initial clustering and incorporating successive improvements
    until no further improvements occur. Clustering is often used as an initial step
    when working on a machine-learning problem to discover what structure exists in
    the data.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[第33.1节](chapter033.xhtml#Sec_33.1)考虑了聚类问题：如何根据点之间的相似性（或更准确地说，不相似性）的度量，将给定的*n*个训练数据点分成给定数量*k*的组或“簇”。该方法是迭代的，从任意初始聚类开始，并逐步改进，直到不再发生改进。在处理机器��习问题时，聚类通常被用作一个初始步骤，以发现数据中存在的结构。'
- en: '[Section 33.2](chapter033.xhtml#Sec_33.2) shows how to make online predictions
    quite accurately when you have a set of predictors, often called “experts,” to
    rely on, many of which might be poor predictors, but some of which are good predictors.
    At first, you do not know which predictors are poor and which are good. The goal
    is to make predictions on new examples that are nearly as good as the predictions
    made by the best predictor. We study an effective multiplicative-weights prediction
    method that associates a positive real weight with each predictor and multiplicatively
    decreases the weights associated with predictors when they make poor predictions.
    The model in this section is online (see [Chapter 27](chapter027.xhtml)): at each
    step, we do not know anything about the future examples. In addition, we are able
    to make predictions even in the presence of adversarial experts, who are collaborating
    against us, a situation that actually happens in game-playing settings.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[第33.2节](chapter033.xhtml#Sec_33.2)展示了当你有一组预测器（通常称为“专家”）可依赖时，如何进行在线预测。其中许多预测器可能是糟糕的预测器，但有些是好的预测器。起初，你不知道哪些预测器是糟糕的，哪些是好的。目标是在新示例上进行预测，几乎与最佳预测器的预测一样好。我们研究了一种有效的乘法权重预测方法，该方法将每个预测器关联到一个正实数权重，并在预测不佳时乘法减少与预测器关联的权重。本节中的模型是在线的（参见[第27章](chapter027.xhtml)）：在每一步中，我们对未来示例一无所知。此外，即使在存在针对我们合作的对手专家的情况下，我们也能够进行预测，这种情况实际上在游戏设置中发生。'
- en: Finally, [Section 33.3](chapter033.xhtml#Sec_33.3) introduces gradient descent,
    a powerful optimization technique used to find parameter settings in machine-learning
    models. Gradient descent also has many applications outside of machine learning.
    Intuitively, gradient descent finds the value that produces a local minimum for
    a function by “walking downhill.” In a learning application, a “downhill step”
    is a step that adjusts hypothesis parameters so that the hypothesis does better
    on the given set of labeled examples.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，[第33.3节](chapter033.xhtml#Sec_33.3)介绍了梯度下降，这是一种强大的优化技术，用于在机器学习模型中找到参数设置。梯度下降在机器学习之外也有许多应用。直观地说，梯度下降通过“下山”来找到产生函数局部最小值的值。在学习应用中，“下山步骤”是调整假设参数的步骤，使得假设在给定的标记示例集上表现更好。
- en: This chapter makes extensive use of vectors. In contrast to the rest of the
    book, vector names in this chapter appear in boldface, such as **x**, to more
    clearly delineate which quantities are vectors. Components of vectors do not appear
    in boldface, so if vector **x** has *d* dimensions, we might write **x** = (*x*[1],
    *x*[2], …, *x*[*d*]).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章广泛使用向量。与本书的其余部分相反，在本章中，向量名称以粗体显示，例如**x**，以更清晰地区分哪些量是向量。向量的分量不以粗体显示，因此如果向量**x**有*d*个维度，我们可能会写成**x**=(*x*[1],
    *x*[2], …, *x*[*d*])。
- en: '[**33.1    Clustering**](toc.xhtml#Rh1-194)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[**33.1    聚类**](toc.xhtml#Rh1-194)'
- en: Suppose that you have a large number of data points (examples), and you wish
    to group them into classes based on how similar they are to each other. For example,
    each data point might represent a celestial star, giving its temperature, size,
    and spectral characteristics. Or, each data point might represent a fragment of
    recorded speech. Grouping these speech fragments appropriately might reveal the
    set of accents of the fragments. Once a grouping of the training data points is
    found, new data can be placed into an appropriate group, facilitating star-type
    recognition or speech recognition.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有大量的数据点（示例），并希望根据它们彼此的相似性将它们分组到类别中。例如，每个数据点可能代表一个天体星星，给出其温度、大小和光谱特征。或者，每个数据点可能代表一段录音的片段。适当地对这些语音片段进行分组可能会揭示这些片段的口音集合。一旦找到训练数据点的分组，新数据就可以放入适当的组中，促进星型识别或语音识别。
- en: These situations, along with many others, fall under the umbrella of clustering.
    The input to a ***clustering*** problem is a set of *n* examples (objects) and
    an integer *k*, with the goal of dividing the examples into at most *k* disjoint
    clusters such that the examples in each cluster are similar to each other. The
    clustering problem has several variations. For example, the integer *k* might
    not be given, but instead arises out of the clustering procedure. In this section
    we presume that *k* is given.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这些情况，以及许多其他情况，都属于聚类的范畴。聚类问题的输入是一组*n*个示例（对象）和一个整数*k*，目标是将示例分成最多*k*个不相交的簇，使得每个簇中的示例彼此相似。聚类问题有几种变体。例如，整数*k*可能不是给定的，而是由聚类过程产生。在本节中，我们假设*k*是给定的。
- en: '**Feature vectors and similarity**'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征向量和相似性**'
- en: 'Let’s formally define the clustering problem. The input is a set of *n* ***examples***.
    Each example has a set of ***attributes*** in common with all other examples,
    though the attribute values may vary among examples. For example, the clustering
    problem shown in [Figure 33.1](chapter033.xhtml#Fig_33-1) clusters *n* = 49 examples—48
    state capitals plus the District of Columbia—into *k* = 4 clusters. Each example
    has two attributes: the latitude and longitude of the capital. In a given clustering
    problem, each example has *d* attributes, with an example **x** specified by a
    *d*-dimensional ***feature vector***'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们正式定义聚类问题。输入是一组*n*个***示例***。每个示例与所有其他示例共享一组***属性***，尽管属性值可能在示例之间变化。例如，[图33.1](chapter033.xhtml#Fig_33-1)中显示的聚类问题将*n*
    = 49个示例（对象）——48个州首府加上哥伦比亚特区——聚类为*k* = 4个簇。每个示例有两个属性：首府的纬度和经度。在给定的聚类问题中，每个示例有*d*个属性，示例**x**由*d*维***特征向量***指定
- en: '**x** = (*x*[1], *x*[2], …, *x*[*d*]).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**x** = (*x*[1], *x*[2], …, *x*[*d*]).'
- en: Here, *x*[*a*] for *a* = 1, 2, …, *d* is a real number giving the value of attribute
    *a* for example **x**. We call **x** the ***point*** in ℝ^(*d*) representing the
    example. For the example in [Figure 33.1](chapter033.xhtml#Fig_33-1), each capital
    **x** has its latitude in *x*[1] and its longitude in *x*[2].
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*x*[*a*]对于*a* = 1, 2, …, *d*是一个给出示例**x**属性*a*的实数。我们称**x**为表示示例的ℝ^(*d*)中的***点***。对于[图33.1](chapter033.xhtml#Fig_33-1)中的示例，每个大写**x**都有其纬度在*x*[1]和经度在*x*[2]。
- en: 'In order to cluster similar points together, we need to define similarity.
    Instead, let’s define the opposite: the ***dissimilarity*** Δ(**x**, **y**) of
    points **x** and **y** is the squared Euclidean distance between them:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将相似的点聚集在一起，我们需要定义相似性。相反，让我们定义相反的：点**x**和**y**的***不相似度***Δ(**x**, **y**)是它们之间的欧氏距离的平方：
- en: '![art](images/Art_P1325.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1325.jpg)'
- en: Of course, for Δ(**x**, **y**) to be well defined, all attribute values must
    be present. If any are missing, then you might just ignore that example, or you
    could fill in a missing attribute value with the median value for that attribute.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，为了使Δ(**x**, **y**)有明确定义，所有属性值必须存在。如果有任何遗漏，那么你可以忽略该示例，或者你可以用该属性的中位数填补缺失的属性值。
- en: The attribute values are often “messy” in other ways, so that some “data cleaning”
    is necessary before the clustering algorithm is run. For example, the scale of
    attribute values can vary widely across attributes. In the example of [Figure
    33.1](chapter033.xhtml#Fig_33-1), the scales of the two attributes vary by a factor
    of 2, since latitude ranges from −90 to +90 degrees but longitude ranges from
    −180 to +180 degrees. You can imagine other scenarios where the differences in
    scales are even greater. If the examples contain information about students, one
    attribute might be grade-point average but another might be family income. Therefore,
    the attribute values are usually scaled or normalized, so that no single attribute
    can dominate the others when computing dissimilarities. One way to do so is by
    scaling attribute values with a linear transform so that the minimum value becomes
    0 and the maximum value becomes 1\. If the attribute values are binary values,
    then no scaling may be needed. Another option is scaling so that the values for
    each attribute have mean 0 and unit variance. Sometimes it makes sense to choose
    the same scaling rule for several related attributes (for example, if they are
    lengths measured to the same scale).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 属性值在其他方面通常是“混乱的”，因此在运行聚类算法之前需要一些“数据清理”。例如，属性值的规模在属性之间可能有很大的差异。在[图33.1](chapter033.xhtml#Fig_33-1)的示例中，两个属性的规模相差2倍，因为纬度范围从-90到+90度，而经度范围从-180到+180度。你可以想象其他情况，其中规模的差异甚至更大。如果示例包含有关学生的信息，一个属性可能是平均绩点，而另一个属性可能是家庭收入。因此，属性值通常被缩放或标准化，以便在计算不相似度时没有单个属性能够主导其他属性。一种方法是通过线性变换缩放属性值，使最小值变为0，最大值变为1。如果属性值是二进制值，则可能不需要缩放。另一种选择是缩放，使每个属性的值具有均值为0和单位方差。有时选择为几个相关属性选择相同的缩放规则是有意义的（例如，如果它们是以相同比例测量的长度）。
- en: '![art](images/Art_P1326.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1326.jpg)'
- en: '**Figure 33.1** The iterations of Lloyd’s procedure when clustering the capitals
    of the lower 48 states and the District of Columbia into *k* = 4 clusters. Each
    capital has two attributes: latitude and longitude. Each iteration reduces the
    value *f*, measuring the sum of squares of distances of all capitals to their
    cluster centers, until the value of *f* does not change. **(a)** The initial four
    clusters, with the capitals of Arkansas, Kansas, Louisiana, and Tennessee chosen
    as centers. **(b)–(k)** Iterations of Lloyd’s procedure. **(l)** The 11th iteration
    results in the same value of *f* as the 10th iteration in part (k), and so the
    procedure terminates.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**图33.1** Lloyd过程在将48个下属州和哥伦比亚特区的首府聚类成*k*=4个簇时的迭代。每个首府有两个属性：纬度和经度。每次迭代都会减少值*f*，衡量所有首府到其簇中心的距离平方和，直到值*f*不再改变。**(a)**
    初始的四个簇，选择阿肯色州、堪萨斯州、路易斯安那州和田纳西州的首府作为中心。**(b)–(k)** Lloyd过程的迭代。**(l)** 第11次迭代的结果与第10次迭代的结果相同，因此过程终止。'
- en: Also, the choice of dissimilarity measure is somewhat arbitrary. The use of
    the sum of squared differences as in equation (33.1) is not required, but it is
    a conventional choice and mathematically convenient. For the example of [Figure
    33.1](chapter033.xhtml#Fig_33-1), you might use the actual distance between capitals
    rather than equation (33.1).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，选择不相似性度量的方式有些是任意的。使用方程（33.1）中的平方差之和并不是必需的，但这是一个传统选择且在数学上方便。对于[图33.1](chapter033.xhtml#Fig_33-1)的例子，您可以使用首府之间的实际距离而不是方程（33.1）。
- en: '**Clusterings**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**聚类**'
- en: With the notion of similarity (actually, *dis*similarity) defined, let’s see
    how to define clusters of similar points. Let *S* denote the given set of *n*
    points in ℝ^(*d*). In some applications the points are not necessarily distinct,
    so that *S* is a multiset rather than a set.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 有了相似性（实际上是*不*相似性）的概念，让我们看看如何定义相似点的簇。让*S*表示实数空间ℝ^(*d*)中给定的*n*个点的集合。在一些应用中，点不一定是不同的，因此*S*是一个多重集而不是一个集合。
- en: Because the goal is to create *k* clusters, we define a ***k-clustering*** of
    *S* as a decomposition of *S* into a sequence 〈*S*^((1)), *S*^((2)), …, *S*^((*k*))〉
    of *k* disjoint subsets, or ***clusters***, so that
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 因为目标是创建*k*个簇，我们将*S*的***k-聚类***定义为将*S*分解为一系列〈*S*^((1)), *S*^((2)), …, *S*^((*k*))〉的*k*个不相交子集，或***簇***，以便
- en: '*S* = *S*^((1)) ⋃ *S*^((2)) ⋃ ⋯ ⋃ *S*^((*k*)).'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*S* = *S*^((1)) ⋃ *S*^((2)) ⋃ ⋯ ⋃ *S*^((*k*)).'
- en: A cluster may be empty, for example if *k* > 1 but all of the points in *S*
    have the same attribute values.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一个簇可能为空，例如如果*k*>1但*S*中的所有点具有相同的属性值。
- en: There are many ways to define a *k*-clustering of *S* and many ways to evaluate
    the quality of a given *k*-clustering. We consider here only *k*-clusterings of
    *S* that are defined by a sequence *C* of *k* ***centers***
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 定义*S*的*k*聚类的方式有很多种，评估给定*k*聚类的质量的方式也有很多种。我们这里只考虑由*k*个***中心***序列*C*定义的*S*的*k*聚类
- en: '*C* = 〈**c^((1))**, **c^((2))**, …, **c^((*k*))**〉,'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*C* = 〈**c^((1))**, **c^((2))**, …, **c^((*k*))**〉,'
- en: 'where each center is a point in ℝ^(*d*), and the ***nearest-center rule***
    says that a point **x** may belong to cluster *S*^((ℓ)) if the center of no other
    cluster is closer to **x** than the center **c^((ℓ))** of *S*^((ℓ)):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 每个中心都是ℝ^(*d*)中的一个点，***最近中心规则***表示如果没有其他簇的中心比点**x**到簇*S*^((ℓ))的中心**c^((ℓ))**更近，则点**x**可能属于簇*S*^((ℓ))：
- en: '**x** ∈ *S*^((ℓ)) only if Δ(**x**, **c^((ℓ))**) = min {Δ(**x**, **c^((*j*))**):
    1 ≤ *j* ≤ *k*}.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**x** ∈ *S*^((ℓ)) 当且仅当 Δ(**x**, **c^((ℓ))**) = min {Δ(**x**, **c^((*j*))**):
    1 ≤ *j* ≤ *k*}.'
- en: A center can be anywhere, and not necessarily a point in *S*.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一个中心可以位于任何地方，不一定是*S*中的一个点。
- en: Ties are possible and must be broken so that each point lies in exactly one
    cluster. In general, ties may be broken arbitrarily, although we’ll need the property
    that we never change which cluster a point **x** is assigned to unless the distance
    from **x** to its new cluster center is *strictly smaller* than the distance from
    **x** to its old cluster center. That is, if the current cluster has a center
    that is one of the closest cluster centers to **x**, then don’t change which cluster
    **x** is assigned to.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会出现并且必须解决平局的情况，以确保每个点只属于一个簇。一般来说，可以任意解决平局，尽管我们需要的是这样一个性质：除非点**x**到新簇中心的距离*严格小于*点**x**到旧簇中心的距离，否则不改变将点**x**分配给的簇。也就是说，如果当前簇的中心是距离点**x**最近的簇中心之一，则不要改变将点**x**分配给的簇。
- en: 'The ***k-means problem*** is then the following: given a set *S* of *n* points
    and a positive integer *k*, find a sequence ***C*** = 〈**c^((1))**, **c^((2))**,
    …, **c^((*k*))**〉 of *k* center points minimizing the sum *f*(*S*, *C*) of the
    squared distance from each point to its nearest center, where'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '***k均值问题***如下：给定一个*n*个点的集合*S*和一个正整数*k*，找到一系列***C*** = 〈**c^((1))**, **c^((2))**,
    …, **c^((*k*))**〉的*k*个中心点，使得每个点到其最近中心的距离的平方和*f*(*S*, *C*)最小，其中'
- en: '![art](images/Art_P1327.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1327.jpg)'
- en: In the second line, the *k*-clustering 〈*S*^((1)),*S*^((2)),…,*S*^((*k*))〉 is
    defined by the centers ***C*** and the nearest-center rule. See Exercise 33.1-1
    for an alternative formulation based on pairwise interpoint distances.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二行中，*k*聚类 〈*S*^((1)),*S*^((2)),…,*S*^((*k*))〉由中心***C***和最近中心规则定义。参见练习33.1-1，基于点对间距离的另一种表述。
- en: Is there a polynomial-time algorithm for the *k*-means problem? Probably not,
    because it is NP-hard [[310](bibliography001.xhtml#endnote_310)]. As we’ll see
    in [Chapter 34](chapter034.xhtml), NP-hard problems have no known polynomial-time
    algorithm, but nobody has ever proven that polynomial-time algorithms for NP-hard
    problems cannot exist. Although we know of no polynomial-time algorithm that finds
    the global minimum over all clusterings (according to equation (33.2)), we *can*
    find a local minimum.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*-均值问题是否存在多项式时间算法？可能不存在，因为它是NP难问题[[310](bibliography001.xhtml#endnote_310)]。正如我们将在[第34章](chapter034.xhtml)中看到的，NP难问题没有已知的多项式时间算法，但从未有人证明NP难问题不存在多项式时间算法。尽管我们不知道有没有多项式时间算法可以找到所有聚类中的全局最小值（根据方程（33.2）），但我们*可以*找到一个局部最小值。'
- en: 'Lloyd [[304](bibliography001.xhtml#endnote_304)] proposed a simple procedure
    that finds a sequence ***C*** of *k* centers that yields a local minimum of *f*(***S***,
    ***C***). A local minimum in the *k*-means problem satisfies two simple properties:
    each cluster has an optimal center (defined below), and each point is assigned
    to the cluster (or one of the clusters) with the closest center. Lloyd’s procedure
    finds a good clustering—possibly optimal—that satisfies these two properties.
    These properties are necessary, but not sufficient, for optimality.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Lloyd [[304](bibliography001.xhtml#endnote_304)] 提出了一个简单的过程，找到一系列*k*个中心点***C***，使得*f*(***S***,
    ***C***)的局部最小值。 *k*-means 问题的局部最小值满足两个简单的属性：每个聚类有一个最佳中心（下文定义），每个点被分配到最近中心的聚类（或其中一个聚类）。
    Lloyd 的过程找到一个满足这两个属性的良好聚类——可能是最优的。这些属性是必要的，但不足以保证最优性。
- en: '**Optimal center for a given cluster**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**给定聚类的最佳中心**'
- en: In an optimal solution to the *k*-means problem, each center point must be the
    ***centroid***, or ***mean***, of the points in its cluster. The centroid is a
    *d*-dimensional point, where the value in each dimension is the mean of the values
    of all the points in the cluster in that dimension (that is, the mean of the corresponding
    attribute values in the cluster). That is, if **c^((ℓ))** is the centroid for
    cluster *S*^((ℓ)), then for attributes *a* = 1, 2, …, *d*, we have
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在*k*-means问题的最优解中，每个中心点必须是其聚类中点的***质心***或***均值***。 质心是一个*d*维点，其中每个维度的值是该维度中聚类中所有点的值的平均���（即，聚类中相应属性值的平均值）。
    也就是说，如果**c^((ℓ))**是聚类*S*^((ℓ))的质心，则对于属性*a* = 1, 2, …, *d*，我们有
- en: '![art](images/Art_P1328.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1328.jpg)'
- en: Over all attributes, we write
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有属性上，我们写
- en: '![art](images/Art_P1329.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1329.jpg)'
- en: '***Theorem 33.1***'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '***定理 33.1***'
- en: Given a nonempty cluster *S*^((ℓ)), its centroid (or mean) is the unique choice
    for the cluster center **c^((ℓ))** ∈ ℝ^(*d*) that minimizes
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个非空聚类 *S*^((ℓ))，其质心（或均值）是使得聚类中心 **c^((ℓ))** ∈ ℝ^(*d*) 最小化的唯一选择
- en: '![art](images/Art_P1330.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1330.jpg)'
- en: '***Proof***   We wish to minimize, by choosing **c^((ℓ))** ∈ ℝ^(*d*), the sum'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '***证明***   我们希望通过选择 **c^((ℓ))** ∈ ℝ^(*d*) 来最小化和'
- en: '![art](images/Art_P1331.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1331.jpg)'
- en: 'For each attribute *a*, the term summed is a convex quadratic function in ![art](images/Art_P1332.jpg).
    To minimize this function, take its derivative with respect to ![art](images/Art_P1332.jpg)
    and set it to 0:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个属性*a*，求和的项是![艺术](images/Art_P1332.jpg)的凸二次函数。 为了最小化这个函数，对![艺术](images/Art_P1332.jpg)求导，并将其设置为0：
- en: '![art](images/Art_P1334.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1334.jpg)'
- en: or, equivalently,
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，等价地，
- en: '![art](images/Art_P1335.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1335.jpg)'
- en: Since the minimum is obtained uniquely when each coordinate of ![art](images/Art_P1332.jpg)
    is the average of the corresponding coordinate for **x** ∈ *S*^((ℓ)), the overall
    minimum is obtained when **c^((ℓ))** is the centroid of the points **x**, as in
    equation (33.3).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 由于当 ![艺术](images/Art_P1332.jpg) 的每个坐标都是**x** ∈ *S*^((ℓ)) 的对应坐标的平均值时，最小值是唯一获得的，因此当
    **c^((ℓ))** 是点 **x** 的质心时，整体最小值是在方程(33.3)中获得的。
- en: ▪
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ▪
- en: '**Optimal clusters for given centers**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**给定中心的最佳聚类**'
- en: The following theorem shows that the nearest-center rule—assigning each point
    **x** to one of the clusters whose center is nearest to **x**—yields an optimal
    solution to the *k*-means problem.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 以下定理表明，最近中心规则——将每个点**x**分配给其最近的中心所在的聚类——对*k*-means问题产生最优解。
- en: '***Theorem 33.2***'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '***定理 33.2***'
- en: Given a set *S* of *n* points and a sequence 〈**c^((1))**, **c^((2))**, …, **c^((*k*))**〉
    of *k* centers, a clustering 〈*S*^((1)), *S*^((2)), …, *S*^((*k*))〉 minimizes
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个包含*n*个点的集合 *S* 和一个序列 〈**c^((1))**, **c^((2))**, …, **c^((*k*))**〉 的*k*个中心，一个聚类
    〈*S*^((1)), *S*^((2)), …, *S*^((*k*))〉 最小化
- en: '![art](images/Art_P1337.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1337.jpg)'
- en: if and only if it assigns each point **x** ∈ *S* to a cluster *S*^((ℓ)) that
    minimizes Δ(**x**, **c^((ℓ))**).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当且仅当将每个点**x** ∈ *S*分配给最小化Δ(**x**, **c^((ℓ))**)的聚类*S*^((ℓ))时。
- en: '***Proof***   The proof is straightforward: each point **x** ∈ *S* contributes
    exactly once to the sum (33.4), and choosing to put **x** in a cluster whose center
    is nearest minimizes the contribution from **x**.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '***证明***   证明很简单：每个点**x** ∈ *S*恰好对和贡献一次，选择将**x**放入其最近中心的聚类最小化了**x**的贡献。'
- en: ▪
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ▪
- en: '**Lloyd’s procedure**'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**Lloyd 的过程**'
- en: 'Lloyd’s procedure just iterates two operations—assigning points to clusters
    based on the nearest-center rule, followed by recomputing the centers of clusters
    to be their centroids—until the results converge. Here is Lloyd’s procedure:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Lloyd 的过程只是迭代两个操作——根据最近中心规则将点分配到聚类，然后重新计算聚类的中心为它们的质心——直到结果收敛。以下是 Lloyd 的过程：
- en: '**Input:** A set *S* of points in ℝ^(*d*), and a positive integer *k*.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入:** 一个在 ℝ^(*d*) 中的点集 *S*，以及一个正整数 *k*。'
- en: '**Output:** A *k*-clustering 〈*S*^((1)), *S*^((2)), …, *S*^((*k*))〉 of *S*
    with a sequence of centers 〈**c^((1))**, **c^((2))**, …, **c^((*k*))**〉.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出:** 一个*k*聚类 〈*S*^((1)), *S*^((2)), …, *S*^((*k*))〉，带有一系列中心点 〈**c^((1))**,
    **c^((2))**, …, **c^((*k*))**〉。'
- en: '**Initialize centers:** Generate an initial sequence 〈**c^((1))**, **c^((2))**,
    …, **c^((*k*))**〉 of *k* centers by picking *k* points independently from *S*
    at random. (If the points are not necessarily distinct, see Exercise 33.1-3.)
    Assign all points to cluster *S*^((1)) to begin.'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**初始化中心:** 通过从*S*中随机选择*k*个点（如果点不一定不同，请参见练习 33.1-3）生成一个*k*个中心的初始序列 〈**c^((1))**,
    **c^((2))**, …, **c^((*k*))**〉。开始时将所有点分配给聚类*S*^((1))。'
- en: '**Assign points to clusters:** Use the nearest-center rule to define the clustering
    〈*S*^((1)), *S*^((2)), …, *S*^((*k*))〉. That is, assign each point **x** ∈ *S*
    to a cluster *S*^((ℓ)) having a nearest center (breaking ties arbitrarily, but
    not changing the assignment for a point **x** unless the new cluster center is
    strictly closer to **x** than the old one).'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**将点分配给聚类:** 使用最近中心规则定义聚类 〈*S*^((1)), *S*^((2)), …, *S*^((*k*))〉。 也就是说，将每个点**x**
    ∈ *S*分配给具有最近中心的聚类*S*^((ℓ))（任意打破平局，但不改变点的分配，除非新的聚类中心严格比旧的更接近**x**）。'
- en: '**Stop if no change:** If step 2 did not change the assignments of any points
    to clusters, then stop and return the clustering 〈*S*^((1)), *S*^((2)), …, *S*^((*k*))〉
    and the associated centers 〈**c^((1))**, **c^((2))**, …, **c^((*k*))**〉. Otherwise,
    go to step 4.'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**如果没有变化则停止：** 如果步骤2没有改变任何点分配到簇的情况，则停止并返回聚类〈*S*^((1)), *S*^((2)), …, *S*^((*k*))〉和相关中心〈**c^((1))**,
    **c^((2))**, …, **c^((*k*))**〉。否���，转到步骤4。'
- en: '**Recompute centers as centroids:** For ℓ = 1, 2, …, *k*, compute the center
    **c^((ℓ))** of cluster *S*^((ℓ)) as the centroid of the points in *S*^((ℓ)). (If
    *S*^((ℓ)) is empty, let **c^((ℓ))** be the zero vector.) Then go to step 2.'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**重新计算中心作为质心：** 对于ℓ = 1, 2, …, *k*，将簇*S*^((ℓ))的中心**c^((ℓ))**计算为*S*^((ℓ))中点的质心。（如果*S*^((ℓ))为空，则让**c^((ℓ))**为零向量。）然后转到步骤2。'
- en: It is possible for some of the clusters returned to be empty, particularly if
    many of the input points are identical.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的一些簇可能为空，特别是如果许多输入点是相同的。
- en: Lloyd’s procedure always terminates. By Theorem 33.1, recomputing the centers
    of each cluster as the cluster centroid cannot increase *f*(*S*, *C*). Lloyd’s
    procedure ensures that a point is reassigned to a different cluster only when
    such an operation strictly decreases *f*(*S*, *C*). Thus each iteration of Lloyd’s
    procedure, except the last iteration, must strictly decrease *f*(*S*, *C*). Since
    there are only a finite number of possible *k*-clusterings of *S* (at most *k*^(*n*)),
    the procedure must terminate. Furthermore, once one iteration of Lloyd’s procedure
    yields no decrease in *f*, further iterations would not change anything, and the
    procedure can stop at this locally optimum assignment of points to clusters.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Lloyd的过程总是终止的。根据定理33.1，重新计算每个簇的中心作为簇质心不会增加*f*(*S*, *C*)。Lloyd的过程确保只有在这样的操作严格减少*f*(*S*,
    *C*)时才将点重新分配到不同的簇中。因此，Lloyd的过程的每次迭代，除了最后一次迭代外，必须严格减少*f*(*S*, *C*)。由于*S*的可能*k*个聚类（最多*k*^(*n*)）只有有限数量，该过程必须终止。此外，一旦Lloyd的过程的一个迭代没有减少*f*，进一步的迭代将不会改变任何内容，过程可以在这个局部最优的点分配到簇中停止。
- en: If Lloyd’s procedure really required *k*^(*n*) iterations, it would be impractical.
    In practice, it sometimes suffices to terminate the procedure when the percentage
    decrease in *f*(*S*, *C*) in the latest iteration falls below a predetermined
    threshold. Because Lloyd’s procedure is guaranteed to find only a locally optimal
    clustering, one approach to finding a good clustering is to run Lloyd’s procedure
    many times with different randomly chosen initial centers, taking the best result.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果Lloyd的过程确实需要*k*^(*n*)次迭代，那将是不切实际的。实际上，有时在最新迭代中*f*(*S*, *C*)的百分减少低于预定阈值时终止该过程就足够了。由于Lloyd的过程只能找到局部最优的聚类，找到一个好的聚类的一种方法是用不同随机选择的初始中心多次运行Lloyd的过程，取最佳结果。
- en: The running time of Lloyd’s procedure is proportional to the number *T* of iterations.
    In one iteration, assigning points to clusters based on the nearest-center rule
    requires *O*(*dkn*) time, and recomputing new centers for each cluster requires
    *O*(*dn*) time (because each point is in one cluster). The overall running time
    of the *k*-means procedure is thus *O*(*Tdkn*).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Lloyd的过程的运行时间与迭代次数*T*成正比。在一次迭代中，根据最近中心规则将点分配到簇需要*O*(*dkn*)时间，并且为每个簇重新计算新中心需要*O*(*dn*)时间（因为每个点只在一个簇中）。因此，*k*均值过程的总运行时间为*O*(*Tdkn*)。
- en: 'Lloyd’s algorithm illustrates an approach common to many machine-learning algorithms:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Lloyd算法说明了许多机器学习算法共有的一种方法：
- en: First, define a hypothesis space in terms an appropriate sequence *θ* of parameters,
    so that each *θ* is associated with a specific hypothesis *h*[*θ*]. (For the *k*-means
    problem, *θ* is a *dk*-dimensional vector, equivalent to *C*, containing the *d*-dimensional
    center of each of the *k* clusters, and *h*[*θ*] is the hypothesis that each data
    point **x** should be grouped with a cluster having a center closest to **x**.)
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，在适当的参数序列*θ*中定义一个假设空间，使得每个*θ*与特定假设*h*[*θ*]相关联。 （对于*k*均值问题，*θ*是一个*dk*维向量，等同于*C*，包含*k*个簇的每个*d*维中心，*h*[*θ*]是每个数据点**x**应该与最接近**x**的中心的簇分组的假设。）
- en: Second, define a measure *f*(*E*, *θ*) describing how poorly hypothesis *h*[*θ*]
    fits the given training data *E*. Smaller values of *f*(*E*, *θ*) are better,
    and a (locally) optimal solution (locally) minimizes *f*(*E*, *θ*). (For the *k*-means
    problem, *f*(*E*, *θ*) is just *f*(*S*, *C*).)
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，定义一个度量*f*(*E*, *θ*)描述假设*h*[*θ*]如何拟合给定的训练数据*E*。*f*(*E*, *θ*)的值越小越好，而（局部）最优解（局部）最小化*f*(*E*,
    *θ*)。 （对于*k*均值问题，*f*(*E*, *θ*)就是*f*(*S*, *C*)。）
- en: Third, given a set of training data *E*, use a suitable optimization procedure
    to find a value of *θ** that minimizes *f*(*E*, *θ**), at least locally. (For
    the *k*-means problem, this value of *θ** is the sequence *C* of *k* center points
    returned by Lloyd’s algorithm.)
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三，给定一组训练数据*E*，使用适当的优化过程找到最小化*f*(*E*, *θ*)的*θ**值，至少在局部上。 （对于*k*均值问题，Lloyd算法返回的*k*个中心点序列*C*就是这个*θ**值。）
- en: Return *θ** as the answer.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将*θ**作为答案返回。
- en: In this framework, we see that optimization becomes a powerful tool for machine
    learning. Using optimization in this way is flexible. For example, ***regularization***
    terms can be incorporated in the function to be minimized, in order to penalize
    hypotheses that are “too complicated” and that “overfit” the training data. (Regularization
    is a complex topic that isn’t pursued further here.)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个框架中，我们看到优化成为机器学习的强大工具。以这种方式使用优化是灵活的。例如，***正则化***项可以被纳入要最小化的函数中，以惩罚“太复杂”并且“过度拟合”训练数据的假设。（正则化是一个复杂的主题，在这里不再深究。）
- en: '**Examples**'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**例子**'
- en: '[Figure 33.1](chapter033.xhtml#Fig_33-1) demonstrates Lloyd’s procedure on
    a set of *n* = 49 cities: 48 U.S. state capitals and the District of Columbia.
    Each city has *d* = 2 dimensions: latitude and longitude. The initial clustering
    in part (a) of the figure has the initial cluster centers arbitrarily chosen as
    the capitals of Arkansas, Kansas, Louisiana, and Tennessee. As the procedure iterates,
    the value of the function *f* decreases, until the 11th iteration in part (l),
    where it remains the same as in the 10th iteration in part (k). Lloyd’s procedure
    then terminates with the clusters shown in part (l).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[图33.1](chapter033.xhtml#Fig_33-1)展示了洛伊德过程在一组*n* = 49个城市上的应用：48个美国州首府和哥伦比亚特区。每个城市有*d*
    = 2个维度：纬度和经度。图的(a)部分中的初始聚类将初始聚类中心任意选择为阿肯色州、堪萨斯州、路易斯安那州和田纳西州的首府。随着过程的迭代，函数*f*的值减小，直到第11次迭代在(l)部分，其值与第10次迭代在(k)部分相同。然后洛伊德过程以(l)部分显示的聚类结束。'
- en: As [Figure 33.2](chapter033.xhtml#Fig_33-2) shows, Lloyd’s procedure can also
    apply to “vector quantization.” Here, the goal is to reduce the number of distinct
    colors required to represent a photograph, thereby allowing the photograph to
    be greatly compressed (albeit in a lossy manner). In part (a) of the figure, an
    original photograph 700 pixels wide and 500 pixels high uses 24 bits (three bytes)
    per pixel to encode a triple of red, green, and blue (RGB) primary color intensities.
    Parts (b)–(e) of the figure show the results of using Lloyd’s procedure to compress
    the picture from a initial space of 2^(24) possible values per pixel to a space
    of only *k* = 4, *k* = 16, *k* = 64, or *k* = 256 possible values per pixel; these
    *k* values are the cluster centers. The photograph can then be represented with
    only 2, 4, 6, or 8 bits per pixel, respectively, instead of the 24-bits per pixel
    needed by the initial photograph. An auxiliary table, the “palette,” accompanies
    the compressed image; it holds the *k* 24-bit cluster centers and is used to map
    each pixel value to its 24-bit cluster center when the photo is decompressed.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图33.2](chapter033.xhtml#Fig_33-2)所示，洛伊德的过程也适用于“向量量化”。在这里，目标是减少表示照片所需的不同颜色数量，从而允许照片被大大压缩（尽管以有损的方式）。在图的(a)部分，一个原始照片宽700像素，高500像素，每个像素使用24位（三个字节）来编码红色、绿色和蓝色（RGB）主色强度的三元组。图的(b)–(e)部分显示了使用洛伊德过程将图片从每个像素可能值为2^(24)压缩到每个像素只有*k*
    = 4、*k* = 16、*k* = 64或*k* = 256个可能值的结果；这些*k*值是聚类中心。然后，照片可以用每个像素只有2、4、6或8位来表示，而不是初始照片所需的每个像素24位。一个辅助表，“调色板”，伴随着压缩图像；它保存*k*个24位聚类中心，并在解压照片时用于将每个像素值映射到其24位聚类中心。
- en: '**Exercises**'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习**'
- en: '***33.1-1***'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '***33.1-1***'
- en: Show that the objective function *f*(*S*, *C*) of equation (33.2) may be alternatively
    written as
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 表明方程(33.2)的目标函数*f*(*S*, *C*)也可以替代地写成
- en: '![art](images/Art_P1338.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1338.jpg)'
- en: '***33.1-2***'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '***33.1-2***'
- en: Give an example in the plane with *n* = 4 points and *k* = 2 clusters where
    an iteration of Lloyd’s procedure does not improve *f*(*S*, *C*), yet the *k*-clustering
    is not optimal.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 给出一个在平面上具有*n* = 4个点和*k* = 2个簇的例子，其中洛伊德过程的一次迭代并没有改善*f*(*S*, *C*)，但*k*聚类并不是最佳的。
- en: '***33.1-3***'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '***33.1-3***'
- en: When the input to Lloyd’s procedure contains many repeated points, a different
    initialization procedure might be used. Describe a way to pick a number of centers
    at random that maximizes the number of distinct centers picked. (*Hint:* See Exercise
    5.3-5.)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 当洛伊德过程的输入包含许多重复的点时，可能会使用不同的初始化过程。描述一种以随机方式选择中心点数量的方法，以最大化选择的不同中心点的数量。（*提示：*参见练习5.3-5。）
- en: '***33.1-4***'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '***33.1-4***'
- en: Show how to find an optimal *k*-clustering in polynomial time when there is
    just one attribute (*d* = 1).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 展示如何在只有一个属性（*d* = 1）时在多项式时间内找到最佳的*k*聚类。
- en: '![art](images/Art_P1339.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1339.jpg)'
- en: '**Figure 33.2** Using Lloyd’s procedure for vector quantization to compress
    a photo by using fewer colors. **(a)** The original photo has 350,000 pixels (700
    × 500), each a 24-bit RGB (red/blue/green) triple of 8-bit values; these pixels
    (colors) are the “points” to be clustered. Points repeat, so there are only 79,083
    distinct colors (less than 2^(24)). After compression, only *k* distinct colors
    are used, so each pixel is represented by only ⌈1g *k*⌉ bits instead of 24\. A
    “palette” maps these values back to 24-bit RGB values (the cluster centers). **(b)–(e)**
    The same photo with *k* = 4, 16, 64, and 256 colors. (Photo from standuppaddle,
    pixabay.com.)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**图33.2** 使用洛伊德过程对照片进行向量量化压缩，使用更少的颜色。**(a)** 原始照片有35万像素（700 × 500），每个像素是一个24位RGB（红/蓝/绿）三元组，每个值为8位；这些像素（颜色）是要进行聚类的“点”。由于像素重复，因此只有79,083种不同的颜色（少于2^(24)）。压缩后，只使用*k*种不同的颜色，因此每个像素只用⌈1g
    *k*⌉位来表示，而不是24位。一个“调色板”将这些值映射回24位RGB值（聚类中心）。**(b)–(e)** 具有*k* = 4、16、64和256种颜色的相同照片。（照片来自standuppaddle,
    pixabay.com。）'
- en: '[**33.2    Multiplicative-weights algorithms**](toc.xhtml#Rh1-195)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[**33.2    乘法权重算法**](toc.xhtml#Rh1-195)'
- en: 'This section considers problems that require you to make a series of decisions.
    After each decision you receive feedback as to whether your decision was correct.
    We will study a class of algorithms that are called ***multiplicative-weights
    algorithms***. This class of algorithms has a wide variety of applications, including
    game playing in economics, approximately solving linear-programming and multicommodity-flow
    problems, and various applications in online machine learning. We emphasize the
    online nature of the problem here: you have to make a sequence of decisions, but
    some of the information needed to make the *i*th decision appears only after you
    have already made the (*i* – 1)st decision. In this section, we look at one particular
    problem, known as “learning from experts,” and develop an example of a multiplicative-weights
    algorithm, called the weighted-majority algorithm.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 本节考虑需要你做一系列决策的问题。每次决策后，你会收到有关你的决策是否正确的反馈。我们将研究一类被称为***乘权算法***的算法。这类算法有各种应用，包括在经济学中进行游戏，近似解决线性规划和多商品流问题，以及在线机器学习中的各种应用。我们在这里强调问题的在线性质：你必须做出一系列决策，但有关做第*i*个决策所需的一些信息只有在你已经做出第(*i*
    – 1)个决策后才会出现。在本节中，我们将研究一个特定的问题，称为“从专家中学习”，并开发一个乘权算法的示例，称为加权多数算法。
- en: 'Suppose that a series of events will occur, and you want to make predictions
    about these events. For example, over a series of days, you want to predict whether
    it is going to rain. Or perhaps you want to predict whether the price of a stock
    will increase or decrease. One way to approach this problem is to assemble a group
    of “experts” and use their collective wisdom in order to make good predictions.
    Let’s denote the experts, *n* of them, by *E*[1], *E*[2], …, *E*[*n*], and let’s
    say that *T* events are going to take place. Each event has an outcome of either
    0 or 1, with *o*^((*t*)) denoting the outcome of the *t*th event. Before event
    *t*, each expert *E*^((*i*)) makes a prediction ![art](images/Art_P1340.jpg).
    You, as the “learner,” then take the set of *n* expert predictions for event *t*
    and produce a single prediction *p*^((*t*)) ∈ {0, 1} of your own. You base your
    prediction only on the predictions of the experts and anything you have learned
    about the experts from their previous predictions. You do not use any additional
    information about the event. Only after making your prediction do you ascertain
    the outcome *o*^((*t*)) of event *t*. If your prediction *p*^((*t*)) matches *o*^((*t*)),
    then you were correct; otherwise, you made a mistake. The goal is to minimize
    the total number *m* of mistakes, where ![art](images/Art_P1341.jpg). You can
    also keep track of the number of mistakes each expert makes: expert *E*[*i*] makes
    *m*[*i*] mistakes, where ![art](images/Art_P1342.jpg).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一系列事件将会发生，你想对这些事件进行预测。例如，在一系列的日子里，你想预测是否会下雨。或者你想预测股票价格是增加还是减少。解决这个问题的一种方法是组建一个“专家”团队，并利用他们的集体智慧来做出良好的预测。我们用*E*[1]，*E*[2]，…，*E*[*n*]来表示*n*个专家，假设将发生*T*个事件。每个事件的结果只能是0或1，其中*o*^((*t*))表示第*t*个事件的结果。在事件*t*之前，每个专家*E*^((*i*))都会做出预测
    ![art](images/Art_P1340.jpg)。你作为“学习者”，然后获取专家对事件*t*的*n*个预测，并产生自己的单一预测*p*^((*t*))
    ∈ {0, 1}。你的预测仅基于专家的预测和你从他们之前预测中学到的任何信息。你不使用有关事件的任何其他信息。只有在做出预测后，你才会确认事件*t*的结果*o*^((*t*))。如果你的预测*p*^((*t*))与*o*^((*t*))匹配，那么你是正确的；否则，你就犯了错误。目标是最小化总错误数*m*，其中
    ![art](images/Art_P1341.jpg)。你还可以跟踪每个专家犯的错误数：专家*E*[*i*]犯了*m*[*i*]个错误，其中 ![art](images/Art_P1342.jpg)。
- en: For example, suppose that you are following the price of a stock, and each day
    you decide whether to invest in it for just that day by buying it at the beginning
    of the day and selling it at the end of the day. If, on some day, you buy the
    stock and it goes up, then you made the correct decision, but if the stock goes
    down, then you made a mistake. Similarly, if on some day, you do not buy the stock
    and it goes down, then you made the correct decision, but if the stock goes up,
    then you made a mistake. Since you would like to make as few mistakes as possible,
    you use the advice of the experts to make your decisions.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你正在关注一只股票的价格，每天你都要决定是否只在当天投资于它，即在当天开始时购买，然后在当天结束时出售。如果某一天你买入股票并且价格上涨，那么你做出了正确的决定，但如果股票下跌，那么你就犯了错误。同样地，如果某一天你没有购买股票而价格下跌，那么你做出了正确的决定，但如果股票上涨，那么你就犯了错误。由于你希望犯尽可能少的错误，你会使用专家的建议来做出决策。
- en: 'We’ll assume nothing about the movement of the stock. We’ll also assume nothing
    about the experts: the experts’ predictions could be correlated, they could be
    chosen to deceive you, or perhaps some are not really experts after all. What
    algorithm would you use?'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不假设股票的波动情况。我们也不假设专家的情况：专家的预测可能是相关的，他们可能被选择来欺骗你，或者有些人根本不是真正的专家。你会使用什么算法？
- en: 'Before designing an algorithm for this problem, we need to consider what is
    a fair way to evaluate our algorithm. It is reasonable to expect that our algorithm
    performs better when the expert predictions are better, and that it performs worse
    when the expert predictions are worse. The goal of the algorithm is to limit the
    number of mistakes you make to be close to the number of mistakes that the best
    of the experts makes. At first, this goal might seem impossible, because you do
    not know until the end which expert is best. We’ll see, however, that by taking
    the advice provided by all the experts into account, you can achieve this goal.
    More formally, we use the notion of “regret,” which compares our algorithm to
    the performance of the best expert (in hindsight) over all. Letting *m** = min
    {*m*[*i*] : 1 ≤ *i* ≤ *n*} denote the number of mistakes made by the best expert,
    the ***regret*** is *m* – *m**. The goal is to design an algorithm with low regret.
    (Regret can be negative, although it typically isn’t, since it is rare that you
    do better than the best expert.)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '在为这个问题设计算法之前，我们需要考虑如何公平地评估我们的算法。合理地期望我们的算法在专家预测更好时表现更好，在专家预测更差时表现更差。算法的目标是将你犯错的次数限制在接近最好专家犯错次数的范围内。起初，这个目标可能看起来不可能实现，因为直到最后你才知道哪位专家是最好的。然而，我们将看到，通过考虑所有专家提供的建议，你可以实现这个目标。更正式地，我们使用“遗憾”这个概念，比较我们的算法与最佳专家（事后）的表现。让
    *m** = min {*m*[*i*] : 1 ≤ *i* ≤ *n*} 表示最佳专家犯错的次数，***遗憾*** 是 *m* – *m**。目标是设计一个具有低遗憾的算法。（遗憾可以是负数，尽管通常不会，因为很少有你比最佳专家做得更好的情况。）'
- en: As a warm-up, let’s consider the case in which one of the experts makes a correct
    prediction each time. Even without knowing who that expert is, you can still achieve
    good results.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 作为热身，让我们考虑一位专家每次都做出正确预测的情况。即使不知道是哪位专家，你仍然可以取得良好的结果。
- en: '***Lemma 33.3***'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '***引理 33.3***'
- en: Suppose that out of *n* experts, there is one who always makes the correct prediction
    for all *T* events. Then there is an algorithm that makes at most ⌈1g *n*⌉ mistakes.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 假设在 *n* 位专家中，有一位专家对所有 *T* 个事件都做出正确预测。那么存在一个算法最多会犯 ⌈1g *n*⌉ 个错误。
- en: '***Proof***   The algorithm maintains a set *S* consisting of experts who have
    not yet made a mistake. Initially, *S* contains all *n* experts. The algorithm’s
    prediction is always the majority vote of the predictions of the experts remaining
    in set *S*. In case of a tie, the algorithm makes any prediction. After each outcome
    is learned, set *S* is updated to remove all the experts who made an incorrect
    prediction about that outcome.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '***证明*** 算法维护一个由尚未犯错的专家组成的集合 *S*。最初，*S* 包含所有 *n* 位专家。算法的预测始终是 *S* 中剩余专家预测的多数意见。在出现平局时，算法会做出任意预测。每次学习到一个结果后，集合
    *S* 会更新，移除所有对该结果做出错误预测的专家。'
- en: We now analyze the algorithm. The expert who always makes the correct prediction
    will always be in set *S*. Every time the algorithm makes a mistake, at least
    half of the experts who were still in *S* also make a mistake, and these experts
    are removed from *S*. If *S*′ is the set of experts remaining after removing those
    who made a mistake, we have that |*S*′| ≤ |*S*|/2\. The size of *S* can be halved
    at most ⌈1g *n*⌉ times until |*S*| = 1\. From this point on, we know that the
    algorithm never makes a mistake, since the set *S* consists only of the one expert
    who never makes a mistake. Therefore, overall the algorithm makes at most ⌈1g
    *n*⌉ mistakes.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来分析算法。总是做出正确预测的专家将始终在集合 *S* 中。每次算法犯错时，至少一半仍在 *S* 中的专家也会犯错，并且这些专家会被移出 *S*。如果
    *S*′ 是移除那些犯错专家后剩余的专家集合，则有 |*S*′| ≤ |*S|/2。*S* 的大小最多可以被减半 ⌈1g *n*⌉ 次，直到 |*S*| =
    1。从这一点开始，我们知道算法永远不会犯错，因为集合 *S* 只包含那位从不犯错的专家。因此，总体上算法最多会犯 ⌈1g *n*⌉ 个错误。
- en: ▪
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ▪
- en: Exercise 33.2-1 asks you to generalize this result to the case when there is
    no expert who makes perfect predictions and show that, for any set of experts,
    there is an algorithm that makes at most *m** ⌈1g *n*⌉ mistakes. The generalized
    algorithm begins in the same way. The set *S* might become empty at some point,
    however. If that ever happens, reset *S* to contain all the experts and continue
    the algorithm.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 33.2-1 要求你将这个结果推广到没有专家能够做出完美预测的情况，并证明，对于任意一组专家，存在一个算法最多会犯 *m** ⌈1g *n*⌉ 个错误。推广算法的开始方式相同。然而，集合
    *S* 可能在某个时刻变为空。如果发生这种情况，重置 *S* 包含所有专家并继续算法。
- en: You can substantially improve your prediction ability by not just tracking which
    experts have not made any mistakes, or have not made any mistakes recently, to
    a more nuanced evaluation of the quality of each expert. The key idea is to use
    the feedback you receive to update your evaluation of how much trust to put in
    each expert. As the experts make predictions, you observe whether they were correct
    and decrease your confidence in the experts who make more mistakes. In this way,
    you can learn over time which experts are more reliable and which are less reliable,
    and weight their predictions accordlingly. The change in weights is accomplished
    via multiplication, hence the term “multiplicative weights.”
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 通过不仅跟踪哪些专家没有犯错，或者最近没有犯错，而是更细致地评估每位专家的质量，你可以大幅提高你的预测能力。关键思想是利用你收到的反馈来更新你对每位专家值得信任程度的评估。当专家做���预测时，你观察他们是否正确，并减少对那些犯更多错误的专家的信心。通过这种方式，你可以随着时间学习哪些专家更可靠，哪些不太可靠，并相应地权衡他们的预测。权重的变化通过乘法实现，因此有“乘法权重”这个术语。
- en: The algorithm appears in the procedure WEIGHTED-MAJORITY on the following page,
    which takes a set *E* = {*E*[1], *E*[2], …, *E*[*n*]} of experts, a number *T*
    of events, the number *n* of experts, and a parameter 0 < *γ* ≤ 1/2 that controls
    how the weights change. The algorithm maintains weights ![art](images/Art_P1343.jpg)
    for *i* = 1, 2, …, *n* and *t* = 1, 2, …, *T*, where ![art](images/Art_P1344.jpg).
    The **for** loop of lines 1–2 sets the initial weights ![art](images/Art_P1345.jpg)
    to 1, capturing the idea that with no knowledge, you trust each expert equally.
    Each iteration of the main **for** loop of lines 3–18 does the following for an
    event *t* = 1, 2, …, *T*. Each expert *E*[*i*] makes a prediction for event *t*
    in line 4\. Lines 5–8 compute *upweight*^((*t*)), the sum of the weights of the
    experts who predict 1 for event *t*, and *downweight*^((*t*)), the sum of the
    weights of the experts who predict 0 for the event. Lines 9–11 decide the algorithm’s
    prediction *p*^((*t*)) for event *t* based on whichever weighted sum is larger
    (breaking ties in favor of deciding 1). The outcome of event *t* is revealed in
    line 12\. Finally, lines 14–17 decrease the weights of the experts who made an
    incorrect prediction for event *t* by multiplying their weights by 1 – *γ*, leaving
    alone the weights of the experts who correctly predicted the event’s outcome.
    Thus, the fewer mistakes each expert makes, the higher that expert’s weight.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 算法出现在下一页的WEIGHTED-MAJORITY过程中，该过程接受一组专家 *E* = {*E*[1], *E*[2], …, *E*[*n*]}，事件数量
    *T*，专家数量 *n*，以及一个控制权重变化的参数 0 < *γ* ≤ 1/2。算法维护了 *i* = 1, 2, …, *n* 和 *t* = 1, 2,
    …, *T* 的权重，其中 ![艺术](images/Art_P1344.jpg)。行1-2的**对于**循环设置了初始权重 ![艺术](images/Art_P1345.jpg)
    为1，捕捉了没有知识时对每个专家的平等信任的想法。主要行3-18的**对于**循环的每次迭代都针对事件 *t* = 1, 2, …, *T* 执行以下操作。每个专家
    *E*[i] 在第4行为事件 *t* 进行预测。行5-8计算了 *upweight*^((*t*))，预测事件 *t* 为1的专家权重之和，以及 *downweight*^((*t*))，预测事件为0的专家权重之和。行9-11根据哪个加权和更大（在决定为1的情况下打破平局）来决定算法对事件
    *t* 的预测 *p*^((*t*))。事件 *t* 的结果在第12行揭示。最后，行14-17通过将错误预测事件 *t* 的专家的权重乘以1 - *γ* 来减少专家的权重，保持正确预测事件结果的专家的权重不变。因此，每个专家犯的错误越少，该专家的权重就越高。
- en: The WEIGHTED-MAJORITY procedure doesn’t do much worse than any expert. In particular,
    it doesn’t do much worse than the best expert. To quantify this claim, let *m*^((*t*))
    be the number of mistakes made by the procedure through event *t*, and let ![art](images/Art_P1346.jpg)
    be the number of mistakes made by expert *E*[*i*] through event *t*. The following
    theorem is the key.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: WEIGHTED-MAJORITY过程的表现不会比任何专家差太多。特别是，它不会比最好的专家差太多。为了量化这一说法，让 *m*^((*t*)) 表示通过事件
    *t* 过程中所犯错误的次数，让 ![艺术](images/Art_P1346.jpg) 表示专家 *E*[i] 通过事件 *t* 所犯错误的次数。以下定理是关键。
- en: WEIGHTED-MAJORITY(*E*, *T*, *n*, *γ*)
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: WEIGHTED-MAJORITY(*E*, *T*, *n*, *γ*)
- en: '|   1 | **for** *i* = 1 **to** *n* |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|   1 | **对于** *i* = 1 **到** *n*'
- en: '|   2 | ![art](images/Art_P1347.jpg) | **//** trust each expert equally |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|   2 | ![艺术](images/Art_P1347.jpg) | **//** 平等信任每个专家'
- en: '|   3 | **for** *t* = 1 **to** *T* |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|   3 | **对于** *t* = 1 **到** *T*'
- en: '|   4 | each expert *E*[*i*] ∈ *E* makes a prediction ![art](images/Art_P1348.jpg)
    |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|   4 | 每个专家 *E*[i] ∈ *E* 都会进行预测 ![艺术](images/Art_P1348.jpg)'
- en: '|   5 | ![art](images/Art_P1349.jpg) | **//** experts who predicted 1 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|   5 | ![艺术](images/Art_P1349.jpg) | **//** 预测为1的专家'
- en: '|   6 | ![art](images/Art_P1350.jpg) | **//** sum of weights of who predicted
    1 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|   6 | ![艺术](images/Art_P1350.jpg) | **//** 预测为1的权重之和'
- en: '|   7 | ![art](images/Art_P1351.jpg) | **//** experts who predicted 0 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|   7 | ![艺术](images/Art_P1351.jpg) | **//** 预测为0的专家'
- en: '|   8 | ![art](images/Art_P1352.jpg) | **//** sum of weights of who predicted
    0 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|   8 | ![艺术](images/Art_P1352.jpg) | **//** 预测为0的权重之和'
- en: '|   9 | **if** *upweight*^((*t*)) ≥ *downweight*^((*t*)) |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|   9 | **如果** *upweight*^((*t*)) ≥ *downweight*^((*t*))'
- en: '| 10 | *p*^((*t*)) = 1 | **//** algorithm predicts 1 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 10 | *p*^((*t*)) = 1 | **//** 算法预测为1'
- en: '| 11 | **else** *p*^((*t*)) = 0 | **//** algorithm predicts 0 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 11 | **否则** *p*^((*t*)) = 0 | **//** 算法预测为0'
- en: '| 12 | outcome *o*^((*t*)) is revealed |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 结果 *o*^((*t*)) 被揭示'
- en: '| 13 | **//** If *p*^((*t*)) ≠ *o*^((*t*)), the algorithm made a mistake. |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 13 | **//** 如果 *p*^((*t*)) ≠ *o*^((*t*))，则算法出错了。'
- en: '| 14 | **for** *i* = 1 **to** *n* |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 14 | **对于** *i* = 1 **到** *n*'
- en: '| 15 | **if** ![art](images/Art_P1353.jpg) | **//** if expert *E*^((*i*)) made
    a mistake … |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 15 | **如果** ![艺术](images/Art_P1353.jpg) | **//** 如果专家 *E*^((*i*)) 犯了错误…'
- en: '| 16 | ![art](images/Art_P1354.jpg) | **//** … then decrease that expert’s
    weight |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 16 | ![艺术](images/Art_P1354.jpg) | **//** … 然后减少该专家的权重。'
- en: '| 17 | **else** ![art](images/Art_P1355.jpg) |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 17 | **否则** ![艺术](images/Art_P1355.jpg)'
- en: '| 18 | **return** *p*^((*t*)) |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 18 | **返回** *p*^((*t*))'
- en: '***Theorem 33.4***'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '***定理 33.4***'
- en: When running WEIGHTED-MAJORITY, we have, for every expert *E*[*i*] and every
    event *T*′ ≤ *T*,
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 运行WEIGHTED-MAJORITY时，对于每个专家 *E*[i] 和每个事件 *T*′ ≤ *T*，
- en: '![art](images/Art_P1356.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1356.jpg)'
- en: '***Proof***   Every time an expert *E*[*i*] makes a mistake, its weight, which
    is initially 1, is multiplied by 1 – *γ*, and so we have'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '***证明*** 每当专家 *E*[i] 犯错时，其初始权重为1，会被乘以1 - *γ*，因此我们有'
- en: '![art](images/Art_P1357.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1357.jpg)'
- en: for *t* = 1, 2, …, *T*.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *t* = 1, 2, …, *T*.
- en: We use a potential function ![art](images/Art_P1358.jpg), summing the weights
    for all *n* experts after iteration *t* of the **for** loop of lines 3–18\. Initially,
    we have *W*(0) = *n* since all *n* weights start out with the value 1\. Because
    each expert belongs to either the set *U* or the set *D* (defined in lines 5 and
    7 of WEIGHTED-MAJORITY), we always have *W*(*t*) = *upweight*^((*t*)) + *downweight*^((*t*))
    after each execution of line 8.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一个潜在函数 ![艺术](images/Art_P1358.jpg)，在行3-18的**对于**循环的第 *t* 次迭代后对所有 *n* 个专家的权重求和。最初，我们有
    *W*(0) = *n*，因为所有 *n* 个权重的初始值为1。因为每个专家都属于行5和7中定义的集合 *U* 或 *D*（WEIGHTED-MAJORITY中定义），所以在每次执行第8行后，我们总是有
    *W*(*t*) = *upweight*^((*t*)) + *downweight*^((*t*))。
- en: Consider an iteration *t* in which the algorithm makes a mistake in its prediction,
    which means that either the algorithm predicts 1 and the outcome is 0 or the algorithm
    predicts 0 and the outcome is 1\. Without loss of generality, assume that the
    algorithm predicts 1 and the outcome is 0\. The algorithm predicted 1 because
    *upweight*^((*t*)) ≥ *downweight*^((*t*)) in line 9, which implies that
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑算法在其预测中犯错误的迭代*t*，这意味着算法预测为1且结果为0，或者算法预测为0且结果为1。不失一般性，假设算法预测为1且结果为0。算法预测为1是因为第9行中*upweight*^((*t*))
    ≥ *downweight*^((*t*))，这意味着
- en: '![art](images/Art_P1359.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1359.jpg)'
- en: Each expert in *U* then has its weight multiplied by 1 – *γ*, and each expert
    in *D* has its weight unchanged. Thus, we have
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，*U*中的每个专家的权重都乘以1 - *γ*，*D*中的每个专家的权重保持不变。因此，我们有
- en: '![art](images/Art_P1360.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1360.jpg)'
- en: Therefore, for every iteration *t* in which the algorithm makes a mistake, we
    have
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于算法在每次迭代*t*中犯错误的情况，我们有
- en: '![art](images/Art_P1361.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1361.jpg)'
- en: In an iteration where the algorithm does not make a mistake, some of the weights
    decrease and some remain unchanged, so that we have
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在算法没有犯错的迭代中，一些权重减少，一些保持不变，因此我们有
- en: '![art](images/Art_P1362.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1362.jpg)'
- en: Since there are *m*^((*T*′)) mistakes made through iteration *T*′, and *W*(1)
    = *n*, we can repeatedly apply inequality (33.8) to iterations where the algorithm
    makes a mistake and inequality (33.9) to iterations where the algorithm does not
    make a mistake, obtaining
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 由于通过迭代*T*'产生了*m*^((*T*'))个错误，且*W*(1) = *n*，我们可以反复应用算法犯错的迭代中的不等式（33.8）和算法不犯错的迭代中的不等式（33.9），得到
- en: '![art](images/Art_P1363.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1363.jpg)'
- en: Because the function *W* is the sum of the weights and all weights are positive,
    its value exceeds any single weight. Therefore, using equation (33.6) we have,
    for any expert *E*[*i*] and for any iteration *T*′ ≤ *T*,
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 因为函数*W*是权重的总和，所有权重都是正的，其值超过任何单个权重。因此，使用方程（33.6），对于任何专家*E*[i]和任何迭代*T*' ≤ *T*，
- en: '![art](images/Art_P1364.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1364.jpg)'
- en: Combining inequalities (33.10) and (33.11) gives
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 结合不等式（33.10）和（33.11）得到
- en: '![art](images/Art_P1365.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1365.jpg)'
- en: Taking the natural logarithm of both sides yields
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 两边取自然对数得到
- en: '![art](images/Art_P1366.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1366.jpg)'
- en: We now use the Taylor series expansion to derive upper and lower bounds on the
    logarithmic factors in inequality (33.12). The Taylor series for ln(1+*x*) is
    given in equation (3.22) on page 67\. Substituting −*x* for *x*, we have that
    for 0 < *x* ≤ 1/2,
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用泰勒级数展开来推导不等式（33.12）中对数因子的上界和下界。ln(1+*x*)的泰勒级数在第67页的方程（3.22）中给出。将−*x*代入*x*，我们有对于0
    < *x* ≤ 1/2，
- en: '![art](images/Art_P1367.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1367.jpg)'
- en: Since each term on the right-hand side is negative, we can drop all terms except
    the first and obtain an upper bound of ln(1 – *x*) ≤ −*x*. Since 0 < *γ* ≤ 1/2,
    we have
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 由于右侧的每一项都是负的，我们可以舍弃除第一项之外的所有项，并得到ln(1 - *x*) ≤ - *x*的上界。由于0 < *γ* ≤ 1/2，我们有
- en: '![art](images/Art_P1368.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1368.jpg)'
- en: For the lower bound, Exercise 33.2-2 asks you to show that ln(1 – *x*) ≥ −*x*
    − *x*² when 0 < *x* ≤ 1/2, so that
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 对于下界，练习33.2-2要求您证明当0 < *x* ≤ 1/2时，ln(1 - *x*) ≥ - *x* - *x*²，因此
- en: '![art](images/Art_P1369.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1369.jpg)'
- en: Thus, we have
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有
- en: '![art](images/Art_P1370.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1370.jpg)'
- en: so that
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 以至于
- en: '![art](images/Art_P1371.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1371.jpg)'
- en: Subtracting ln *n* from both sides of inequality (33.16) and then multiplying
    both sides by −2/*γ* yields ![art](images/Art_P1372.jpg), thus proving the theorem.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 从不等式（33.16）两边减去ln *n*，然后将两边乘以-2/*γ*得到![艺术](images/Art_P1372.jpg)，从而证明了定理。
- en: ▪
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ▪
- en: Theorem 33.4 applies to any expert and any event *T*′ ≤ *T*. In particular,
    we can compare against the best expert after all events have occurred, producing
    the following corollary.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 定理33.4适用于任何专家和任何事件*T*' ≤ *T*。特别地，我们可以在所有事件发生后与最佳专家进行比较，得出以下推论。
- en: '***Corollary 33.5***'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '***推论33.5***'
- en: At the end of procedure WEIGHTED-MAJORITY, we have
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在WEIGHTED-MAJORITY过程结束时，我们有
- en: '![art](images/Art_P1373.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1373.jpg)'
- en: ▪
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ▪
- en: Let’s explore this bound. Assuming that ![art](images/Art_P1374.jpg), we can
    choose ![art](images/Art_P1375.jpg) and plug into inequality (33.17) to obtain
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探讨这个界限。假设![艺术](images/Art_P1374.jpg)，我们可以选择![艺术](images/Art_P1375.jpg)并将其代入不等式（33.17）中得到
- en: '![art](images/Art_P1376.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1376.jpg)'
- en: and so the number of errors is at most twice the number of errors made by the
    best expert plus a term that is often slower growing than *m**. Exercise 33.2-4
    shows that you can decrease the bound on the number of errors by a factor of 2
    by using randomization, which leads to much stronger bounds. In particular, the
    upper bound on regret (*m* – *m**) is reduced from (1 + 2*γ*)*m** + (2 ln *n*)/*γ*
    to an expected value of *ϵm** + (ln *n*)/*ϵ*, where both *γ* and *ϵ* are at most
    1/2\. Numerically, we can see that if*γ* = 1/2, WEIGHTED-MAJORITY makes at most
    3 times the number of errors as the best expert, plus 4 ln *n* errors. As another
    example, suppose that *T* = 1000 predictions are being made by *n* = 20 experts,
    and the best expert is correct 95% of the time, making 50 errors. Then WEIGHTED-MAJORITY
    makes at most 100(1+*γ*)+2 ln 20/*γ* errors. By choosing *γ* = 1/4, WEIGHTED-MAJORITY
    makes at most 149 errors, or a success rate of at least 85%.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，错误的数量最多是最佳专家犯的错误数量的两倍加上一个通常增长较慢的项*m*。练习33.2-4表明，通过使用随机化，可以将错误数量的上界减少一半，从(1
    + 2*γ*)*m* + (2 ln *n*)/*γ*减少到*ϵm* + (ln *n*)/*ϵ*的期望值，其中*γ*和*ϵ*最多为1/2。从数值上看，如果*γ*
    = 1/2，则WEIGHTED-MAJORITY最多比最佳专家多出3倍的错误，再加上4 ln *n*的错误。另一个例子，假设*n* = 20个专家进行了*T*
    = 1000次预测，最佳专家正确率为95%，犯了50个错误。那么WEIGHTED-MAJORITY最多会犯100(1+*γ*)+2 ln 20/*γ*个错误。通过选择*γ*
    = 1/4，WEIGHTED-MAJORITY最多会犯149个错误，或者至少85%的成功率。
- en: Multiplicative weights methods typically refer to a broader class of algorithms
    that includes WEIGHTED-MAJORITY. The outcomes and predictions need not be only
    0 or 1, but can be real numbers, and there can be a loss associated with a particular
    outcome and prediction. The weights can be updated by a multiplicative factor
    that depends on the loss, and the algorithm can, given a set of weights, treat
    them as a distribution on experts and use them to choose an expert to follow in
    each event. Even in these more general settings, bounds similar to Theorem 33.4
    hold.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 乘法权重方法通常指的是一类更广泛的算法，其中包括WEIGHTED-MAJORITY。结果和预测不一定只能是0或1，而可以是实数，并且特定结果和预测可能会有损失。权重可以根据损失更新乘法因子，算法可以在给定一组权重的情况下将它们视为专家的分布，并用它们在每个事件中选择要跟随的专家。即使在这些更一般的设置中，类似于定理33.4的界限仍然成立。
- en: '**Exercises**'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习**'
- en: '***33.2-1***'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '***33.2-1***'
- en: The proof of Lemma 33.3 assumes that some expert never makes a mistake. It is
    possible to generalize the algorithm and analysis to remove this assumption. The
    new algorithm begins in the same way. The set *S* might become empty at some point,
    however. If that ever happens, reset *S* to contain all the experts and continue
    the algorithm. Show that the number of mistakes that this algorithm makes is at
    most *m** ⌈1g *n*⌉.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 引理 33.3 的证明假设某位专家永远不会犯错。可以将算法和分析推广以消除这一假设。新算法以相同方式开始。然而，集合*S*可能在某个时刻变为空。如果发生这种情况，请将*S*重置为包含所有专家并继续算法。证明此算法所犯错误的数量最多为*m**
    ⌈1g *n*⌉。
- en: '***33.2-2***'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '***33.2-2***'
- en: Show that ln(1 – *x*) ≥ −*x* – *x*² when 0 < *x* ≤ 1/2\. (*Hint:* Start with
    equation (33.13), group all the terms after the first three, and use equation
    (A.7) on page 1142.)
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 证明当0 < *x* ≤ 1/2时，ln(1 – *x*) ≥ −*x* – *x*²。(*提示:* 从方程(33.13)开始，将第一个三个项之后的所有项分组，并在第1142页使用方程(A.7)。)
- en: '***33.2-3***'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '***33.2-3***'
- en: Consider a randomized variant of the algorithm given in the proof of Lemma 33.3,
    in which some expert never makes a mistake. At each step, choose an expert *E*[*i*]
    uniformly at random from the set *S* and then make the same predication as *E*[*i*].
    Show that the expected number of mistakes made by this algorithm is ⌈1g *n*⌉.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑引理33.3证明中给出的算法的随机化变体，其中某位专家永远不会犯错。在每一步中，从集合*S*中均匀随机选择一个专家*E*[*i*]，然后做与*E*[*i*]相同的预测。证明此算法所做的预期错误次数为⌈1g
    *n*⌉。
- en: '***33.2-4***'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '***33.2-4***'
- en: Consider a randomized version of WEIGHTED-MAJORITY. The algorithm is the same,
    except for the prediction step, which interprets the weights as a probability
    distribution over the experts and chooses an expert *E*[*i*] according to that
    distribution. It then chooses its prediction to be the same as the prediction
    made by expert *E*[*i*]. Show that, for any 0 < *ϵ* < 1/2, the expected number
    of mistakes made by this algorithm is at most (1 + *ϵ*)*m** + (ln *n*)/*ϵ*.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑WEIGHTED-MAJORITY的随机化版本。算法相同，只是在预测步骤中，将权重解释为专家之间的概率分布，并根据该分布选择一个专家*E*[*i*]。然后，选择其预测与专家*E*[*i*]所做的预测相同。证明，对于任意0
    < *ϵ* < 1/2，此算法所做的预期错误次数最多为(1 + *ϵ*)*m** + (ln *n*)/*ϵ*。
- en: '[**33.3    Gradient descent**](toc.xhtml#Rh1-196)'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '[**33.3    梯度下降**](toc.xhtml#Rh1-196)'
- en: Suppose that you have a set {*p*[1], *p*[2], …, *p*[*n*]} of points and you
    want to find the line that best fits these points. For any line ℓ, there is a
    distance *d*[*i*] between each point *p*[*i*] and the line. You want to find the
    line that minimizes some function *f*(*d*[1], …, *d*[*n*]). There are many possible
    choices for the definition of distance and for the function *f*. For example,
    the distance can be the projection distance to the line and the function can be
    the sum of the squares of the distances. This type of problem is common in data
    science and machine learning—the line is the hypothesis that best describes the
    data—where the particular definition of best is determined by the definition of
    distance and the objective *f*. If the definition of distance and the function
    *f* are linear, then we have a linear-programming problem, as discussed in [Chapter
    29](chapter029.xhtml). Although the linear-programming framework captures several
    important problems, many other problems, including various machine-learning problems,
    have objectives and constraints that are not necessarily linear. We need frameworks
    and algorithms to solve such problems.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一组{*p*[1], *p*[2], …, *p*[*n*]}点，并且想要找到最适合这些点的直线。对于任意直线ℓ，每个点*p*[*i*]与直线之间有一个距离*d*[i]。你想要找到最小化某个函数*f*(*d*[1],
    …, *d*[*n*])的直线。关于距离和函数*f*的定义有许多可能的选择。例如，距离可以是到直线的投影距离，函数可以是距离的平方和。这种问题在数据科学和机器学习中很常见——直线是最好描述数据的假设——其中最佳定义由距离的定义和目标*f*决定。如果距离的定义和函数*f*是线性的，那么我们有一个线性规划问题，如[第29章](chapter029.xhtml)中所讨论的。尽管线性规划框架涵盖了几个重要问题，但许多其他问题，包括各种机器学习问题，具有不一定是线性的目标和约束。我们需要框架和算法来解决这些问题。
- en: 'In this section, we consider the problem of optimizing a continuous function
    and discuss one of the most popular methods to do so: gradient descent. Gradient
    descent is a general method for finding a local minimum of a function *f* : ℝ^(*n*)
    → ℝ, where informally, a local minimum of a function *f* is a point **x** for
    which *f*(**x**) ≤ *f*(**x**′) for all **x**′ that are “near” **x**. When the
    function is convex, it can find a point near the ***global minimizer*** of *f*:
    an *n*-vector argument **x** = (*x*[1], *x*[2], …, *x*[*n*]) such that *f*(**x**)
    is minimum. For the intuitive idea behind gradient descent, imagine being in a
    landscape of hills and valleys, and wanting to get to a low point as quickly as
    possible. You survey the terrain and choose to move in the direction that takes
    you downhill the fastest from your current position. You move in that direction,
    but only for a short while, because as you proceed, the terrain changes and you
    might need to choose a different direction. So you stop, reevaluate the possible
    directions and move another short distance in the steepest downhill direction,
    which might differ from the direction of your previous movement. You continue
    this process until you reach a point from which all directions lead up. Such a
    point is a local minimum.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们考虑优化连续函数的问题，并讨论其中一种最流行的方法：梯度下降。梯度下降是一种寻找函数*f*：ℝ^(*n*) → ℝ的局部最小值的通用方法，非正式地说，函数*f*的局部最小值是一个点**x**，对于所有“接近”**x**的**x**′，都有*f*(**x**)
    ≤ *f*(**x**′)。当函数是凸的时，它可以找到一个接近*f*的***全局最小值***的点：一个*n*维向量**x**=(*x*[1], *x*[2],
    …, *x*[*n*])，使得*f*(**x**)最小。对于梯度下降背后的直观想法，想象自己处于山丘和山谷的景观中，想尽快到达低点。你调查地形并选择向下坡最快的方向移动。你沿着这个方向移动，但只是短暂地，因为随着你前进，地形会改变，你可能需要选择不同的方向。所以你停下来，重新评估可能的方向，并沿着最陡的下坡方向再移动一小段距离，这可能与你之前移动的方向不同。你继续这个过程，直到达到一个所有方向都指向上坡的点。这样的点是一个局部最小值。
- en: 'In order to make this informal procedure more formal, we need to define the
    gradient of a function, which in the analogy above is a measure of the steepness
    of the various directions. Given a function *f* : ℝ^(*n*) → ℝ, its ***gradient***
    ∇*f* is a function ∇*f* : ℝ^(*n*) → ℝ^(*n*) comprising *n* partial derivatives:
    ![art](images/Art_P1377.jpg). Analogous to the derivative of a function of a single
    variable, the gradient can be viewed as a direction in which the function value
    locally increases the fastest, and the rate of that increase. This view is informal;
    in order to make it formal we would have to define what local means and place
    certain conditions, such as continuity or existence of derivatives, on the function.
    Nevertheless, this view motivates the key step of gradient descent—move in the
    direction opposite to the gradient, by a distance influenced by the magnitude
    of the gradient.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这个非正式的过程更加正式，我们需要定义函数的梯度，这在上面的类比中是各个方向陡峭程度的度量。给定一个函数*f*：ℝ^(*n*) → ℝ，它的***梯度***∇*f*是一个函数∇*f*：ℝ^(*n*)
    → ℝ^(*n*)，包括*n*个偏导数：![art](images/Art_P1377.jpg)。类似于单变量函数的导数，梯度可以被视为函数值在局部增加最快的方向，以及增加的速率。这种观点是非正式的；为了使其正式化，我们需要定义局部的含义，并对函数施加某些条件，如连续性或导数的存在。尽管如此，这种观点激发了梯度下降的关键步骤——沿着梯度相反的方向移动，移动距离受梯度大小的影响。
- en: The general procedure of gradient descent proceeds in steps. You start at some
    initial point **x^((0))**, which is an *n*-vector. At each step *t*, you compute
    the value of the gradient of *f* at point **x^((*t*))**, that is, (∇*f*)(**x^((*t*))**),
    which is also an *n*-vector. You then move in the direction opposite to the gradient
    in each dimension at **x^((*t*))** to arrive at the next point **x^((*t*+1))**,
    which again is an *n*-vector. Because you moved in a monotonically decreasing
    direction in each dimension, you should have that *f*(**x^((*t*+1))**) ≤ *f*(**x^((*t*))**).
    Several details are needed to turn this idea into an actual algorithm. The two
    main details are that you need an initial point and that you need to decide how
    far to move in the direction of the negative gradient. You also need to understand
    when to stop and what you can conclude about the quality of the solution found.
    We will explore these issues further in this section, for both constrained minimization,
    where there are additional constraints on the points, and unconstrained minimization,
    where there are none.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降的一般过程是分步进行的。你从某个初始点**x^((0))**开始，这是一个*n*维向量。在每一步*t*，你计算在点**x^((*t*))**处的梯度值，即(∇*f*)(**x^((*t*))**)，这也是一个*n*维向量。然后你沿着每个维度在**x^((*t*))**处梯度的相反方向移动，到达下一个点**x^((*t*+1))**，这同样是一个*n*维向量。因为你在每个维度上沿着单调递减的方向移动，所以应该有*f*(**x^((*t*+1))**)
    ≤ *f*(**x^((*t*))**)。要将这个想法转化为实际算法，需要一些细节。两个主要细节是你需要一个初始点，以及你需要决定在负梯度方向上移动多远。你还需要了解何时停止以及对找到的解的质量可以得出什么结论。我们将在本节进一步探讨这些问题，无论是有额外约束的最小化，还是没有约束的最小化。
- en: '**Unconstrained gradient descent**'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '**无约束梯度下降**'
- en: 'In order to gain intuition, let’s consider unconstrained gradient descent in
    just one dimension, that is, when *f* is a function of a scalar *x*, so that *f*
    : ℝ → ℝ. In this case, the gradient ∇*f* of *f* is just *f*′(*x*), the derivative
    of *f* with respect to *x*. Consider the function *f* shown in blue in [Figure
    33.3](chapter033.xhtml#Fig_33-3), with minimizer *x** and starting point *x*^((0)).
    The gradient (derivative) *f*′(*x*^((0))), shown in orange, has a negative slope,
    so that a small step from *x*^((0)) in the direction of increasing *x* results
    in a point *x*′ for which *f*(*x*′) < *f*(*x*^((0))). Too large a step, however,
    results in a'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得直觉，让我们考虑在一维中进行无约束的梯度下降，也就是当*f*是标量*x*的函数时，即*f*：ℝ → ℝ。在这种情况下，*f*的梯度∇*f*就是*f*′(*x*)，即*f*关于*x*的导数。考虑在[图33.3](chapter033.xhtml#Fig_33-3)中以蓝色显示的函数*f*，具有最小值*x*和起始点*x*^((0))。梯度（导数）*f*′(*x*^((0)))，橙色显示，具有负斜率，因此从*x*^((0))向增加*x*的方向的小步长导致*f*(*x*′)
    < *f*(*x*^((0)))。然而，太大的步长会导致
- en: '![art](images/Art_P1378.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1378.jpg)'
- en: '**Figure 33.3** A function *f* : ℝ → ℝ, shown in blue. Its gradient at point
    *x*^((0)), in orange, has a negative slope, and so a small increase in *x* from
    *x*^((0)) to *x*′ results in *f*(*x*′) < *f*(*x*^((0))). Small increases in *x*
    from *x*^((0)) head toward ![art](images/Art_P1379.jpg), which gives a local minimum.
    Too large an increase in *x* can end up at *x*″, where *f*(*x*″) > *f*(*x*^((0))).
    Small steps starting from *x*^((0)) and going only in the direction of decreasing
    values of *f* cannot end up at the global minimizer *x**.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '**图33.3** 函数*f*：ℝ → ℝ，蓝色显示。在点*x*^((0))处的梯度，橙色显示，具有负斜率，因此从*x*^((0))到*x*′的微小增加导致*f*(*x*′)
    < *f*(*x*^((0)))。从*x*^((0))开始的小步长朝着![艺术](images/Art_P1379.jpg)前进，这给出了一个局部最小值。然而，从*x*^((0))开始只朝着*f*值减小的方向走小步长，无法到达全局最小值*x**。'
- en: point *x*″ for which *f*(*x*″) > *f*(*x*^((0))), so this is a bad idea. Restricting
    ourselves to small steps, where each one has *f*(*x*′) < *f*(*x*), eventually
    results in getting close to point ![art](images/Art_P1380.jpg), which gives a
    local minimum. By taking only small downhill steps, however, gradient descent
    has no chance to get to the global minimizer *x**, given the starting point *x*^((0)).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 点*x*″，使得*f*(*x*″) > *f*(*x*^((0)))，所以这是一个坏主意。限制自己走小步长，每一步都有*f*(*x*′) < *f*(*x*)，最终会接近点![艺术](images/Art_P1380.jpg)，这给出了一个局部最小值。然而，只走小的下坡步长，梯度下降无法到达全局最小值*x**，鉴于起始点*x*^((0))。
- en: We draw two observations from this simple example. First, gradient descent converges
    toward a local minimum, and not necessarily a global minimum. Second, the speed
    at which it converges and how it behaves are related to properties of the function,
    to the initial point, and to the step size of the algorithm.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从这个简单的例子中得出两个观察结果。首先，梯度下降收敛于局部最小值，而不一定是全局最小值。其次，它收敛的速度以及行为方式与函数的性质、初始点以及算法的步长大小有关。
- en: The procedure GRADIENT-DESCENT on the facing page takes as input a function
    *f*, an initial point **x^((0))** ∈ ℝ^(*n*), a fixed step-size multiplier *γ*
    > 0, and a number *T* > 0 of steps to take. Each iteration of the **for** loop
    of lines 2–4 performs a step by computing the *n*-dimensional gradient at point
    **x^((*t*))** and then moving distance *γ* in the opposite direction in the *n*-dimensional
    space. The complexity of computing the gradient depends on the function *f* and
    can sometimes be expensive. Line 3 sums the points visited. After the loop terminates,
    line 6 returns **x-avg**, the average of all the points visited except for the
    last one, **x^((*T*))**. It might seem more natural to return **x^((*T*))**, and
    in fact, in many circumstances, you might prefer to have the function return **x^((*T*))**.
    For the version we will analyze, however, we use **x-avg**.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 面向页面的梯度下降过程接受函数*f*、初始点**x^((0))** ∈ ℝ^(*n*)、固定步长乘数*γ* > 0和要执行的步数*T* > 0作为输入。行2-4的**for**循环的每次迭代都通过计算点**x^((*t*))**处的*n*维梯度，然后在*n*维空间中相反方向移动距离*γ*来执行一步。计算梯度的复杂度取决于函数*f*，有时可能很昂贵。第3行对访问的点求和。循环结束后，第6行返回**x-avg**，除了最后一个点**x^((*T*))**之外，所有访问的点的平均值。返回**x^((*T*))**可能更自然，实际上，在许多情况下，您可能更喜欢函数返回**x^((*T*))**。但是，对于我们将��分析的版本，我们使用**x-avg**。
- en: GRADIENT-DESCENT(*f*, **x^((0))**, *γ*, *T*)
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降(*f*, **x^((0))**, *γ*, *T*)
- en: '| 1 | **sum** = 0 | **//** *n*-dimensional vector, initially all 0 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 1 | **sum** = 0 | **//** *n*维向量，初始值为0 |'
- en: '| 2 | **for** *t* = 0 **to** *T* – 1 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 2 | **for** *t* = 0 **to** *T* – 1 |'
- en: '| 3 | **sum** = **sum** + **x^((*t*))** | **//** add each of *n* dimensions
    into **sum** |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 3 | **sum** = **sum** + **x^((*t*))** | **//** 将*n*个维度的每个值加入**sum**中 |'
- en: '| 4 | **x^((*t*+1)) = x^((*t*)) – γ · (∇*f*)(x^((*t*)))** | **//** (∇*f*)(**x^((*t*))),
    x^((*t*+1))** are *n*-dimensional |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 4 | **x^((*t*+1)) = x^((*t*)) – γ · (∇*f*)(x^((*t*)))** | **//** (∇*f*)(**x^((*t*))),
    x^((*t*+1))** 都是*n*维的 |'
- en: '| 5 | **x-avg** = **sum**/*T* | **//** divide each of *n* dimensions by *T*
    |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 5 | **x-avg** = **sum**/*T* | **//** 将*n*个维度的每个值除以*T* |'
- en: '| 6 | **return x-avg** |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 6 | **return x-avg** |'
- en: '[Figure 33.4](chapter033.xhtml#Fig_33-4) depicts how gradient descent ideally
    runs on a convex 1-dimensional function.^([1](#footnote_1)) We’ll define convexity
    more formally below, but the figure shows that each iteration moves in the direction
    opposite to the gradient, with the distance moved being proportional to the magnitude
    of the gradient. As the iterations proceed, the magnitude of the gradient decreases,
    and thus the distance moved along the horizontal axis decreases. After each iteration,
    the distance to the optimal point **x*** decreases. This ideal behavior is not
    guaranteed to occur in general, but the analysis in the remainder of this section
    formalizes when this behavior occurs and quantifies the number of iterations needed.
    Gradient descent does not always work, however. We have already seen that if the
    function is not convex, gradient descent can converge to a local, rather than
    global, minimum. We have also seen that if the step size is too large, GRADIENT-DESCENT
    can overshoot the minimum and wind up farther away. (It is also possible to overshoot
    the minimum and wind up closer to the optimum.)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '[图33.4](chapter033.xhtml#Fig_33-4)描述了梯度下降在凸一维函数上理想运行的方式。我们将在下面更正式地定义凸性，但图表明每次迭代都沿着梯度相反的方向移动，移动的距离与梯度的大小成比例。随着迭代的进行，梯度的大小减小，因此沿水平轴移动的距离减小。每次迭代后，到最优点**x***的距离减小。这种理想行为不保证在一般情况下发生，但本节其余部分的分析将形式化此行为发生的条件并量化所需的迭代次数。然而，梯度下降并不总是有效。我们已经看到，如果函数不是凸的，梯度下降可能会收敛到局部最小值而不是全局最小值。我们还看到，如果步长过大，梯度下降可能会超过最小值并最终偏离更远。（也可能超过最小值并最终更接近最优值。）'
- en: '**Analysis of unconstrained gradient descent for convex functions**'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '**对凸函数的无约束梯度下降的分析**'
- en: 'Our analysis of gradient descent focuses on convex functions. Inequality (C.29)
    on page 1194 defines a convex function of one variable, as shown in [Figure 33.5](chapter033.xhtml#Fig_33-5).
    We can extend that definition to a function *f* : ℝ^(*n*) → ℝ and say that *f*
    is ***convex*** if for all **x**, **y** ∈ ℝ^(*n*) and for all 0 ≤ λ ≤ 1, we have'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对梯度下降的分析集中在凸函数上。不等式（C.29）在第1194页定���了一维凸函数，如[图33.5](chapter033.xhtml#Fig_33-5)所示。我们可以将该定义扩展到函数*f*：ℝ^(*n*)
    → ℝ，并说*f*是***凸的***，如果对于所有**x**，**y** ∈ ℝ^(*n*)和所有0 ≤ λ ≤ 1，我们有
- en: '![art](images/Art_P1381.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1381.jpg)'
- en: (Inequalities (33.18) and (C.29) are the same, except for the dimensions of
    **x** and **y**.) We also assume that our convex functions are closed^([2](#footnote_2))
    and differentiable.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: （不等式（33.18）和（C.29）是相同的，除了**x**和**y**的维度不同。）我们还假设我们的凸函数是闭合的^([2](#footnote_2))并且可微的。
- en: '![art](images/Art_P1382.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1382.jpg)'
- en: '**Figure 33.4** An example of running gradient descent on a convex function
    *f* : ℝ → ℝ, shown in blue. Beginning at point **x^((0))**, each iteration moves
    in the direction opposite to the gradient, and the distance moved is proportional
    to the magnitude of the gradient. Orange lines represent the negative of the gradient
    at each point, scaled by the step size *γ*. As the iterations proceed, the magnitude
    of the gradient decreases, and the distance moved decreases correspondingly. After
    each iteration, the distance to the optimal point **x*** decreases.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '**图33.4** 在凸函数*f*：ℝ → ℝ上运行梯度下降的示例，显示为蓝色。从点**x^((0))**开始，每次迭代都沿着相反于梯度的方向移动，移动的距离与梯度的大小成比例。橙色线代表每个点处的负梯度，按步长*γ*缩放。随着迭代的进行，梯度的大小减小，因此沿水平轴移动的距离相应减小。每次迭代后，到最优点**x***的距离减小。'
- en: '![art](images/Art_P1383.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1383.jpg)'
- en: '**Figure 33.5** A convex function *f* : ℝ → ℝ, shown in blue, with local and
    global minimizer **x***. Because *f* is convex, *f*(*λ***x** + (1 – *λ*)**y**)
    ≤ *λf*(**x**) + (1 – *λ*)*f*(**y**) for any two values **x** and **y** and all
    0 ≤ *λ* ≤ 1, shown for a particular value of *λ*. Here, the orange line segment
    represents all values *λf*(**x**) + (1 – *λ*)*f*(**y**) for 0 ≤ *λ* ≤ 1, and it
    is above the blue line.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '**图33.5** 凸函数*f*：ℝ → ℝ，显示为蓝色，具有局部和全局最小值**x***。由于*f*是凸的，对于任意两个值**x**和**y**以及所有0
    ≤ *λ* ≤ 1，*f*（*λ***x** +（1 – *λ*）**y**）≤ *λf*（**x**）+（1 – *λ*）*f*（**y**），对于特定值的*λ*显示。在这里，橙色线段代表所有值*λf*（**x**）+（1
    – *λ*）*f*（**y**）对于0 ≤ *λ* ≤ 1，并且它在蓝线上方。'
- en: A convex function has the property that any local minimum is also a global minimum.
    To verify this property, consider inequality (33.18), and suppose for the purpose
    of contradiction that **x** is a local minimum but not a global minimum and **y**
    ≠ **x** is a global minimum, so *f*(**y**) < *f*(**x**). Then we have
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 凸函数具有这样的性质，即任何局部最小值也是全局最小值。要验证这一性质，考虑不等式（33.18），假设**x**是局部最小值但不是全局最小值，**y**
    ≠ **x**是全局最小值，因此*f*（**y**）<*f*（**x**）。然后我们有
- en: '| *f*(*λ***x** + (1 – *λ*)**y**) | ≤ | *λf*(**x**) + (1 – *λ*)*f*(**y**) |
    (by inequality (33.18)) |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| *f*（*λ***x** +（1 – *λ*）**y**）| ≤ | *λf*（**x**）+（1 – *λ*）*f*（**y**）|（根据不等式（33.18））'
- en: '|  | < | *λf*(**x**) + (1 – *λ*)*f*(**x**) |  |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '|  | < | *λf*（**x**）+（1 – *λ*）*f*（**x**）|  |'
- en: '|  | = | *f*(**x**). |  |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '|  | = | *f*（**x**）。|  |'
- en: Thus, letting approach 1, we see that there is another point near **x**, say
    **x**′, such that *f*(**x**′) < *f*(**x**), so **x** is not a local minimum.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们接近方法1，我们看到附近还有另一个点**x**，称为**x**′，使得*f*（**x**′）<*f*（**x**），因此**x**不是局部最小值。
- en: Convex functions have several useful properties. The first property, whose proof
    we leave as Exercise 33.3-1, says that a convex function always lies above its
    tangent hyperplane. In the context of gradient descent, angle brackets denote
    the notation for inner product defined on page 1219 rather than denoting a sequence.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 凸函数具有几个有用的性质。第一个性质，其证明我们留作练习33.3-1，说凸函数始终位于其切线超平面之上。在梯度下降的上下文中，尖括号表示内积的符号，而不是表示一个序列。
- en: '***Lemma 33.6***'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '***引理33.6***'
- en: 'For any convex differentiable function *f* : ℝ^(*n*) → ℝ and for all *x*, *y*
    ∈ ℝ^(*n*), we have ≤ *f*(**x**) ≤ *f*(**y**) + 〈(∇*f*)(**x**), **x** – **y**〉.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任意凸可微函数*f*：ℝ^(*n*) → ℝ和所有**x**，**y** ∈ ℝ^(*n*)，我们有≤ *f*（**x**）≤ *f*（**y**）+
    〈（∇*f*）（**x**），**x** – **y**〉。
- en: ▪
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ▪
- en: The second property, which Exercise 33.3-2 asks you to prove, is a repeated
    application of the definition of convexity in inequality (33.18).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个性质，练习 33.3-2 要求你证明，是对不等式 (33.18) 中凸性定义的重复应用。
- en: '***Lemma 33.7***'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '***引理 33.7***'
- en: 'For any convex function *f* : ℝ^(*n*) → ℝ, for any integer *T* ≥ 1, and for
    all **x^((0))**, …, **x^((*T*–1))** ∈ ℝ^(*n*), we have'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '对于任意凸函数 *f* : ℝ^(*n*) → ℝ，对于任意整数 *T* ≥ 1，以及所有 **x^((0))**, …, **x^((*T*–1))**
    ∈ ℝ^(*n*)，我们有'
- en: '![art](images/Art_P1384.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1384.jpg)'
- en: ▪
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ▪
- en: The left-hand side of inequality (33.19) is the value of *f* at the vector **x-avg**
    that GRADIENT-DESCENT returns.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 不等式 (33.19) 的左侧是梯度下降返回的 **x-avg** 处的 *f* 值。
- en: We now proceed to analyze GRADIENT-DESCENT. It might not return the exact global
    minimizer **x***. We use an error bound *ϵ*, and we want to choose *T* so that
    *f*(**x-avg**) – *f*(**x***) ≤ *ϵ* at termination. The value of *ϵ* depends on
    the number *T* of iterations and two additional values. First, since you expect
    it to be better to start close to the global minimizer, *ϵ* is a function of
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们继续分析梯度下降。它可能不会返回确切的全局最小值 **x***。我们使用一个误差界限 *ϵ*，并希望选择 *T*，使得终止时 *f*(**x-avg**)
    – *f*(**x***) ≤ *ϵ*。误差界限 *ϵ* 的值取决于迭代次数 *T* 和另外两个值。首先，因为你期望从全局最小值开始更接近，*ϵ* 是关于
- en: '![art](images/Art_P1385.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1385.jpg)'
- en: the euclidean norm (or distance, defined on page 1219) of the difference between
    **x^((0))** and **x***. The error bound *ϵ* is also a function of a quantity we
    call *L*, which is an upper bound on the magnitude ∥(∇*f*)(**x**)∥ of the gradient,
    so that
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得范数（或距离，在第 1219 页定义）的 **x^((0))** 和 **x*** 之间的差的函数。误差界限 *ϵ* 也是一个我们称为 *L*
    的量的函数，它是梯度 ∥(∇*f*)(**x**)∥ 的上界，使得
- en: '![art](images/Art_P1386.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1386.jpg)'
- en: where **x** ranges over all the points **x^((0))**, …, **x^((*T*–1))** whose
    gradients are computed by GRADIENT-DESCENT. Of course, we don’t know the values
    of *L* and *R*, but for now let’s assume that we do. We’ll discuss later how to
    remove these assumptions. The analysis of GRADIENT-DESCENT is summarized in the
    following theorem.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '**x** 取值于所有通过梯度下降计算的点 **x^((0))**, …, **x^((*T*–1))**。当然，我们不知道 *L* 和 *R* 的值，但现在让我们假设知道。我们将讨论如何消除这些假设。梯度下降的分析总结在以下定理中。'
- en: '***Theorem 33.8***'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '***定理 33.8***'
- en: Let **x*** ∈ ℝ^(*n*) be the minimizer of a convex function *f*, and suppose
    that an execution of GRADIENT-DESCENT(*f*, **x^((0))**, *γ*, *T*) returns **x-avg**,
    where ![art](images/Art_P1387.jpg) and *R* and *L* are defined in equations (33.20)
    and (33.21). Let ![art](images/Art_P1388.jpg). Then we have *f*(**x-avg**) – *f*(**x***)
    ≤ *ϵ*.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 设 **x*** ∈ ℝ^(*n*) 是凸函数 *f* 的最小化器，并假设梯度下降(*f*, **x^((0))**, *γ*, *T*) 的执行返回
    **x-avg**，其中 ![艺术](images/Art_P1387.jpg)，*R* 和 *L* 在方程式 (33.20) 和 (33.21) 中定义。设
    ![艺术](images/Art_P1388.jpg)。那么我们有 *f*(**x-avg**) – *f*(**x***) ≤ *ϵ*。
- en: ▪
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ▪
- en: We now prove this theorem. We do not give an absolute bound on how much progress
    each iteration makes. Instead, we use a potential function, as in [Section 16.3](chapter016.xhtml#Sec_16.3).
    Here, we define a potential Φ(*t*) after computing **x^((*t*))**, such that Φ(*t*)
    ≥ 0 for *t* = 0, …, *T*. We define the ***amortized progress*** in the iteration
    that computes **x^((*t*))** as
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们证明这个定理。我们不给出每次迭代的进度有多大的绝���界限。相反，我们使用一个势函数，就像 [第 16.3 节](chapter016.xhtml#Sec_16.3)
    中一样。在这里，我们定义在计算 **x^((*t*))** 后的势 Φ(*t*)，使得对于 *t* = 0, …, *T*，Φ(*t*) ≥ 0。我们定义计算
    **x^((*t*))** 的迭代中的 ***摊销进度*** 为
- en: '![art](images/Art_P1389.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1389.jpg)'
- en: Along with including the change in potential (Φ(*t* + 1) – Φ(*t*)), equation
    (33.22) also subtracts the minimum value *f*(**x***) because ultimately, you care
    not about the values *f*(**x^((*t*))**) but about how close they are to *f*(**x***).
    Suppose that we can show that *p*(*t*) ≤ *B* for some value *B* and *t* = 0, …,
    *T* – 1\. Then we can substitute for *p*(*t*) using equation (33.22), giving
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 除了包括势能变化（Φ(*t* + 1) – Φ(*t*)）之外，方程式 (33.22) 还减去最小值 *f*(**x***)，因为最终，你关心的不是 *f*(**x^((*t*))**)
    的值，而是它们与 *f*(**x***) 有多接近。假设我们可以证明对于某个值 *B* 和 *t* = 0, …, *T* – 1，*p*(*t*) ≤ *B*。那么我们可以使用方程式
    (33.22) 替换 *p*(*t*)，得到
- en: '![art](images/Art_P1390.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1390.jpg)'
- en: Summing inequality (33.23) over *t* = 0, …, *T* – 1 yields
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 将不等式 (33.23) 在 *t* = 0, …, *T* – 1 上求和得到
- en: '![art](images/Art_P1391.jpg)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1391.jpg)'
- en: Observing that we have a telescoping series on the right and regrouping terms,
    we have that
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 观察右侧的等差级数并重新排列项，我们有
- en: '![art](images/Art_P1392.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1392.jpg)'
- en: Dividing by *T* and dropping the positive term Φ(*T*) gives
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 除以 *T* 并去掉正项 Φ(*T*)，得到
- en: '![art](images/Art_P1393.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1393.jpg)'
- en: and thus we have
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 因此我们有
- en: '![art](images/Art_P1394.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1394.jpg)'
- en: In other words, if we can show that *p*(*t*) ≤ *B* for some value *B* and choose
    a potential function where Φ(0) is not too large, then inequality (33.25) tells
    us how close the function value *f*(**x-avg**) is to the function value *f*(**x***)
    after *T* iterations. That is, we can set the error bound *ϵ* to *B* + Φ(0)/*T*.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，如果我们可以证明对于某个值 *B*，*p*(*t*) ≤ *B*，并选择一个势函数，其中 Φ(0) 不是太大，那么不等式 (33.25) 告诉我们在
    *T* 次迭代后，函数值 *f*(**x-avg**) 与函数值 *f*(**x***) 有多接近。也就是说，我们可以将误差界限 *ϵ* 设为 *B* +
    Φ(0)/*T*。
- en: In order to bound the amortized progress, we need to come up with a concrete
    potential function. Define the potential function Φ(*t*) by
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 为了限制摊销进度，我们需要提出一个具体的势函数。定义势函数 Φ(*t*) 为
- en: '![art](images/Art_P1395.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1395.jpg)'
- en: that is, the potential function is proportional to the square of the distance
    between the current point and the minimizer **x***. With this potential function
    in hand, the next lemma provides a bound on the amortized progress made in any
    iteration of GRADIENT-DESCENT.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，势函数与当前点与最小值 **x*** 之间距离的平方成比例。有了这个势函数，下一个引理提供了梯度下降的任何迭代中摊销进度的界限。
- en: '***Lemma 33.9***'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '***引理 33.9***'
- en: Let **x*** ∈ ℝ^(*n*) be the minimizer of a convex function *f*, and consider
    an execution of GRADIENT-DESCENT(*f*, **x^((0))**, *γ*, *T*). Then for each point
    **x^((*t*))** computed by the procedure, we have that
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 让**x*** ∈ ℝ^(*n*)是凸函数*f*的最小化器，并考虑梯度下降(*f*, **x^((0))**, *γ*, *T*)的执行。那么对于过程计算得到的每个点**x^((*t*))**，我们有
- en: '![art](images/Art_P1396.jpg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1396.jpg)'
- en: '***Proof***   We first bound the potential change Φ(*t* + 1) – Φ(*t*). Using
    the definition of Φ(*t*) from equation (33.26), we have'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '***证明*** 我们首先限制潜在变化Φ(*t* + 1) – Φ(*t*)。使用方程（33.26）中Φ(*t*)的定义，我们有'
- en: '![art](images/Art_P1397.jpg)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1397.jpg)'
- en: From line 4 in GRADIENT-DESCENT, we know that
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 从梯度下降的第4行，我们知道
- en: '![art](images/Art_P1398.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1398.jpg)'
- en: and so we would like to rewrite equation (33.27) to have **x^((*t*+1))** – **x^((*t*))**
    terms. As Exercise 33.3-3 asks you to prove, for any two vectors **a**, **b**
    ∈ ℝ^(*n*), we have
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们希望重新编写方程（33.27）以包含**x^((*t*+1))** – **x^((*t*))**项。正如练习33.3-3要求您证明的那样，对于任意两个向量**a**，**b**
    ∈ ℝ^(*n*)，我们有
- en: '![art](images/Art_P1399.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1399.jpg)'
- en: Letting **a** = **x^((*t*))** – **x*** and **b** = **x^((*t*+1))** – **x^((*t*))**,
    we can write the right-hand side of equation (33.27) as ![art](images/Art_P1400.jpg).
    Then we can express the potential change as
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 让**a** = **x^((*t*))** – **x***，**b** = **x^((*t*+1))** – **x^((*t*))**，我们可以将方程（33.27）的右侧写成![艺术](images/Art_P1400.jpg)。然后我们可以将潜在变化表示为
- en: '![art](images/Art_P1401.jpg)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1401.jpg)'
- en: and thus we have
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 因此我们有
- en: '![art](images/Art_P1402.jpg)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1402.jpg)'
- en: We can now proceed to bound *p*(*t*). By the bound on the potential change from
    inequality (33.31), and using the definition of *L* (inequality (33.21)), we have
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以继续限制*p*(*t*)。根据不等式（33.31）中潜在变化的限制，并使用*L*的定义（不等式（33.21）），我们有
- en: '![art](images/Art_P1403.jpg)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1403.jpg)'
- en: ▪
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: ▪
- en: sult in the following theorem
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 导致以下定理
- en: Having bounded the amortized progress in one step, we now analyze the entire
    GRADIENT-DESCENT procedure, completing the proof of Theorem 33.8.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个步骤中限制了摊销进展后，我们现在分析整个梯度下降过程，完成了定理33.8的证明。
- en: '***Proof of Theorem 33.8*** Inequality (33.25) tells us that if we have an
    upper bound of *B* for *p*(*t*), then we also have the bound *f*(**x-avg**) –
    *f*(**x***) ≤ *B* + Φ(0)/*T*. By equations (33.20) and (33.26), we have that Φ(0)
    = *R*²/(2*γ*). Lemma 33.9 gives us the upper bound of *B* = *γL*²/2, and so we
    have'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '***定理33.8的证明*** 不等式（33.25）告诉我们，如果我们对*p*(*t*)有一个上界*B*，那么我们也有上界*f*(**x-avg**)
    – *f*(**x***) ≤ *B* + Φ(0)/*T*。根据方程（33.20）和（33.26），我们有Φ(0) = *R*²/(2*γ*)。引理33.9给出了*B*的上界为*γL*²/2，因此我们有'
- en: '![art](images/Art_P1404.jpg)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1404.jpg)'
- en: Our choice of ![art](images/Art_P1405.jpg) in the statement of Theorem 33.8
    balances the two terms, and we obtain
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在定理33.8的陈述中选择了![艺术](images/Art_P1405.jpg)来平衡这两项，我们得到
- en: '![art](images/Art_P1406.jpg)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1406.jpg)'
- en: Since we chose ![art](images/Art_P1407.jpg) in the theorem statement, the proof
    is complete.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在定理33.8的陈述中选择了![艺术](images/Art_P1407.jpg)，证明完成。
- en: ▪
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: ▪
- en: Continuing under the assumption that we know *R* (from equation (33.20)) and
    *L* (from inequality (33.21)), we can think of the analysis in a slightly different
    way. We can presume that we have a target accuracy *ϵ* and then compute the number
    of iterations needed. That is, we can solve ![art](images/Art_P1408.jpg) for *T*,
    obtaining *T* = *R*²*L*²/*ϵ*². The number of iterations thus depends on the square
    of *R* and *L* and, most importantly, on 1/*ϵ*². (The definition of *L* from inequality
    (33.21) depends on *T*, but we may know an upper bound on *L* that doesn’t depend
    on the particular value of *T*.) Thus, if you want to halve your error bound,
    you need to run four times as many iterations.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在假设我们知道*R*（来自方程（33.20））和*L*（来自不等式（33.21））的情况下，我们可以以稍微不同的方式思考分析。我们可以假设我们有一个目标精度*ϵ*，然后计算所需的迭代次数。也就是说，我们可以解方程![艺术](images/Art_P1408.jpg)得到*T*
    = *R*²*L*²/*ϵ*²。迭代次数取决于*R*和*L*的平方，最重要的是取决于1/*ϵ*²。（不等式（33.21）中*L*的定义取决于*T*，但我们可能知道一个不依赖于*T*特定值的*L*上界。）因此，如果你想将误差边界减半，你需要运行四倍的迭代次数。
- en: It is quite possible that we don’t really know *R* and *L*, since you’d need
    to know **x*** in order to know *R* (since *R* = ∥**x^((0))** – **x***∥), and
    you might not have an explicit upper bound on the gradient, which would provide
    *L*. You can, however, interpret the analysis of gradient descent as a proof that
    there is some step size for which the procedure makes progress toward the minimum.
    You can then compute a step size for which *f*(**x^((*t*))**) – *f*(**x^((*t*+1))**)
    is large enough. In fact, not having a fixed step size multiplier can actually
    help in practice, as you are free to use any step size *s* that achieves sufficient
    decrease in the value of *f*. You can search for a step size that achieves a large
    decrease via a binary-search-like routine, which is often called ***line search***.
    For a given function *f* and step size *s*, define the function *g*(**x^((*t*))**,
    *s*) = *f*(**x^((*t*))**) – *s*(∇*f*)(**x^((*t*))**). Start with a small step
    size *s* for which *g*(**x^((*t*))**, *s*) ≤ *f*(**x^((*t*))**). Then repeatedly
    double *s* until *g*(**x^((*t*))**, 2*s*) ≥ *g*(**x^((*t*))**, *s*), and then
    perform a binary search in the interval [*s*, 2*s*]. This procedure can produce
    a step size that achieves a significant decrease in the objective function. In
    other circumstances, however, you may know good upper bounds on *R* and *L*, typically
    from problem-specific information, which can suffice.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 很可能我们实际上并不知道*R*和*L*，因为你需要知道**x***才能知道*R*（因为*R* = ∥**x^((0))** – **x***∥），而且你可能没有梯度的明确上界，这将提供*L*。然而，你可以将梯度下降的分析解释为证明存在某个步长使得该过程朝着最小值取得进展。然后你可以计算一个步长，使得*f*(**x^((*t*))**)
    – *f*(**x^((*t*+1))**)足够大。事实上，没有固定的步长乘数实际上在实践中有所帮助，因为你可以自由选择任何能够实现足够减少*f*值的步长*s*。你可以搜索一个能够实现大幅减少目标函数值的步长，通过类似二分搜索的例程，通常称为***线搜索***。对于给定的函数*f*和步长*s*，定义函数*g*(**x^((*t*))**,
    *s*) = *f*(**x^((*t*))**) – *s*(∇*f*)(**x^((*t*))**). 从一个小的步长*s*开始，使得*g*(**x^((*t*))**,
    *s*) ≤ *f*(**x^((*t*))**)。然后重复将*s*加倍，直到*g*(**x^((*t*))**, 2*s*) ≥ *g*(**x^((*t*))**,
    *s*)，然后在区间[*s*, 2*s*]上执行二分搜索。这个过程可以产生一个能够显著减少目标函数的步长。然而，在其他情况下，你可能知道*R*和*L*的良好上界，通常来自问题特定信息，这可能足够。
- en: The dominant computational step in each iteration of the **for** loop of lines
    2–4 is computing the gradient. The complexity of computing and evaluating a gradient
    varies widely, depending on the application at hand. We’ll discuss several applications
    later.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2-4行的**for**循环的每次迭代中，计算梯度是主要的计算步骤。根据具体应用的不同，计算和评估梯度的复杂性差异很大。我们稍后会讨论几个应用。
- en: '**Constrained gradient descent**'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '**约束梯度下降**'
- en: 'We can adapt gradient descent for constrained minimization to minimize a closed
    convex function *f*(**x**), subject to the additional requirement that **x** ∈
    *K*, where *K* is a closed convex body. A ***body*** *K* ⊆ ℝ^(*n*) is ***convex***
    if for all **x**, **y** ∈ *K*, the convex combination *λ***x**+(1–*λ*)**y** ∈
    *K* for all 0 ≤ *λ* ≤ 1\. A ***closed*** convex body contains its limit points.
    Somewhat surprisingly, restricting to the constrained problem does not significantly
    increase the number of iterations of gradient descent. The idea is that you run
    the same algorithm, but in each iteration, check whether the current point **x^((*t*))**
    is still within the convex body *K*. If it is not, just move to the closest point
    in *K*. Moving to the closest point is known as ***projection***. We formally
    define the projection ∏[*K*](**x**) of a point **x** in *n* dimensions onto a
    convex body *K* as the point **y** ∈ *K* such that ∥**x** – **y**∥ = min {∥**x**
    – **z**∥ : *z* ∈ *K*}. If we have **x** ∈ *K*, then ∏[*K*](**x**) = **x**.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '我们可以将梯度下降调整为约束最小化，以最小化一个闭合凸函数*f*(**x**)，并且额外要求**x** ∈ *K*，其中*K*是一个闭合凸体。一个***体***
    *K* ⊆ ℝ^(*n*) 是***凸的***，如果对于所有**x**，**y** ∈ *K*，凸组合*λ***x**+(1–*λ*)**y** ∈ *K*对于所有0
    ≤ *λ* ≤ 1。一个***闭合的***凸体包含其极限点。有点令人惊讶的是，限制到约束问题并没有显著增加梯度下降的迭代次数。思路��你运行相同的算法，但在每次迭代中，检查当前点**x^((*t*))**是否仍在凸体*K*内。如果不在，只需移动到*K*中最接近的点。移动到最接近点被称为***投影***。我们正式定义了一个点**x**在*n*维空间中到凸体*K*的投影∏[*K*](**x**)为点**y**
    ∈ *K*，使得∥**x** – **y**∥ = min {∥**x** – **z**∥ : *z* ∈ *K*}。如果我们有**x** ∈ *K*，那么∏[*K*](**x**)
    = **x**。'
- en: This one change yields the procedure GRADIENT-DESCENT-CONSTRAINED, in which
    line 4 of GRADIENT-DESCENT is replaced by two lines. It assumes that **x^((0))**
    ∈ *K*. Line 4 of GRADIENT-DESCENT-CONSTRAINED moves in the direction of the negative
    gradient, and line 5 projects back onto *K*. The lemma that follows helps to show
    that when **x*** ∈ *K*, if the projection step in line 5 moves from a point outside
    of *K* to a point in *K*, it cannot be moving away from **x***.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 这一变化导致了**梯度下降约束**的过程，其中梯度下降的第4行被替换为两行。它假设**x^((0))** ∈ *K*。梯度下降约束的第4行沿着负梯度方向移动，第5行投影回*K*。接下来的引理有助于表明，当**x***
    ∈ *K*时，如果第5行的投影步骤从*K*外的点移动到*K*内的点，则它不会远离**x***。
- en: GRADIENT-DESCENT-CONSTRAINED(*f*, **x^((0))**, *γ*, *T*, *K*)
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '**梯度下降约束**(*f*, **x^((0))**, *γ*, *T*, *K*)'
- en: '| 1 | **sum** = 0 | **//** *n*-dimensional vector, initially all 0 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| 1 | **sum** = 0 | **//** *n*维向量，最初全为0 |'
- en: '| 2 | **for** *t* = 0 **to** *T* – 1 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| 2 | **对于** *t* = 0 **到** *T* – 1 |'
- en: '| 3 | **sum** = **sum** + **x^((*t*))** | **//** add each of *n* dimensions
    into **sum** |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 3 | **sum** = **sum** + **x^((*t*))** | **//** 将*n*维中的每个维度加入**sum** |'
- en: '| 4 | **x′**^((*t*+1)) = **x^((*t*))** – *γ* · (∇*f*)(**x^((*t*))**) | **//**
    (∇*f*)(**x^((*t*))**), **x′^((*t*+1))** are *n*-dimensional |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| 4 | **x′**^((*t*+1)) = **x^((*t*))** – *γ* · (∇*f*)(**x^((*t*))**) | **//**
    (∇*f*)(**x^((*t*))**), **x′^((*t*+1))** 是 *n* 维的 |'
- en: '| 5 | **x^((*t*+1))** = ∏[*K*](**x^((*t*+1))**) | **//** project onto *K* |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| 5 | **x^((*t*+1))** = ∏[*K*](**x^((*t*+1))**) | **//** 投影到*K* |'
- en: '| 6 | **x-avg** = **sum**/*T* | **//** divide each of *n* dimensions by *T*
    |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 6 | **x-avg** = **sum**/*T* | **//** 将*n*维中的每个维度除以*T* |'
- en: '| 7 | **return x-avg** |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 7 | **返回 x-avg** |'
- en: '***Lemma 33.10***'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '***引理 33.10***'
- en: Consider a convex body *K* ⊆ ℝ^(*n*) and points **a** ∈ *K* and **b**′ ∈ ℝ^(*n*).
    Let **b** = ∏[*K*](**b**′). Then ∥**b** – **a**∥² ≤ ∥**b**′ – **a**∥².
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个凸体*K* ⊆ ℝ^(*n*)，点**a** ∈ *K*和**b**′ ∈ ℝ^(*n*)。设**b** = ∏[*K*](**b**′)。那么∥**b**
    – **a**∥² ≤ ∥**b**′ – **a**∥²。
- en: '***Proof***   If **b**′ ∈ *K*, then **b** = **b**′ and the claim is true. Otherwise,
    **b**′ ≠ **b**, and as [Figure 33.6](chapter033.xhtml#Fig_33-6) shows, we can
    extend the line segment between **b** and **b**′ to a line ℓ. Let **c** be the
    projection of **a** onto ℓ. Point **c** may or may not be in *K*, and if **a**
    is on the boundary of *K*, then **c** could coincide with **b**. If **c** coincides
    with **b** (part (c) of the figure), then **abb**′ is a right triangle, and so
    ∥**b** – **a**∥² ≤ ∥**b**′ – **a**∥². If **c** does not coincide with **b** (parts
    (a) and (b) of the figure), then because of convexity, the angle ∠**abb**′ must
    be obtuse. Because angle ∠**abb**′ is obtuse, **b** lies between **c** and **b**′
    on ℓ. Furthermore, because **c** is the projection of **a** onto line ℓ, **acb**
    and **acb**′ must be right triangles. By the Pythagorean theorem, we have that
    ∥b′ – **a**∥² = ∥**a** – **c**∥²+∥**c** – **b**′∥² and ∥**b** – **a**∥² = ∥**a**
    – **c**∥²+∥**c** – **b**∥². Subtracting these two equations gives ∥**b**′ – **a**∥²
    – ∥**b** – **a**∥² = ∥**c** – **b**′∥² – ∥**c** – **b**∥². Because **b** is between
    **c** and **b**′, we must have ∥**c** – **b**′∥² ≥ ∥**c** – **b**∥², and thus
    ∥**b**′ – **a**∥² – ∥**b** – **a**∥² ≥ 0\. The lemma follows.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '***证明***   如果**b**′ ∈ *K*，那么**b** = **b**′，命题���立。否则，**b**′ ≠ **b**，如[图33.6](chapter033.xhtml#Fig_33-6)所示，我们可以将**b**和**b**′之间的线段延伸为一条直线ℓ。设**c**为**a**在ℓ上的投影。点**c**可能在*K*内，也可能不在*K*内，如果**a**在*K*的边界上，则**c**可能与**b**重合。如果**c**与**b**重合（图中的部分(c)），那么**abb**′是一个直角三角形，因此∥**b**
    – **a**∥² ≤ ∥**b**′ – **a**∥²。如果**c**不与**b**重合（图中的部分(a)和(b)），则由���凸性，角∠**abb**′必须是钝角。由于角∠**abb**′是钝角，**b**在ℓ上位于**c**和**b**′之间。此外，因为**c**是**a**在直线ℓ上的投影，**acb**和**acb**′必须是直角三角形。根据毕达哥拉斯定理，我们有∥b′
    – **a**∥² = ∥**a** – **c**∥²+∥**c** – **b**′∥²和∥**b** – **a**∥² = ∥**a** – **c**∥²+∥**c**
    – **b**∥²。将这两个方程相减得到∥**b**′ – **a**∥² – ∥**b** – **a**∥² = ∥**c** – **b**′∥² –
    ∥**c** – **b**∥²。因为**b**在**c**和**b**′之间，我们必须有∥**c** – **b**′∥² ≥ ∥**c** – **b**∥²，因此∥**b**′
    – **a**∥² – ∥**b** – **a**∥² ≥ 0。引理得证。'
- en: '![art](images/Art_P1409.jpg)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1409.jpg)'
- en: '**Figure 33.6** Projecting a point **b**′ outside the convex body *K* to the
    closest point **b** = ∏[*K*](**b**′) in *K*. Line ℓ is the line containing **b**
    and **b**′, and point **c** is the projection of **a** onto ℓ. **(a)** When **c**
    is in *K*. **(b)** When **c** is not in *K*. **(c)** When **a** is on the boundary
    of *K* and **c** coincides with **b**.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '**图33.6** 将点**b**′投影到凸体*K*外最接近的点**b** = ∏[*K*](**b**′)。线ℓ是包含**b**和**b**′的直线，点**c**是**a**在ℓ上的投影。**(a)**
    当**c**在*K*内时。**(b)** 当**c**不在*K*内时。**(c)** 当**a**在*K*的边界上且**c**与**b**重合时。'
- en: ▪
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: ▪
- en: 'We can now repeat the entire proof for the unconstrained case and obtain the
    same bounds. Lemma 33.10 with **a** = **x***, **b** = **x^((*t*+1))**, and **b**′
    = **x′^((*t*+1))** tells us that ∥**x^((*t*+1))**–**x***∥² ≤ ∥**x′^((*t*+1))**–**x***∥².
    We can therefore derive an upper bound that matches inequality (33.31). We continue
    to define Φ(*t*) as in equation (33.26), but noting that **x^((*t*+1))**, computed
    in line 5 of GRADIENT-DESCENT-CONSTRAINED, has a different meaning here from in
    inequality (33.31):'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以针对无约束情况重复整个证明，并获得相同的界限。引理33.10中，取**a** = **x***，**b** = **x^((*t*+1))**，以及**b**′
    = **x′^((*t*+1))**告诉我们∥**x^((*t*+1))**–**x***∥² ≤ ∥**x′^((*t*+1))**–**x***∥²。因此，我们可以得出一个上界，与不等式(33.31)相匹配。我们继续定义Φ(*t*)如方程(33.26)中所示，但要注意，GRADIENT-DESCENT-CONSTRAINED中第5行计算的**x^((*t*+1))**在这里与不等式(33.31)中的含义不同：
- en: '![art](images/Art_P1410.jpg)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![艺术](images/Art_P1410.jpg)'
- en: With the same upper bound on the change in the potential function as in equation
    (33.30), the entire proof of Lemma 33.9 can proceed as before. We can therefore
    conclude that the procedure GRADIENT-DESCENT-CONSTRAINED has the same asymptotic
    complexity as GRADIENT-DESCENT. We summarize this result in the following theorem.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在潜在函数变化的上界与方程(33.30)中相同的情况下，引理33.9的整个证明可以像以前一样进行。因此，我们可以得出结论，GRADIENT-DESCENT-CONSTRAINED过程的渐近复杂度与GRADIENT-DESCENT相同。我们在以下定理中总结这一结果。
- en: '***Theorem 33.11***'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '***定理33.11***'
- en: Let *K* ⊆ ℝ^(*n*) be a convex body, **x*** ∈ ℝ^(*n*) be the minimizer of a convex
    function *f* over *K*, and ![art](images/Art_P1411.jpg), where *R* and *L* are
    defined in equations (33.20) and (33.21). Suppose that the vector **x-avg** is
    returned by an execution of GRADIENT-DESCENT-CONSTRAINED(*f*, **x^((0))**, *γ*,
    *T*, *K*). Let ![art](images/Art_P1412.jpg). Then we have *f*(**x-avg**) – *f*(**x***)
    ≤ *ϵ*.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 设*K* ⊆ ℝ^(*n*)为一个凸体，**x*** ∈ ℝ^(*n*)为凸函数*f*在*K*上的最小化器，且![艺术](images/Art_P1411.jpg)，其中*R*和*L*在方程(33.20)和(33.21)中定义。假设向量**x-avg**是通过执行GRADIENT-DESCENT-CONSTRAINED(*f*,
    **x^((0))**, *γ*, *T*, *K*)返回的。设![艺术](images/Art_P1412.jpg)。那么我们有*f*(**x-avg**)
    – *f*(**x***) ≤ *ϵ*。
- en: ▪
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: ▪
- en: '**Applications of gradient descent**'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '**梯度下降的应用**'
- en: 'Gradient descent has many applications to minimizing functions and is widely
    used in optimization and machine learning. Here we sketch how it can be used to
    solve linear systems. Then we discuss an application to machine learning: prediction
    using linear regression.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降在最小化函数方面有许多应用，并广泛用于优化和机器学习。在这里，我们概述了如何将其用于解决线性系统。然后我们讨论了一个机器学习的应用：使用线性回归进行预测。
- en: In [Chapter 28](chapter028.xhtml), we saw how to use Gaussian elimination to
    solve a system of linear equations *A***x** = **b**, thereby computing **x** =
    *A*^(−1)**b**. If *A* is an *n* × *n* matrix and **b** is a length-*n* vector,
    then the running time of Gaussian elimination is Θ(*n*³), which for large matrices
    might be prohibitively expensive. If an approximate solution is acceptable, however,
    you can use gradient descent.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第28章](chapter028.xhtml)中，我们看到如何使用高斯消元法解决线性方程组*A***x** = **b**，从而计算**x** =
    *A*^(−1)**b**。如果*A*是一个*n* × *n*矩阵，**b**是一个长度为*n*的向量，则高斯消元法的运行时间为Θ(*n*³)，对于大矩阵来说可能过于昂贵。然而，如果可以接受近似解，可以使用梯度下降法。
- en: First, let’s see how to use gradient descent as a roundabout—and admittedly
    inefficient—way to solve for *x* in the scalar equation *ax* = *b*, where *a*,
    *x*, *b* ∈ ℝ. This equation is equivalent to *ax* – *b* = 0\. If *ax* – *b* is
    the derivative of a convex function *f*(*x*), then *ax* – *b* = 0 for the value
    of *x* that minimizes *f*(*x*). Given *f*(*x*), gradient descent can then determine
    this minimizer. Of course, *f*(*x*) is just the integral of *ax* – *b*, that is,
    ![art](images/Art_P1413.jpg), which is convex if *a* ≥ 0\. Therefore, one way
    to solve *ax* = *b* for *a* ≥ 0 is to find the minimizer for ![art](images/Art_P1414.jpg)
    via gradient descent.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看如何使用梯度下降作为解决标量方程*ax* = *b*的迂回方式——诚然是低效的，其中*a*、*x*、*b* ∈ ℝ。这个方程等价于*ax*
    – *b* = 0。如果*ax* – *b*是凸函数*f*(*x*)的导数，那么*ax* – *b* = 0对于最小化*f*(*x*)的*x*值成立。给定*f*(*x*)，梯度下降可以确定这个最小值。当然，*f*(*x*)只是*ax*
    – *b*的积分，即![art](images/Art_P1413.jpg)，如果*a* ≥ 0，那么它是凸的。因此，解决*a*x = b*，其中*a* ≥
    0的一种方法是通过梯度下降找到![art](images/Art_P1414.jpg)的最小值。
- en: We now generalize this idea to higher dimensions, where using gradient descent
    may actually lead to a faster algorithm. One *n*-dimensional analog is the function
    ![art](images/Art_P1415.jpg), where *A* is an *n* × *n* matrix. The gradient of
    *f* with respect to **x** is the function *A***x** – **b**. To find the value
    of **x** that minimizes *f*, we set the gradient of *f* to 0 and solve for **x**.
    Solving *A***x**–**b** = 0 for **x**, we obtain **x** = *A*^(−1)**b**, Thus, minimizing
    *f*(**x**) is equivalent to solving *A***x** = **b**. If *f*(**x**) is convex,
    then gradient descent can approximately compute this minimum.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将这个想法推广到更高维度，使用梯度下降实际上��能会导致更快的算法。一个*n*维的类似函数是![art](images/Art_P1415.jpg)，其中*A*是一个*n*
    × *n*矩阵。关于**x**的*f*的梯度是函数*A***x** – **b**。为了找到最小化*f*的**x**的值，我们将*f*的梯度设为0并解出**x**。解方程*A***x**–**b**
    = 0得到**x** = *A*^(−1)**b**，因此，最小化*f*(**x**)等价于解方程*A***x** = **b**。如果*f*(**x**)是凸的，那么梯度下降可以近似计算出这个最小值。
- en: 'A 1-dimensional function is convex when its second derivative is positive.
    The equivalent definition for a multidimensional function is that it is convex
    when its Hessian matrix is positive-semidefinite (see page 1222 for a definition),
    where the ***Hessian matrix*** (∇²*f*)(**x**) of a function *f*(**x**) is the
    matrix in which entry (*i*, *j*) is the partial derivative of *f* with respect
    to *i* and *j*:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 一维函数的二阶导数为正时是凸的。多维函数的等价定义是当其Hessian矩阵为半正定时是凸的（有关定义，请参见第1222页），其中函数*f*(*x*)的***Hessian矩阵***
    (∇²*f*)(**x**)是矩阵，其中条目(*i*, *j*)是*f*关于*i*和*j*的偏导数：
- en: '![art](images/Art_P1416.jpg)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![art](images/Art_P1416.jpg)'
- en: Analogous to the 1-dimensional case, the Hessian of *f* is just *A*, and so
    if *A* is a positive-semidefinite matrix, then we can use gradient descent to
    find a point **x** where *A***x** ≈ **b**. If *R* and *L* are not too large, then
    this method is faster than using Gaussian elimination.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于一维情况，*f*的Hessian矩阵就是*A*，因此如果*A*是半正定矩阵，那么我们可以使用梯度下降找到一个点**x**，使得*A***x** ≈
    **b**。如果*R*和*L*不太大，那么这种方法比使用高斯消元法更快。
- en: '**Gradient descent in machine learning**'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习中的梯度下降**'
- en: 'As a concrete example of supervised learning for prediction, suppose that you
    want to predict whether a patient will develop heart disease. For each of *m*
    patients, you have *n* different attributes. For example, you might have *n* =
    4 and the four pieces of data are age, height, blood pressure, and number of close
    family members with heart disease. Denote the data for patient *i* as a vector
    **x^((*i*))** ∈ ℝ^(*n*), with ![art](images/Art_P1417.jpg) giving the *j*th entry
    in vector **x^((*i*))**. The ***label*** of patient *i* is denoted by a scalar
    *y*^((*i*)) ∈ ℝ, signifying the severity of the patient’s heart disease. The hypothesis
    should capture a relationship between the **x^((*i*))** values and *y*^((*i*)).
    For this example, we make the modeling assumption that the relationship is linear,
    and therefore the goal is to compute the “best” linear relationship between the
    **x^((*i*))** values and *y*^((*i*)): a linear function *f* : ℝ^(*n*) → ℝ such
    that *f*(**x^((*i*))**) ≈ *y*^((*i*)) for each patient *i*. Of course, no such
    function may exist, but you would like one that comes as close as possible. A
    linear function *f* can be defined by a vector of weights **w** = (*w*[0], *w*[1],
    …, *w*[*n*]), with'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '作为预测的监督学习的一个具体例子，假设您想要预测一个患者是否会患心脏病。对于每个*m*个患者，您有*n*个不同的属性。例如，您可能有*n* = 4，四个数据是年龄、身高、血压和患有心脏病的近亲人数。将患者*i*的数据表示为向量**x^((*i*))**
    ∈ ℝ^(*n*)，其中![art](images/Art_P1417.jpg)给出向量**x^((*i*))**中第*j*个条目。患者*i*的***标签***用标量*y*^((*i*))
    ∈ ℝ表示，表示患者心脏病的严重程度。假设应该捕捉**x^((*i*))**值与*y*^((*i*)）之间的关系。对于这个例子，我们做出建模假设，即关系是线性的，因此目标是计算**x^((*i*))**值和*y*^((*i*)）之间的“最佳”线性关系：一个线性函数*f*
    : ℝ^(*n*) → ℝ，使得*f*(**x^((*i*))**) ≈ *y*^((*i*)）对于每个患者*i*成立。当然，可能不存在这样的函数，但您希望找到一个尽可能接近的函数。线性函数*f*可以由权重向量**w**
    = (*w*[0], *w*[1], …, *w*[*n*])定义，其中'
- en: '![art](images/Art_P1418.jpg)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![art](images/Art_P1418.jpg)'
- en: When evaluating a machine-learning model, you need to measure how close each
    value *f*(**x^((*i*))**) is to its corresponding label *y*^((*i*)). In this example,
    we define the error *e*^((*i*)) ∈ ℝ associated with patient *i* as *e*^((*i*))
    = *f*(**x^((*i*))**) – *y*^((*i*)). The objective function we choose is to minimize
    the sum of squares of the errors, which is
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估机器学习模型时，您需要衡量每个值*f*(**x^((*i*))**)与其对应标签*y*^((*i*))之间的接近程度。在这个例子中，我们将与第*i*个患者相关联的误差*e*^((*i*))
    ∈ ℝ定义为*e*^((*i*)) = *f*(**x^((*i*))**) – *y*^((*i*))。我们选择的目标函数是最小化误差平方和，即
- en: '![art](images/Art_P1419.jpg)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![art](images/Art_P1419.jpg)'
- en: The objective function is typically called the ***loss function***, and the
    ***least-squares error*** given by equation (33.33) is just one example of many
    possible loss functions. The goal is then, given the **x^((*i*))** and *y*^((*i*))
    values, to compute the weights *w*[0], *w*[1], …, *w*[*n*] so as to minimize the
    loss function in equation (33.33). The variables here are the weights *w*[0],
    *w*[1], …, *w*[*n*] and not the **x^((*i*))** or *y*^((*i*)) values.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 目标函数通常被称为***损失函数***，方程（33.33）给出的***最小二乘误差***只是许多可能损失函数中的一个例子。因此，目标是，给定**x^((*i*))**和*y*^((*i*))值，计算权重
    *w*[0]、*w*[1]、…、*w*[*n*]，以使方程（33.33）中的损失函数最小化。这里的变量是权重 *w*[0]、*w*[1]、…、*w*[*n*]，而不是**x^((*i*))**或*y*^((*i*))值。
- en: This particular objective is sometimes known as a ***least-squares fit***, and
    the problem of finding a linear function to fit data and minimize the least-squares
    error is called ***linear regression***. Finding a least-squares fit is also addressed
    in [Section 28.3](chapter028.xhtml#Sec_28.3).
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 这个特定的目标有时被称为***最小二乘拟合***，找到一个线性函数来拟合数据并最小化最小二乘误差的问题被称为***线性回归***。寻找最小二乘拟合也在[第28.3节](chapter028.xhtml#Sec_28.3)中讨论。
- en: When the function *f* is linear, the loss function defined in equation (33.33)
    is convex, because it is the sum of squares of linear functions, which are themselves
    convex. Therefore, we can apply gradient descent to compute a set of weights to
    approximately minimize the least-squares error. The concrete goal of learning
    is to be able to make predictions on new data. Informally, if the features are
    all reported in the same units and are from the same range (perhaps from being
    normalized), then the weights tend to have a natural interpretation because the
    features of the data that are better predictors of the label have a larger associated
    weight. For example, you would expect that, after normalization, the weight associated
    with the number of family members with heart disease would be larger than the
    weight associated with height.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 当函数*f*是线性的时，方程（33.33）中定义的损失函数是凸的，因为它是线性函数的平方和，而线性函数本身是凸的。因此，我们可以应用梯度下降来计算一组权重，以近似最小化最小二乘误差。学习的具体目标是能够对新数据进行预测。非正式地说，如果特征都以相同单位报告并且来自相同范围（可能是经过归一化处理），那么权重往往具有自然的解释，因为数据的特征更好地预测标签的权重会更大。例如，你会期望，在归一化后，与患有心脏病家庭成员数量相关的权重会大于与身高相关的权重。
- en: The computed weights form a model of the data. Once you have a model, you can
    make predictions, so that given new data, you can predict its label. In our example,
    given a new patient **x**′ who is not part of the original training data set,
    you would still hope to predict the chance that the new patient develops heart
    disease. You can do so by computing the label *f*(**x**′), incorporating the weights
    computed by gradient descent.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 计算得到的权重形成了数据的模型。一旦有了模型，就可以进行预测，因此，给定新数据，可以预测其标签。在我们的例子中，给定一个不属于原始训练数据集的新患者**x**′，你仍然希望预测新患者患心脏病的几率。你可以通过计算标签*f*(**x**′)来实现，其中包含了梯度下降计算得到的权重。
- en: For this linear-regression problem, the objective is to minimize the expression
    in equation (33.33), which is a quadratic in each of the *n*+1 weights *w*[*j*].
    Thus, entry *j* in the gradient is linear in *w*[*j*]. Exercise 33.3-5 asks you
    to explicitly compute the gradient and see that it can be computed in *O*(*nm*)
    time, which is linear in the input size. Compared with the exact method of solving
    equation (33.33) in [Chapter 28](chapter028.xhtml), which needs to invert a matrix,
    gradient descent is typically much faster.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个线性回归问题，目标是最小化方程（33.33）中的表达式，其中每个 *n*+1 个权重 *w*[*j*] 都是二次的。因此，梯度中的第 *j* 项对
    *w*[*j*] 是线性的。练习 33.3-5 要求你明确计算梯度，并看到它可以在*O*(*nm*)时间内计算，这在输入规模上是线性的。与在[第28章](chapter028.xhtml)中解方程（33.33）的精确方法需要求逆矩阵相比，梯度下降通常快得多。
- en: '[Section 33.1](chapter033.xhtml#Sec_33.1) briefly discussed regularization—the
    idea that a complicated hypothesis should be penalized in order to avoid overfitting
    the training data. Regularization often involves adding a term to the objective
    function, but it can also be achieved by adding a constraint. One way to regularize
    this example would be to explicitly limit the norm of the weights, adding a constraint
    that ∥**w**∥ ≤ *B* for some bound *B* > 0\. (Recall again that the components
    of the vector **w** are the variables in the present application.) Adding this
    constraint controls the complexity of the model, as the number of values *w*[*j*]
    that can have large absolute value is now limited.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '[第33.1节](chapter033.xhtml#Sec_33.1)简要讨论了正则化的概念——复杂的假设应该受到惩罚，以避免过度拟合训练数据。正则化通常涉及向目标函数添加一个项，但也可以通过添加约束来实现。正则化这个例子的一种方法是明确限制权重的范数，添加一个约束
    ∥**w**∥ ≤ *B*，其中 *B* > 0。（再次回想一下，向量 **w** 的分量是当前应用中的变量。）添加这个约束控制了模型的复杂性，因为现在可以具有大绝对值的值
    *w*[*j*] 的数量受到限制。'
- en: In order to run GRADIENT-DESCENT-CONSTRAINED for any problem, you need to implement
    the projection step, as well as to compute bounds on *R* and *L*. We conclude
    this section by describing these calculations for gradient descent with the constraint
    ∥**w**∥ ≤ *B*. First, consider the projection step in line 5\. Suppose that the
    update in line 4 results in a vector **w**′. The projection is implemented by
    computing ∏[*k*](**w**′) where *K* is defined by ∥**w**∥ ≤ *B*. This particular
    projection can be accomplished by simply scaling **w**′, since we know that closest
    point in *K* to **w**′ must be the point along the vector whose norm is exactly
    *B*. The amount *z* by which we need to scale **w**′ to hit the boundary of *K*
    is the solution to the equation *z* ∥**w**′∥ = *B*, which is solved by *z* = *B*/∥**w**′∥.
    Hence line 5 is implemented by computing **w** = **w**′*B*/∥**w**′∥. Because we
    always have ∥**w**∥ ≤ *B*, Exercise 33.3-6 asks you to show that the upper bound
    on the magnitude *L* of the gradient is *O*(*B*). We also get a bound on *R*,
    as follows. By the constraint ∥**w**∥ ≤ *B*, we know that both ∥**w^((0))**∥ ≤
    *B* and ∥**w***∥ ≤ *B*, and thus ∥**w^((0))** – **w***∥ ≤ 2*B*. Using the definition
    of *R* in equation (33.20), we have *R* = *O*(*B*). The bound ![art](images/Art_P1420.jpg)
    on the accuracy of the solution after *T* iterations in Theorem 33.11 becomes
    ![art](images/Art_P1421.jpg).
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对任何问题运行GRADIENT-DESCENT-CONSTRAINED，您需要实现投影步骤，以及计算*R*和*L*的边界。我们通过描述带有约束∥**w**∥
    ≤ *B*的梯度下降的这些计算来结束本节。首先，考虑第5行中的投影步骤。假设第4行的更新导致向量**w**′。通过计算∏[*k*](**w**′)来实现投影，其中*K*由∥**w**∥
    ≤ *B*定义。通过简单地缩放**w**′，可以实现这种特定的投影，因为我们知道*K*中距离**w**′最近的点必须是沿着其范数恰好为*B*的向量的点。我们需要缩放**w**′的量*z*以达到*K*的边界，这个量*z*是方程*z*
    ∥**w**′∥ = *B*的解，通过*z* = *B*/∥**w**′∥来解决。因此，第5行通过计算**w** = **w**′*B*/∥**w**′∥来实现。因为我们始终有∥**w**∥
    ≤ *B*，练习33.3-6要求您展示梯度的幅度*L*的上界为*O*(*B*)。我们还得到了*R*的上界，如下所示。通过约束∥**w**∥ ≤ *B*，我们知道∥**w^((0))**∥
    ≤ *B*和∥**w***∥ ≤ *B*，因此∥**w^((0))** – **w***∥ ≤ 2*B*。使用方程（33.20）中的*R*的定义，我们有*R*
    = *O*(*B*)。在定理33.11中，*T*次迭代后解的精度上界为![art](images/Art_P1420.jpg)。
- en: '**Exercises**'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习**'
- en: '***33.3-1***'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '***33.3-1***'
- en: Prove Lemma 33.6\. Start from the definition of a convex function given in equation
    (33.18). (*Hint:* You can prove the statement when *n* = 1 first. The proof for
    general values of *n* is similar.)
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 证明引理33.6。从方程（33.18）中给出的凸函数的定义开始。(*提示:*您可以先证明*n* = 1时的陈述。对于一般值的*n*，证明类似。)
- en: '***33.3-2***'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '***33.3-2***'
- en: Prove Lemma 33.7.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 证明引理33.7。
- en: '***33.3-3***'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '***33.3-3***'
- en: Prove equation (33.29). (*Hint:* The proof for *n* = 1 dimension is straightforward.
    The proof for general values of *n* dimensions follows along similar lines.)
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 证明方程（33.29）。(*提示:*对于*n* = 1维的证明很简单。对于一般值的*n*维，证明沿着类似的线路进行。)
- en: '***33.3-4***'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '***33.3-4***'
- en: Show that the function *f* in equation (33.32) is a convex function of the variables
    *w*[0], *w*[1], …, *w*[*n*].
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 展示方程（33.32）中的函数*f*是变量*w*[0]，*w*[1]，…，*w*[*n*]的凸函数。
- en: '***33.3-5***'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '***33.3-5***'
- en: Compute the gradient of expression (33.33) and explain how to evaluate the gradient
    in *O*(*nm*) time.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 计算表达式（33.33）的梯度，并解释如何在*O*(*nm*)时间内评估梯度。
- en: '***33.3-6***'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '***33.3-6***'
- en: Consider the function *f* defined in equation (33.32), and suppose that you
    have a bound ∥**w**∥ ≤ *B*, as is considered in the discussion on regularization.
    Show that *L* = *O*(*B*) in this case.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑方程（33.32）中定义的函数*f*，假设您有一个边界∥**w**∥ ≤ *B*，就像在正则化讨论中考虑的那样。证明在这种情况下*L* = *O*(*B*)。
- en: '***33.3-7***'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '***33.3-7***'
- en: Equation (33.2) on page 1009 gives a function that, when minimized, gives an
    optimal solution to the *k*-means problem. Explain how to use gradient descent
    to solve the *k*-means problem.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 方程（33.2）在第1009页给出了一个函数，当最小化时，可以给出*k*-means问题的最优解。解释如何使用梯度下降来解决*k*-means问题。
- en: '**Problems**'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**'
- en: '***33-1     Newton’s method***'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '***33-1     牛顿法***'
- en: 'Gradient descent iteratively moves closer to a desired value (the minimum)
    of a function. Another algorithm in this spirit is known as ***Newton’s method***,
    which is an iterative algorithm that finds the root of a function. Here, we consider
    Newton’s method which, given a function *f* : ℝ → ℝ, finds a value *x** such that
    *f*(*x** ) = 0\. The algorithm moves through a series of points *x*^((0)), *x*^((1)),
    …. If the algorithm is currently at a point *x*^((*t*)), then to find point *x*^((*t*+1)),
    it first takes the equation of the line tangent to the curve at *x* = *x*^((*t*)),'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降迭代地接近函数的期望值（最小值）。这种精神中的另一个算法被称为***牛顿法***，这是一种找到函数根的迭代算法。在这里，我们考虑牛顿法，给定一个函数*f*：ℝ
    → ℝ，找到一个值*x**，使得*f*(*x** ) = 0。该算法通过一系列点*x*^((0))，*x*^((1))，…移动。如果算法当前位于点*x*^((*t*))，那么要找到点*x*^((*t*+1))，它首先取曲线在*x*
    = *x*^((*t*))处的切线方程，
- en: '*y* = *f*′(*x*^((*t*)))(*x* – *x*^((*t*))) + *f*(*x*^((*t*))).'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* = *f*′(*x*^((*t*)))(*x* – *x*^((*t*))) + *f*(*x*^((*t*))).'
- en: It then uses the *x*-intercept of this line as the next point *x*^((*t*+1)).
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用这条线的*x*-截距作为下一个点*x*^((*t*+1))。
- en: '***a.*** Show that the algorithm described above can be summarized by the update
    rule'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '***a.*** 展示上述描述的算法可以总结为更新规则'
- en: '![art](images/Art_P1422.jpg)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![art](images/Art_P1422.jpg)'
- en: We restrict our attention to some domain *I* and assume that *f*′(*x*) ≠ 0 for
    all *x* ∈ *I* and that *f*″(*x*) is continuous. We also assume that the starting
    point *x*^((0)) is sufficiently close to *x**, where “sufficiently close” means
    that we can use only the first two terms of the Taylor expansion of *f*(*x**)
    about *x*^((0)), namely
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将注意力集中在某个域*I*上，并假设对于所有*x* ∈ *I*，*f*′(*x*) ≠ 0，并且*f*″(*x*)连续。我们还假设起始点*x*^((0))足够接近*x**，这里“足够接近”意味着我们可以仅使用关于*x*^((0))的*f*(*x**)的泰勒展开的前两项，即
- en: '![art](images/Art_P1423.jpg)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
  zh: '![art](images/Art_P1423.jpg)'
- en: where *γ*^((0)) is some value between *x*^((0)) and *x**. If the approximation
    in equation (33.34) holds for *x*^((0)), it also holds for any point closer to
    *x**.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*γ*^((0))是介于*x*^((0))和*x**之间的某个值。如果方程（33.34）中对*x*^((0))的近似成立，则对于任何更接近*x**的点也成立。
- en: '***b.*** Assume that the function *f* has exactly one point *x** for which
    *f*(*x**) = 0\. Let *ϵ*^((*t*)) = |*x*^((*t*)) – *x**|. Using the Taylor expansion
    in equation (33.34), show that'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '***b.*** 假设函数*f*恰好有一个点*x**，使得*f*(*x**) = 0。令*ϵ*^((*t*)) = |*x*^((*t*)) – *x**|。使用方程(33.34)中的泰勒展开，证明'
- en: '![art](images/Art_P1424.jpg)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
  zh: '![art](images/Art_P1424.jpg)'
- en: where *γ*^((*t*)) is some value between *x*^((*t*)) and *x**.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*γ*^((*t*))是在*x*^((*t*))和*x**之间的某个值。
- en: '***c.*** If'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '***c.*** 如果'
- en: '![art](images/Art_P1425.jpg)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
  zh: '![art](images/Art_P1425.jpg)'
- en: for some constant *c* and *ϵ*^((0)) < 1, then we say that the function *f* has
    ***quadratic convergence***, since the error decreases quadratically. Assuming
    that *f* has quadratic convergence, how many iterations are needed to find a root
    of *f*(*x*) to an accuracy of *δ*? Your answer should include *δ*.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某个常数*c*和*ϵ*^((0)) < 1，如果函数*f*具有***二次收敛性***，那么我们说错误会二次减少。假设*f*具有二次收敛性，需要多少次迭代才能找到*f*(*x*)的根，使其精度为*δ*？您的答���应包括*δ*。
- en: '***d.*** Suppose you wish to find a root of the function *f*(*x*) = (*x* –
    3)², which is also the minimizer, and you start at *x*^((0)) = 3.5\. Compare the
    number of iterations needed by gradient descent to find the minimizer and Newton’s
    method to find the root.'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '***d.*** 假设您希望找到函数*f*(*x*) = (*x* – 3)²的根，这也是最小化器，并且您从*x*^((0)) = 3.5开始。比较梯度下降和牛顿法找到最小化器和根所需的迭代次数。'
- en: '***33-2     Hedge***'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '***33-2     Hedge***'
- en: Another variant in the multiplicative-weights framework is known as HEDGE. It
    differs from WEIGHTED MAJORITY in two ways. First, HEDGE makes the prediction
    randomly—in iteration *t*, it assigns a probability ![art](images/Art_P1426.jpg)
    to expert *E*[*i*], where ![art](images/Art_P1427.jpg). It then chooses an expert
    *E*[*i*′] according to this probability distribution and predicts according to
    *E*[*i*′]. Second, the update rule is different. If an expert makes a mistake,
    line 16 updates that expert’s weight by the rule ![art](images/Art_P1428.jpg),
    for some 0 < *ϵ* < 1\. Show that the expected number of mistakes made by HEDGE,
    running for *T* rounds, is at most *m** + (ln *n*)/*ϵ* + *ϵT*.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 在乘法权重框架中的另一种变体被称为HEDGE。它与WEIGHTED MAJORITY有两点不同。首先，HEDGE随机进行预测—在第*t*次迭代中，它为专家*E*[*i*]分配概率![art](images/Art_P1426.jpg)，其中![art](images/Art_P1427.jpg)。然后根据这个概率分布选择一个专家*E*[*i*′]，并根据*E*[*i*′]进行预测。其次，更新规则不同。如果一个专家犯了错误，第16行根据规则![art](images/Art_P1428.jpg)更新该专家的权重，其中0
    < *ϵ* < 1。证明HEDGE在运行*T*轮时所犯错误的期望次数最多为*m** + (ln *n*)/*ϵ* + *ϵT*。
- en: '***33-3     Nonoptimality of Lloyd’s procedure in one dimension***'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '***33-3     Lloyd在一维中的非最优性***'
- en: Give an example to show that even in one dimension, Lloyd’s procedure for finding
    clusters does not always return an optimum result. That is, Lloyd’s procedure
    may terminate and return as a result a set *C* of clusters that does not minimize
    *f*(*S*, *C*), even when *S* is a set of points on a line.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 给出一个示例，表明即使在一维中，寻找聚类的Lloyd过程并不总是返回最佳结果。也就是说，即使*S*是一条线上的点集，Lloyd过程可能终止并返回一组不最小化*f*(*S*,
    *C*)的聚类集*C*作为结果。
- en: '***33-4     Stochastic gradient descent***'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '***33-4     随机梯度下降***'
- en: Consider the problem described in [Section 33.3](chapter033.xhtml#Sec_33.3)
    of fitting a line *f*(*x*) = *ax* + *b* to a given set of point/value pairs *S*
    = {(*x*[1], *y*[1]), …, (*x*[*T*], *y*[*T*])} by optimizing the choice of the
    parameters *a* and *b* using gradient descent to find a best least-squares fit.
    Here we consider the case where *x* is a real-valued variable, rather than a vector.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑在[第33.3节](chapter033.xhtml#Sec_33.3)中描述的问题，通过梯度下降将线*f*(*x*) = *ax* + *b*拟合到给定的点/值对集合*S*
    = {(*x*[1], *y*[1]), …, (*x*[*T*], *y*[*T*])}中，通过优化参数*a*和*b*的选择来找到最佳的最小二乘拟合。在这里，我们考虑*x*是一个实值变量的情况，而不是一个向量。
- en: Suppose that you are not given the point/value pairs in *S* all at once, but
    only one at a time in an online manner. Furthermore, the points are given in random
    order. That is, you know that there are *n* points, but in iteration *t* you are
    given only (*x*[*i*], *y*[*i*]) where *i* is independently and randomly chosen
    from {1, …, *T*}.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您不是一次性给出*S*中的点/值对，而是以在线方式逐个给出。此外，这些点是以随机顺序给出的。也就是说，在第*t*次迭代中，您只会得到一个(*x*[*i*],
    *y*[*i*])，其中*i*是从{1, …, *T*}中独立随机选择的。
- en: You can use gradient descent to compute an estimate to the function. As each
    point (*x*[*i*], *y*[*i*]) is considered, you can update the current values of
    *a* and *b* by taking the derivative with respect to *a* and *b* of the term of
    the objective function depending on (*x*[*i*], *y*[*i*]). Doing so gives you a
    stochastic estimate of the gradient, and you can then take a small step in the
    opposite direction.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用梯度下降来计算函数的估计值。当考虑每个点(*x*[*i*], *y*[*i*])时，您可以通过对依赖于(*x*[*i*], *y*[*i*])的目标函数的项对*a*和*b*分别求导来更新当前的*a*和*b*的值。这样做会给您一个随机梯度的估计，然后您可以朝着相反的方向迈出一小步。
- en: Give pseudcode to implement this variant of gradient descent. What would the
    expected value of the error be as a function of *T*, *L*, and *R*? (*Hint:* Replicate
    the analysis of GRADIENT-DESCENT in [Section 33.3](chapter033.xhtml#Sec_33.3)
    for this variant.)
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 给出伪代码来实现这种梯度下降的变体。作为*T*、*L*和*R*的函数，误差的期望值会是多少？（*提示：*复制[第33.3节](chapter033.xhtml#Sec_33.3)中GRADIENT-DESCENT的分析。）
- en: This procedure and its variants are known as ***stochastic gradient descent***.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程及其变体被称为***随机梯度下降***。
- en: '**Chapter notes**'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '**章节注释**'
- en: For a general introduction to artificial intelligence, we recommend Russell
    and Norvig [[391](bibliography001.xhtml#endnote_391)]. For a general introduction
    to machine learning, we recommend Murphy [[340](bibliography001.xhtml#endnote_340)].
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 对于人工智能的一般介绍，我们推荐Russell和Norvig[[391](bibliography001.xhtml#endnote_391)]。对于机器学习的一般介绍，我们推荐Murphy[[340](bibliography001.xhtml#endnote_340)]。
- en: Lloyd’s procedure for the *k*-means problem was first proposed by Lloyd [[304](bibliography001.xhtml#endnote_304)]
    and also later by Forgy [[151](bibliography001.xhtml#endnote_151)]. It is sometimes
    called “Lloyd’s algorithm” or the “Lloyd-Forgy algorithm.” Although Mahajan et
    al. [[310](bibliography001.xhtml#endnote_310)] showed that finding an optimal
    clustering is NP-hard, even in the plane, Kanungo et al. [[241](bibliography001.xhtml#endnote_241)]
    have shown that there is an approximation algorithm for the *k*-means problem
    with approximation ratio 9 + *ϵ*, for any *ϵ* > 0.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: Lloyd提出的*k*-means问题的过程首先由Lloyd提出[[304](bibliography001.xhtml#endnote_304)]，后来由Forgy提出[[151](bibliography001.xhtml#endnote_151)]。有时被称为“Lloyd算法”或“Lloyd-Forgy算法”。尽管Mahajan等人[[310](bibliography001.xhtml#endnote_310)]表明，即使在平面上，找到最佳聚类也是NP难的，但Kanungo等人[[241](bibliography001.xhtml#endnote_241)]已经证明了存在一个近似比率为9
    + *ϵ*的*k*-means问题的近似算法，对于任意*ϵ* > 0。
- en: The multiplicative-weights method is surveyed by Arora, Hazan, and Kale [[25](bibliography001.xhtml#endnote_25)].
    The main idea of updating weights based on feedback has been rediscovered many
    times. One early use is in game theory, where Brown defined “Fictitious Play”
    [[74](bibliography001.xhtml#endnote_74)] and conjectured its convergence to the
    value of a zero-sum game. The convergence properties were established by Robinson
    [[382](bibliography001.xhtml#endnote_382)].
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 乘法权重方法由Arora，Hazan和Kale进行了调查[[25](bibliography001.xhtml#endnote_25)]。基于反馈更新权重的主要思想已经被多次重新发现。早期的一个用途是在博弈论中，Brown定义了“虚构游戏”[[74](bibliography001.xhtml#endnote_74)]，并推测其收敛到零和游戏的价值。Robinson证明了其收敛性质[[382](bibliography001.xhtml#endnote_382)]。
- en: In machine learning, the first use of multiplicative weights was by Littlestone
    in the Winnow algorithm [[300](bibliography001.xhtml#endnote_300)], which was
    later extended by Littlestone and Warmuth to the weighted-majority algorithm described
    in [Section 33.2](chapter033.xhtml#Sec_33.2) [[301](bibliography001.xhtml#endnote_301)].
    This work is closely connected to the boosting algorithm, originally due to Freund
    and Shapire [[159](bibliography001.xhtml#endnote_159)]. The multiplicative-weights
    idea is also closely related to several more general optimization algorithms,
    including the perceptron algorithm [[328](bibliography001.xhtml#endnote_328)]
    and algorithms for optimization problems such as packing linear programs [[177](bibliography001.xhtml#endnote_177),
    [359](bibliography001.xhtml#endnote_359)].
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，第一次使用乘法权重的是Littlestone在Winnow算法中[[300](bibliography001.xhtml#endnote_300)]，后来由Littlestone和Warmuth扩展为在[第33.2节](chapter033.xhtml#Sec_33.2)中描述的加权多数算法[[301](bibliography001.xhtml#endnote_301)]。这项工作与boosting算法密切相关，最初由Freund和Shapire提出[[159](bibliography001.xhtml#endnote_159)]。乘法权重的想法也与几种更一般的优化算法密切相关，包括感知器算法[[328](bibliography001.xhtml#endnote_328)]和用于打包线性规划等优化问题的算法[[177](bibliography001.xhtml#endnote_177),
    [359](bibliography001.xhtml#endnote_359)]。
- en: The treatment of gradient descent in this chapter draws heavily on the unpublished
    manuscript of Bansal and Gupta [[35](bibliography001.xhtml#endnote_35)]. They
    emphasize the idea of using a potential function and using ideas from amortized
    analysis to explain gradient descent. Other presentations and analyses of gradient
    descent include works by Bubeck [[75](bibliography001.xhtml#endnote_75)], Boyd
    and Vanderberghe [[69](bibliography001.xhtml#endnote_69)], and Nesterov [[343](bibliography001.xhtml#endnote_343)].
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中对梯度下降的处理在Bansal和Gupta的未发表手稿中得到了很大的启发[[35](bibliography001.xhtml#endnote_35)]。他们强调使用潜在函数的想法，并利用摊销分析的思想来解释梯度下降。其他关于梯度下降的演示和分析包括Bubeck的作品[[75](bibliography001.xhtml#endnote_75)]，Boyd和Vanderberghe的作品[[69](bibliography001.xhtml#endnote_69)]，以及Nesterov的作品[[343](bibliography001.xhtml#endnote_343)]。
- en: Gradient descent is known to converge faster when functions obey stronger properties
    than general convexity. For example, a function *f* is ***α-strongly convex***
    if *f*(**y**) ≥ *f*(**x**) + 〈(∇*f*)(**x**), (**y** – **x**)〉 + *α*∥**y** – **x**∥
    for all **x**, **y** ∈ ℝ^(*n*). In this case, GRADIENT-DESCENT can use a variable
    step size and return **x^((*T*))**. The step size at step *t* becomes *γ*[*t*]
    = 1/(*α*(*t* + 1)), and the procedure returns a point such that *f*(**x-avg**)
    – *f*(**x***) ≤ *L*²/(*α*(*T* + 1)). This convergence is better than that of Theorem
    33.8 because the number of iterations needed is linear, rather than quadratic,
    in the desired error parameter *ϵ*, and because the performance is independent
    of the initial point.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 当函数遵守比一般凸性更强的性质时，梯度下降收敛更快。例如，如果对于所有**x**，**y** ∈ ℝ^(*n*)，函数*f*是***α-强凸***的，那么*f*(**y**)
    ≥ *f*(**x**) + 〈(∇*f*)(**x**), (**y** – **x**)〉 + *α*∥**y** – **x**∥。在这种情况下，GRADIENT-DESCENT可以使用可变步长并返回**x^((*T*))**。在第*t*步的步长变为*γ*[*t*]
    = 1/(*α*(*t* + 1))，并且该过程返回一个点，使得*f*(**x-avg**) – *f*(**x***) ≤ *L*²/(*α*(*T* +
    1))。这种收敛比定理33.8更好，因为所需的迭代次数是线性的，而不是二次的，所需的误差参数*ϵ*，并且性能与初始点无关。
- en: Another case in which gradient descent can be shown to perform better than the
    analysis in [Section 33.3](chapter033.xhtml#Sec_33.3) suggests is for smooth convex
    functions. We say that a function is ***β-smooth*** if ![art](images/Art_P1429.jpg).
    This inequality goes in the opposite direction from the one for *≈*-strong convexity.
    Better bounds on gradient descent are possible here as well.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降比[第33.3节](chapter033.xhtml#Sec_33.3)中的分析表明在光滑凸函数中表现更好的另一个案例。我们说一个函数是***β-光滑***的，如果![art](images/Art_P1429.jpg)。这个不等式与*≈*强凸性的不等式方向相反。在这里也可以得到更好的梯度下降界限。
- en: '[¹](#footnote_ref_1) Although the curve in [Figure 33.4](chapter033.xhtml#Fig_33-4)
    looks concave, according to the definition of convexity that we’ll see below,
    the function *f* in the figure is convex.'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '[¹](#footnote_ref_1)尽管[图33.4](chapter033.xhtml#Fig_33-4)中的曲线看起来是凹的，根据我们将在下面看到的凸性定义，图中的函数*f*是凸的。'
- en: '[²](#footnote_ref_2) A function *f* : ℝ^(*n*) → ℝ is closed if, for each *α*
    ∈ ℝ, the set {**x** ∈ dom(*f*) : *f*(**x**) ≤ *α*} is closed, where dom(*f*) is
    the domain of *f*.'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '[²](#footnote_ref_2)一个函数*f*：ℝ^(*n*) → ℝ如果对于每个*α* ∈ ℝ，集合{**x** ∈ dom(*f*)：*f*(**x**)
    ≤ *α*}是闭的，其中dom(*f*)是*f*的定义域，则函数是闭的。'
